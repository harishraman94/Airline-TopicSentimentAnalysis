{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning:\n",
      "\n",
      "detected Windows; aliasing chunkize to chunkize_serial\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "#from emoticons import EmoticonDetector\n",
    "import re as regex\n",
    "import string\n",
    "import collections\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "import operator\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import plotly\n",
    "from plotly import graph_objs\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "import time\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import csv\n",
    "import _pickle as cPickle\n",
    "from nltk.corpus import words, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "num_sem_words = 13\n",
    "\n",
    "kb = set(['bag.n.01','bag.n.04','bag.n.05','bag.n.06','baggage.n.01','baggage.n.03','staff.n.01', 'lost.a.01','dress.v.02','dress.v.10','dress.v.02','break.v.02','clothing.n.01','apparel.n.01','staff.v.02','seat.n.01','seat.v.01','seat.v.05','call.n.01','support.n.02','support.n.01','support.n.11','support.v.01','booking.n.02','reservation.n.06','avail.n.01','internet.n.01', 'engagement.n.05', 'booking.n.02', 'reserve.v.04', 'answer.v.01', 'answer.v.02', 'answer.v.03', 'answer.v.05', 'answer.v.08', 'answer.v.09', 'answer.v.10',  'answer.n.01', 'solution.n.02', 'answer.n.03', 'answer.n.05', 'service.n.01', 'service.n.02', 'service.n.05', 'avail.n.01', 'overhaul.n.01', 'service.n.15', 'call.n.01', 'cry.n.01', 'call.n.06', 'call.n.09', 'name.v.01', 'call.v.03', 'shout.v.02', 'call.v.05', 'visit.v.03', 'call.v.07', 'call.v.09', 'call.v.10', 'call.v.11', 'address.v.06', 'bid.v.04', 'call.v.23', 'call.v.24', 'call.v.25', 'telephone.n.01', 'earphone.n.01', 'chat.n.01', 'support.n.01', 'support.n.02', 'web_site.n.01', 'table.n.02', 'table.n.03', 'floor.n.01', 'floor.n.06', 'shock.v.01', 'customer.n.01', 'services.n.01', 'service.v.01', 'service.v.02', 'speak.v.03', 'address.v.02', 'reservation.n.01', 'reservation.n.05', 'reservation.n.06', 'reservation.n.07', 'hang.v.01', 'hang.v.02', 'hang.v.04', 'hang.v.08', 'cling.v.03', 'ticket.n.01', 'tag.n.01', 'ticket.v.02', 'class.n.01', 'class.n.03', 'class.n.08', 'classify.v.01', 'web.n.01', 'network.n.01', 'world_wide_web.n.01', 'reply.n.02', 'aid.n.02', 'talk.v.02', 'talk.v.01', 'communication.n.01', 'agent.n.02', 'message.n.01', 'message.v.03', 'staff.n.01', 'fee.n.01', 'service.n.02', 'electronic_mail.n.01', 'e-mail.v.01', 'on-line.a.02', 'help.v.01', 'desk.n.01', 'password.n.01', 'card.n.02', 'error.n.06', 'browser.n.02'])\n",
    "\n",
    "df = pd.read_csv('../data/Bag.csv')\n",
    "df2 = pd.read_csv('../data/Flight.csv')\n",
    "df3 = pd.read_csv('../data/Service.csv')\n",
    "#df4 = pd.DataFrame()\n",
    "#df5 = pd.DataFrame()\n",
    "#df6 = pd.DataFrame()\n",
    "#dfannon = pd.read_csv('Annoted_Training_Tweets_13_semwords.csv')\n",
    "#df_input = pd.read_csv('Input_Tweets.csv')\n",
    "df_senti = pd.DataFrame()\n",
    "#df_inter = pd.DataFrame()\n",
    "df_inter = pd.read_csv('../data/Interim_7_words.csv')\n",
    "df_clean = pd.DataFrame()\n",
    "#df_clean = pd.read_csv('Interim_clean.csv')\n",
    "\n",
    "\n",
    "df_Bag = df[:630]\n",
    "df_Flight = df2[:2615]\n",
    "df_Bag_test = df[630:]\n",
    "df_Flight_test = df2[2615:]\n",
    "len_df3 = int(0.8*float(len(df3['text'].tolist())))\n",
    "df_Service = df3[:len_df3]\n",
    "df_Service_test = df3[len_df3:]\n",
    "\n",
    "list_text = []\n",
    "list_topic = []\n",
    "# list_sentiment = []\n",
    "# list_text_Bag = []\n",
    "# list_text_Flight = []\n",
    "# list_text_Service = []\n",
    "\n",
    "# training_tweets = df_Bag['text'].tolist()\n",
    "# training_tweets.extend(df_Flight['text'].tolist())\n",
    "# training_tweets.extend(df_Service['text'].tolist())\n",
    "training_tweets = df_inter['text'].tolist()\n",
    "\n",
    "##testing_tweets = df_Bag_test['text'].tolist()\n",
    "##testing_tweets = df_input['text'].tolist()\n",
    "testing_tweets = []\n",
    "\n",
    "topics = ['bag','flight']\n",
    "\n",
    "set_of_adv = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@', '#', 'virgin', 'america', 'united', 'delta', 'jetblue', 'southwest']\n",
    "    for separator in string.punctuation:\n",
    "        if separator not in entity_prefixes:\n",
    "            text = text.replace(separator, \" \")\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                # words.append(\" \")\n",
    "                words.append(word)\n",
    "            # print(type(''.join(words)))\n",
    "    return \" \".join(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    try:\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        return normalized\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "doc_clean = [clean(tweet).split() for tweet in training_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.246664047241211 seconds ---\n",
      "[(0, '0.056*\"class\" + 0.047*\"service\" + 0.046*\"support\" + 0.041*\"communication\" + 0.033*\"message\" + 0.030*\"aid\" + 0.024*\"flight\" + 0.020*\"error\" + 0.017*\"network\"'), (1, '0.050*\"answer\" + 0.041*\"communication\" + 0.037*\"message\" + 0.032*\"error\" + 0.031*\"reply\" + 0.031*\"aid\" + 0.028*\"support\" + 0.025*\"flight\" + 0.025*\"solution\"'), (2, '0.087*\"bag\" + 0.046*\"web\" + 0.043*\"clothing\" + 0.030*\"baggage\" + 0.022*\"hang\" + 0.019*\"apparel\" + 0.018*\"customer\" + 0.016*\"floor\" + 0.016*\"united\"')]\n",
      "{2: 'bag', 1: 'flight', 0: 'service'}\n",
      "--- 3.626708507537842 seconds ---\n",
      "Enter number of testing tweets\n",
      "3\n",
      "Enter input tweet\n",
      "why is my flight late?\n",
      "flight\n",
      "--- 14.088988304138184 seconds ---\n",
      "Enter input tweet\n",
      "@VirginAmerica this serivce of yours is very poor\n",
      "service\n",
      "--- 26.621584177017212 seconds ---\n",
      "Enter input tweet\n",
      "I found a bag someone lost\n",
      "bag\n",
      "--- 36.72991967201233 seconds ---\n",
      "--- 36.73590302467346 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Lda = gensim.models.ldamodel.LdaModel\n",
    "# ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=250)\n",
    "\n",
    "ldamodel = gensim.models.LdaModel.load('../model/Model_phase_3_7_words')\n",
    "print(ldamodel.print_topics(num_topics=3, num_words=9))\n",
    "\n",
    "dict_topic_number = {}\n",
    "dict_prediction = {}\n",
    "dict_prediction[0] = 0\n",
    "dict_prediction[1] = 0\n",
    "dict_prediction[2] = 0\n",
    "\n",
    "temp_set = set([0,1,2])\n",
    "for tweet in topics:\n",
    "    doc = clean(tweet)\n",
    "    bow = dictionary.doc2bow(doc.split())\n",
    "    t = ldamodel.get_document_topics(bow)\n",
    "    maxi = 0.0\n",
    "    tid = -1\n",
    "    for ele in t:\n",
    "        if ele[1]>maxi:\n",
    "            maxi = ele[1]\n",
    "            tid = ele[0]\n",
    "    dict_topic_number[int(tid)] = tweet\n",
    "    temp_set.remove(int(tid))\n",
    "\n",
    "temp_tid = int(temp_set.pop())\n",
    "dict_topic_number[temp_tid] = 'service'\n",
    "print(dict_topic_number)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Enter number of testing tweets\")\n",
    "number = int(input())\n",
    "for i in range(number):\n",
    "    print(\"Enter input tweet\")\n",
    "    #find sentiment of each tweet here, store in sentiment_value\n",
    "    tweet = input()\n",
    "\n",
    "    tweet = strip_all_entities(tweet)\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "    set_of_n = set()\n",
    "    set_of_v = set()\n",
    "    set_of_adj = set()\n",
    "    sync_set = set()\n",
    "    for element in tagged:\n",
    "        if element[1] == 'NN' or element[1] == 'NNS' or element[1] == 'NNP' or element[1] == 'NNPS':\n",
    "            set_of_n.add(element[0])\n",
    "        if element[1] == 'VB' or element[1] == 'VBD' or element[1] == 'VBG' or element[1] == 'VBN' or element[\n",
    "            1] == 'VBP' or element[1] == 'VBZ':\n",
    "            set_of_v.add(element[0])\n",
    "        if element[1] == 'JJ' or element[1] == 'JJR' or element[1] == 'JJS':\n",
    "            set_of_adj.add(element[0])\n",
    "        if element[1] == 'RBR' or element[1] == 'RB' or element[1] == 'RBS' or element[1] == 'WRB':\n",
    "            set_of_v.add(element[0])\n",
    "\n",
    "    for each_ele in set_of_n:\n",
    "        ele_to_add = lesk(tweet, each_ele, 'n')\n",
    "        if ele_to_add is not None:\n",
    "            sync_set.add(ele_to_add)\n",
    "    for each_ele in set_of_v:\n",
    "        ele_to_add = lesk(tweet, each_ele, 'v')\n",
    "        if ele_to_add is not None:\n",
    "            sync_set.add(ele_to_add)\n",
    "    for each_ele in set_of_adj:\n",
    "        ele_to_add = lesk(tweet, each_ele, 'a')\n",
    "        if ele_to_add is not None:\n",
    "            sync_set.add(ele_to_add)\n",
    "\n",
    "    for each_know in kb:\n",
    "        sync_each_know = wordnet.synset(each_know)\n",
    "        for each_sync in sync_set:\n",
    "            similarity = each_sync.wup_similarity(sync_each_know)\n",
    "            if similarity is not None and float(similarity)>0.80:\n",
    "                dotted_word = each_sync.name()\n",
    "                temp_string = \"\"\n",
    "                for j in range(len(dotted_word)):\n",
    "                    if dotted_word[j] == '.':\n",
    "                        temp_string += \" \" + dotted_word[:j]\n",
    "                        break\n",
    "                tweet += temp_string\n",
    "    doc = clean(tweet)\n",
    "    bow = dictionary.doc2bow(doc.split())\n",
    "    t = ldamodel.get_document_topics(bow)\n",
    "    maxi = 0.0\n",
    "    tid = -1\n",
    "    for ele in t:\n",
    "        if ele[1]>maxi:\n",
    "            maxi = ele[1]\n",
    "            tid = ele[0]\n",
    "    dict_prediction[int(tid)] += 1\n",
    "    topic_name = dict_topic_number[int(tid)]\n",
    "    list_text.append(tweet)\n",
    "    list_topic.append(topic_name)\n",
    "    #list_sentiment.append(sentiment_value)\n",
    "    print(topic_name)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "df_senti['text'] = pd.Series(list_text)\n",
    "df_senti['topic'] = pd.Series(list_topic)\n",
    "#df_senti['sentiment'] = pd.Series(list_sentiment)\n",
    "df_senti.to_csv('../data/Output_Final_phase.csv')\n",
    "\n",
    "#ldamodel.save('final_model_phase_3')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    processed_Traindata = []\n",
    "    wordlist = []  \n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    \n",
    "    def initialize(self, csv_file, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            self.data_model = pd.read_csv(from_cached)\n",
    "            return\n",
    "        self.processed_Traindata = pd.read_csv(csv_file, usecols=[0,1])\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>why is my flight late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this serivce of yours is very poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I found a bag someone lost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                text\n",
       "0           0               why is my flight late\n",
       "1           1  this serivce of yours is very poor\n",
       "2           2          I found a bag someone lost"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Pre-processing for Sentiment Analysis\n",
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"../data/Output_Final_Phase.csv\")\n",
    "data.processed_Traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Data Preprocessing\n",
    "nltk.download('words')\n",
    "word_dictionary = list(set(words.words()))\n",
    "\n",
    "for alphabet in \"bcdefghjklmnopqrstuvwxyzBCDEFGHJKLMNOPQRSTUVWXYZ\":\n",
    "    word_dictionary.remove(alphabet)\n",
    "class DataPreprocessing:\n",
    "    def iterate(self):\n",
    "        for preprocessingMethod in [self.replaceProcessedHashtags,\n",
    "                                   self.removeUrls,\n",
    "                                   self.removeUsernames,\n",
    "                                   self.removeElongatedWords,\n",
    "                                   self.removeNa,\n",
    "                                   self.replaceSlangWords,\n",
    "                                   self.removeSpecialChars,\n",
    "                                   self.removeNumbers]:\n",
    "            yield preprocessingMethod\n",
    "    \n",
    "    @staticmethod\n",
    "    def removeByRegex(tweets, regExp):\n",
    "        tweets.loc[:, \"text\"].replace(regExp, \"\", inplace=True)\n",
    "        return tweets\n",
    "    \n",
    "    def removeUrls(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "    \n",
    "    def removeNa(self, tweets):\n",
    "        return tweets[tweets[\"text\"] != \"\"]\n",
    "    \n",
    "    def removeSpecialChars(self, tweets):\n",
    "        for remove in map(lambda r: regex.compile(regex.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\"\n",
    "                                                                    \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                    \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                    \"!\", \"?\", \".\", \"'\",\n",
    "                                                                    \"--\", \"---\",\"#\"]):\n",
    "            tweets.loc[:, \"text\"].replace(remove, \"\", inplace=True)\n",
    "        return tweets\n",
    "    \n",
    "    def removeUsernames(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "    \n",
    "    def removeElongatedWords(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"(.)\\1+', r'\\1\\1\"))\n",
    "    \n",
    "    def removeNumbers(self, tweets):\n",
    "        #print(tweets)\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))\n",
    "    \n",
    "    def replaceSlangWords(self, tweets):\n",
    "        with open('../data/slang.txt') as file:\n",
    "            slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "            for line in file if line.strip())\n",
    "            #print(tweets[\"text\"])\n",
    "            #print(\"-----------------------------------------END\")\n",
    "            for index,word in tweets['text'].iteritems():\n",
    "                #print(index)\n",
    "                for i in word.split():\n",
    "                    isUpperCase = i.isupper()\n",
    "                    i = i.lower()\n",
    "                    if i in slang_map.keys():\n",
    "                        word = word.replace(i, slang_map[i])\n",
    "                        tweets.loc[(index),\"text\"] = word\n",
    "                if isUpperCase:\n",
    "                    i = i.upper()\n",
    "        #print(tweets.loc[:,\"text\"])\n",
    "        return tweets\n",
    "\n",
    "    # print(split_tweets)\n",
    "    @staticmethod\n",
    "    def removeDigitsFromHashtag(tag):\n",
    "        tag = regex.sub(r\"\\s?[0-9]+\\.?[0-9]*\", \"\", tag)\n",
    "        return tag\n",
    "\n",
    "    @staticmethod\n",
    "    def collect_hashtags_in_tweet(wordList):\n",
    "        hashtags = []\n",
    "        for word in wordList:\n",
    "            index = word.find('#')\n",
    "            if index != -1:\n",
    "                if word[index + 1:] != '':\n",
    "                    hashtags.append(word[index + 1:])\n",
    "        return hashtags\n",
    "\n",
    "    @staticmethod\n",
    "    def split_hashtag_to_words_all_possibilities(hashtag):\n",
    "        all_possibilities = []\n",
    "\n",
    "        split_posibility = [hashtag[:i] in word_dictionary for i in reversed(range(len(hashtag) + 1))]\n",
    "        possible_split_positions = [i for i, x in enumerate(split_posibility) if x == True]\n",
    "\n",
    "        for split_pos in possible_split_positions:\n",
    "            split_words = []\n",
    "            word_1, word_2 = hashtag[:len(hashtag) - split_pos], hashtag[len(hashtag) - split_pos:]\n",
    "\n",
    "            if word_2 in word_dictionary:\n",
    "                split_words.append(word_1)\n",
    "                split_words.append(word_2)\n",
    "                all_possibilities.append(split_words)\n",
    "\n",
    "                another_round = DataPreprocessing.split_hashtag_to_words_all_possibilities(word_2)\n",
    "\n",
    "                if len(another_round) > 0:\n",
    "                    all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in\n",
    "                                                             zip([word_1] * len(another_round), another_round)]\n",
    "            else:\n",
    "                another_round = DataPreprocessing.split_hashtag_to_words_all_possibilities(word_2)\n",
    "\n",
    "                if len(another_round) > 0:\n",
    "                    all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in\n",
    "                                                             zip([word_1] * len(another_round), another_round)]\n",
    "\n",
    "        return all_possibilities\n",
    "\n",
    "    @staticmethod\n",
    "    def process_all_hashtags_in_tweet(hashtags):\n",
    "        all_words = []\n",
    "        for tag in hashtags:\n",
    "            split_hashtag = DataPreprocessing.split_hashtag_to_words_all_possibilities(DataPreprocessing.removeDigitsFromHashtag(tag))\n",
    "            if split_hashtag:\n",
    "                all_words = all_words + split_hashtag[0]\n",
    "            else:\n",
    "                all_words.append(tag)\n",
    "        return all_words \n",
    "    \n",
    "    def replaceProcessedHashtags(self, tweets):\n",
    "            for index,word in tweets['text'].iteritems():\n",
    "                word=word.split()\n",
    "                collectHashtags=DataPreprocessing.collect_hashtags_in_tweet(word)\n",
    "                allHashtags=DataPreprocessing.process_all_hashtags_in_tweet(collectHashtags)\n",
    "                collectHashtags = [\"#\" + tag for tag in collectHashtags]\n",
    "                if allHashtags:\n",
    "                    word = list(set(word) - set(collectHashtags))\n",
    "                    word = word + allHashtags\n",
    "                word = \" \".join(word)\n",
    "                tweets.loc[(index),\"text\"] = word\n",
    "                #print(tweets.loc[(index),\"text\"])\n",
    "           # print(tweets)\n",
    "            return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning the Training Data\n",
    "class CleanTrainingData(TwitterData_Initialize):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        #self.processed_Testdata = previous.processed_Testdata\n",
    "    \n",
    "    def cleaningData(self, cleaner):\n",
    "        train = self.processed_Traindata\n",
    "        #test = self.processed_Testdata\n",
    "        \n",
    "        for cleanerMethod in cleaner.iterate():\n",
    "            train = cleanerMethod(train)\n",
    "            #test = cleanerMethod(test)\n",
    "        self.processed_Traindata = train\n",
    "        #self.processed_Testdata = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>why is my flight late</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this serivce of yours is very poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I found a bag someone lost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                text\n",
       "0           0               why is my flight late\n",
       "1           1  this serivce of yours is very poor\n",
       "2           2          I found a bag someone lost"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CleanTrainingData(data)\n",
    "data.cleaningData(DataPreprocessing())\n",
    "data.processed_Traindata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenizing and Stemming the data\n",
    "class TokenizationStemming(CleanTrainingData):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        #self.processed_TestData = previous.processed_TestData\n",
    "    \n",
    "    def stem(self, stemmer = nltk.PorterStemmer()):\n",
    "        def stemJoin(row):\n",
    "            row[\"text\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"text\"]))\n",
    "            return row\n",
    "    \n",
    "        self.processed_Traindata = self.processed_Traindata.apply(stemJoin, axis=1)\n",
    "    \n",
    "    def tokenize(self, tokenizer = nltk.word_tokenize):\n",
    "        def tokenizeRow(row):\n",
    "            row[\"text\"] = tokenizer(row[\"text\"])\n",
    "            row[\"tokenizedText\"] = [] + row[\"text\"]\n",
    "            return row\n",
    "        \n",
    "        self.processed_Traindata = self.processed_Traindata.apply(tokenizeRow, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 15), ('o', 8), ('s', 7), ('e', 6), ('i', 5)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building Wordlist\n",
    "#Un-filtered version without removing stopwords\n",
    "words = collections.Counter()\n",
    "for idx in data.processed_Traindata.index:\n",
    "    words.update(data.processed_Traindata.loc[idx, \"text\"])\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 15), ('e', 6), ('r', 4), ('h', 3), ('f', 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "whitelist = [\"n't\", \"not\"]\n",
    "for idx, stop_word in enumerate(stopwords):\n",
    "    if stop_word not in whitelist:\n",
    "        del words[stop_word]\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating the final wordlist\n",
    "class WordList(TokenizationStemming):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "    \n",
    "    whitelist = [\"n't\", \"not\"]\n",
    "    wordlist = []\n",
    "    \n",
    "    def buildWordlist(self, min_occurrences=3, max_occurences=3000, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                     whitelist=None):\n",
    "        self.wordlist = []\n",
    "        whitelist = self.whitelist if whitelist is None else whitelist\n",
    "        import os\n",
    "        if os.path.isfile('../data/wordlist.csv'):\n",
    "            word_df = pd.read_csv('../data/wordlist.csv', encoding = \"ISO-8859-1\")\n",
    "            word_df = word_df[word_df[\"occurrences\"] > min_occurrences]\n",
    "            self.wordlist = list(word_df.loc[:, \"word\"])\n",
    "            return\n",
    "        words = collections.Counter()\n",
    "        for idx in self.processed_Traindata.index:\n",
    "            words.update(self.processed_Traindata.loc[idx, \"text\"])\n",
    "        \n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "        \n",
    "        word_df = pd.DataFrame(data={\"word\" : [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                    \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                              columns = [\"word\", \"occurrences\"])\n",
    "        \n",
    "        word_df.to_csv(\"../data/wordlist.csv\", index_label=\"idx\", encoding = \"utf8\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = WordList(data)\n",
    "data.buildWordlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transforming into Bag-of-Words\n",
    "class BagOfWords(WordList):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        self.wordlist = previous.wordlist\n",
    "    \n",
    "    def buildDataModel(self):\n",
    "        columns = list(\n",
    "            map(lambda w: w + \"_bow\", self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        \n",
    "        for idx in self.processed_Traindata.index:\n",
    "            currentRow = []\n",
    "            \n",
    "            tokens = set(self.processed_Traindata.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                currentRow.append(1 if word in tokens else 0)\n",
    "            \n",
    "            rows.append(currentRow)\n",
    "        \n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        \n",
    "        return self.data_model, self.data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour_bow</th>\n",
       "      <th>cancel_bow</th>\n",
       "      <th>help_bow</th>\n",
       "      <th>servic_bow</th>\n",
       "      <th>delay_bow</th>\n",
       "      <th>time_bow</th>\n",
       "      <th>custom_bow</th>\n",
       "      <th>bag_bow</th>\n",
       "      <th>call_bow</th>\n",
       "      <th>im_bow</th>\n",
       "      <th>...</th>\n",
       "      <th>gorgeou_bow</th>\n",
       "      <th>woohoo_bow</th>\n",
       "      <th>thousand_bow</th>\n",
       "      <th>understat_bow</th>\n",
       "      <th>furiou_bow</th>\n",
       "      <th>manual_bow</th>\n",
       "      <th>smell_bow</th>\n",
       "      <th>ber_bow</th>\n",
       "      <th>charleston_bow</th>\n",
       "      <th>nrt_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2414 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour_bow  cancel_bow  help_bow  servic_bow  delay_bow  time_bow  \\\n",
       "0         0           0         0           0          0         0   \n",
       "1         0           0         0           0          0         0   \n",
       "2         0           0         0           0          0         0   \n",
       "\n",
       "   custom_bow  bag_bow  call_bow  im_bow   ...     gorgeou_bow  woohoo_bow  \\\n",
       "0           0        0         0       0   ...               0           0   \n",
       "1           0        0         0       0   ...               0           0   \n",
       "2           0        0         0       0   ...               0           0   \n",
       "\n",
       "   thousand_bow  understat_bow  furiou_bow  manual_bow  smell_bow  ber_bow  \\\n",
       "0             0              0           0           0          0        0   \n",
       "1             0              0           0           0          0        0   \n",
       "2             0              0           0           0          0        0   \n",
       "\n",
       "   charleston_bow  nrt_bow  \n",
       "0               0        0  \n",
       "1               0        0  \n",
       "2               0        0  \n",
       "\n",
       "[3 rows x 2414 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = BagOfWords(data)\n",
    "bow, labels = data.buildDataModel()\n",
    "bow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Detecting Emoticons\n",
    "class EmoticonDetector:\n",
    "    emoticons = {}\n",
    "    \n",
    "    def __init__(self, emoticon_file=\"../data/emoticons.txt\"):\n",
    "        from pathlib import Path\n",
    "        content = Path(emoticon_file).read_text()\n",
    "        positive = True\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if \"positive\" in line.lower():\n",
    "                positive = True\n",
    "                continue\n",
    "            elif \"negative\" in line.lower():\n",
    "                positive = False\n",
    "                continue\n",
    "            \n",
    "            self.emoticons[line] = positive\n",
    "    \n",
    "    def is_positive(self, emoticon):\n",
    "        if emoticon in self.emoticons:\n",
    "            return self.emoticons[emoticon]\n",
    "        return False\n",
    "    \n",
    "    def is_emoticon(self, to_check):\n",
    "        return to_check in self.emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtraFeatures(WordList):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_data_model(self):\n",
    "        extra_columns = [col for col in self.processed_Traindata.columns if col.startswith(\"number_of\")]\n",
    "        columns = extra_columns + list(\n",
    "                map(lambda w: w + \"_bow\", self.wordlist))\n",
    "        \n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_Traindata.index:\n",
    "            current_row = []\n",
    "        \n",
    "            for _,col in enumerate(extra_columns):\n",
    "                current_row.append(self.processed_Traindata.loc[idx, col])\n",
    "        \n",
    "        #adding bag-of-words\n",
    "            tokens = set(self.processed_Traindata.loc[idx, \"text\"])\n",
    "            for _,word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "        \n",
    "            rows.append(current_row)\n",
    "        \n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        \n",
    "        return self.data_model, self.data_labels\n",
    "    \n",
    "    def add_column(self, column_name, column_content):\n",
    "        self.processed_Traindata.loc[:, column_name] = pd.Series(column_content, index=self.processed_Traindata.index)\n",
    "\n",
    "    def build_features(self):\n",
    "        def count_by_lambda(expression, word_array):\n",
    "            return len(list(filter(expression, word_array)))\n",
    "        \n",
    "        def count_occurences(character, word_array):\n",
    "            counter = 0\n",
    "            for j, word in enumerate(word_array):\n",
    "                for char in word:\n",
    "                    if char == character:\n",
    "                        counter += 1\n",
    "            return counter\n",
    "        \n",
    "        def count_interjections(wordArray):\n",
    "            interjections = []\n",
    "            interjectionCount = 0\n",
    "            with open('../data/interjections.txt') as file:\n",
    "                interjections = file.read().splitlines()\n",
    "            for word in wordArray:\n",
    "                if word in interjections:\n",
    "                    interjectionCount += 1\n",
    "            return interjectionCount \n",
    "\n",
    "        def count_by_regex(regex, plain_text):\n",
    "            return len(regex.findall(plain_text))\n",
    "        \n",
    "        self.add_column(\"splitted_text\", map(lambda txt: txt.split(\" \"), self.processed_Traindata[\"text\"]))\n",
    "        \n",
    "        #Number of uppercase words\n",
    "        uppercase = list(map(lambda txt: count_by_lambda(lambda word: word == word.upper(), txt),\n",
    "                                                        self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_uppercase\", uppercase)\n",
    "        \n",
    "        #number of !\n",
    "        exclamations = list(map(lambda txt: count_occurences(\"!\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_exclamation\", exclamations)\n",
    "        \n",
    "        #number of ?\n",
    "        questions = list(map(lambda txt: count_occurences(\"?\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_question\", questions)\n",
    "        \n",
    "        #number of ...\n",
    "        ellipsis = list(map(lambda txt: count_by_regex(regex.compile(r\"\\.\\s?\\.\\s?\\.\"), txt),\n",
    "                           self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_ellipsis\", ellipsis)\n",
    "        \n",
    "        #number of hashtags\n",
    "        hashtags = list(map(lambda txt: count_occurences(\"#\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_hashtags\", hashtags)\n",
    "        \n",
    "        #number of mentions\n",
    "        mentions = list(map(lambda txt: count_occurences(\"@\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_mentions\", mentions)\n",
    "        \n",
    "        #number of quotes\n",
    "        quotes = list(map(lambda plain_text: int(count_occurences(\"'\", [plain_text.strip(\"'\").strip('\"')]) / 2 +\n",
    "                                                 count_occurences('\"', [plain_text.strip(\"'\").strip('\"')]) / 2),\n",
    "                          self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_quotes\", quotes)\n",
    "        \n",
    "        #number of urls\n",
    "        urls = list(map(lambda txt: count_by_regex(regex.compile(r\"http.?://[^\\s]+[\\s]?\"), txt),\n",
    "                             self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_urls\", urls)\n",
    "        \n",
    "        #number of positive emoticons\n",
    "        ed = EmoticonDetector()\n",
    "        positive_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and ed.is_positive(word), txt), \n",
    "                   self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_positive_emo\", positive_emo)\n",
    "        \n",
    "        #number of negative emoticons\n",
    "        negative_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and not ed.is_positive(word), txt), \n",
    "                   self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_negative_emo\", negative_emo)\n",
    "        \n",
    "        #number of interjections\n",
    "        interjections = list(map(lambda txt: count_interjections(txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_interjections\", interjections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_uppercase</th>\n",
       "      <th>number_of_exclamation</th>\n",
       "      <th>number_of_question</th>\n",
       "      <th>number_of_ellipsis</th>\n",
       "      <th>number_of_hashtags</th>\n",
       "      <th>number_of_mentions</th>\n",
       "      <th>number_of_quotes</th>\n",
       "      <th>number_of_urls</th>\n",
       "      <th>number_of_positive_emo</th>\n",
       "      <th>number_of_negative_emo</th>\n",
       "      <th>...</th>\n",
       "      <th>gorgeou_bow</th>\n",
       "      <th>woohoo_bow</th>\n",
       "      <th>thousand_bow</th>\n",
       "      <th>understat_bow</th>\n",
       "      <th>furiou_bow</th>\n",
       "      <th>manual_bow</th>\n",
       "      <th>smell_bow</th>\n",
       "      <th>ber_bow</th>\n",
       "      <th>charleston_bow</th>\n",
       "      <th>nrt_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2425 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_uppercase  number_of_exclamation  number_of_question  \\\n",
       "0                    0                      0                   0   \n",
       "1                    0                      0                   0   \n",
       "2                    1                      0                   0   \n",
       "\n",
       "   number_of_ellipsis  number_of_hashtags  number_of_mentions  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "\n",
       "   number_of_quotes  number_of_urls  number_of_positive_emo  \\\n",
       "0                 0               0                       0   \n",
       "1                 0               0                       0   \n",
       "2                 0               0                       0   \n",
       "\n",
       "   number_of_negative_emo   ...     gorgeou_bow  woohoo_bow  thousand_bow  \\\n",
       "0                       0   ...               0           0             0   \n",
       "1                       0   ...               0           0             0   \n",
       "2                       0   ...               0           0             0   \n",
       "\n",
       "   understat_bow  furiou_bow  manual_bow  smell_bow  ber_bow  charleston_bow  \\\n",
       "0              0           0           0          0        0               0   \n",
       "1              0           0           0          0        0               0   \n",
       "2              0           0           0          0        0               0   \n",
       "\n",
       "   nrt_bow  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "\n",
       "[3 rows x 2425 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ExtraFeatures()\n",
    "data.initialize(\"../data/Output_Final_Phase.csv\")\n",
    "data.build_features()\n",
    "data.cleaningData(DataPreprocessing())\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.buildWordlist()\n",
    "data_model, labels = data.build_data_model()\n",
    "\n",
    "data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input with 2424 features, got 2425 instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-4f447cbc63b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mrf_loaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mresult_nb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_loaded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_nb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Naive-Bayes Prediction : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult_nb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \"\"\"\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features_X\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m             raise ValueError(\"Expected input with %d features, got %d instead\"\n\u001b[1;32m--> 836\u001b[1;33m                              % (n_features, n_features_X))\n\u001b[0m\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[0mneg_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_log_prob_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input with 2424 features, got 2425 instead"
     ]
    }
   ],
   "source": [
    "#Load Naive-Bayes Library\n",
    "with open('../model/NaiveBayesClassifier.pkl', 'rb') as nid:\n",
    "    nb_loaded = cPickle.load(nid)\n",
    "with open('../model/RandomForestClassifier.pkl', 'rb') as rid:\n",
    "    rf_loaded = cPickle.load(rid)\n",
    "\n",
    "result_nb = nb_loaded.predict(data_model)\n",
    "print(type(result_nb))\n",
    "print(\"Naive-Bayes Prediction : \",result_nb)\n",
    "result_rf = rf_loaded.predict(data_model)\n",
    "print(\"Random Forest Prediction : \",result_rf)\n",
    "df_csv = pd.read_csv(\"../data/Output_Final_Phase.csv\")\n",
    "df_csv['NaiveBayesSentiment'] = pd.DataFrame(result_nb)\n",
    "df_csv['RandomForestSentiment'] = pd.DataFrame(result_rf)\n",
    "\n",
    "df_csv.to_csv(\"../data/Output_Final_Phase.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
