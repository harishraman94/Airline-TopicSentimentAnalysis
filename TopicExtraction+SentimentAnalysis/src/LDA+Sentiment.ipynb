{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "#from emoticons import EmoticonDetector\n",
    "import re as regex\n",
    "\n",
    "import string\n",
    "import collections\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "import operator\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import plotly\n",
    "from plotly import graph_objs\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "import time\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import csv\n",
    "import _pickle as cPickle\n",
    "from nltk.corpus import words, stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strip_links(text):\n",
    "    link_regex =  regex.compile('(((http?)|(https?)):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', regex.DOTALL)\n",
    "    links         = regex.findall(link_regex, text)\n",
    "    for link in links:\n",
    "        i=0\n",
    "        text = text.replace(link[i],'')\n",
    "        i+=1\n",
    "    return text\n",
    "\n",
    "\n",
    "def strip_all_entities(text):\n",
    "    entity_prefixes = ['@', '#', 'virgin', 'america', 'united', 'delta', 'jetblue', 'southwest']\n",
    "    for separator in string.punctuation:\n",
    "        if separator not in entity_prefixes:\n",
    "            text = text.replace(separator, \" \")\n",
    "    words = []\n",
    "    for word in text.split():\n",
    "        word = word.strip()\n",
    "        if word:\n",
    "            if word[0] not in entity_prefixes:\n",
    "                # words.append(\" \")\n",
    "                words.append(word)\n",
    "            # print(type(''.join(words)))\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "\n",
    "    stop = set(stopwords.words('english'))\n",
    "\n",
    "    exclude = set(string.punctuation)\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    # print(stop_free)\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    # print(punc_free)\n",
    "\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in stop_free.split())\n",
    "\n",
    "\n",
    "    return (normalized)\n",
    "\n",
    "\n",
    "\n",
    "def get_clean_docs(documents):\n",
    "    doc_complete = [doc for doc in documents]\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    sp = [strip_links(doc) for doc in doc_complete]\n",
    "    sp = [strip_all_entities(doc) for doc in sp]\n",
    "\n",
    "    doc_clean = [clean(doc) for doc in sp]\n",
    "\n",
    "    doc_clean = [regex.sub(r'(.)\\1+', r'\\1\\1', (doc)).split() for doc in doc_clean]\n",
    "    stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=False)\n",
    "\n",
    "    for doc in doc_clean:\n",
    "        if 'u' in doc:\n",
    "            doc.remove('u')\n",
    "\n",
    "    print (doc_clean)\n",
    "    return  doc_clean\n",
    "\n",
    "def get_dicts():\n",
    "    df = pd.read_csv('../data/Tweets.csv')\n",
    "    doc_complete = [row[1] for row in df[df.columns[10]].iteritems()]\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    sp = [strip_links(doc) for doc in doc_complete]\n",
    "    sp = [strip_all_entities(doc) for doc in sp]\n",
    "\n",
    "    doc_clean = [clean(doc) for doc in sp]\n",
    "\n",
    "    doc_clean = [regex.sub(r'(.)\\1+', r'\\1\\1', (doc)).split() for doc in doc_clean]\n",
    "    stemmer2 = SnowballStemmer(\"english\", ignore_stopwords=False)\n",
    "\n",
    "    for doc in doc_clean:\n",
    "        if 'u' in doc:\n",
    "            doc.remove('u')\n",
    "\n",
    "    return doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginiaAirlines your customer service is very bad\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[['customer', 'service', 'bad']]\n",
      "[(0, '0.020*\"flight\" + 0.017*\"thank\" + 0.010*\"aa\" + 0.008*\"‚Äù\" + 0.008*\"fleet\" + 0.008*\"fleek\" + 0.007*\"get\" + 0.006*\"weather\" + 0.006*\"dca\" + 0.006*\"good\"'), (1, '0.049*\"flight\" + 0.026*\"hour\" + 0.017*\"2\" + 0.015*\"time\" + 0.014*\"bag\" + 0.013*\"plane\" + 0.012*\"get\" + 0.011*\"delayed\" + 0.011*\"still\" + 0.010*\"hold\"'), (2, '0.010*\"seat\" + 0.010*\"would\" + 0.009*\"like\" + 0.007*\"class\" + 0.007*\"one\" + 0.007*\"airline\" + 0.007*\"first\" + 0.007*\"fly\" + 0.007*\"ticket\" + 0.006*\"go\"'), (3, '0.039*\"service\" + 0.037*\"customer\" + 0.031*\"thanks\" + 0.012*\"guy\" + 0.010*\"thank\" + 0.010*\"dm\" + 0.010*\"great\" + 0.009*\"would\" + 0.009*\"response\" + 0.009*\"like\"'), (4, '0.058*\"flight\" + 0.035*\"cancelled\" + 0.022*\"get\" + 0.019*\"call\" + 0.018*\"help\" + 0.017*\"flightled\" + 0.016*\"phone\" + 0.011*\"change\" + 0.010*\"need\" + 0.009*\"one\"')]\n"
     ]
    }
   ],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "tweet = input()\n",
    "ldamodel=Lda.load('../model/lda_10')\n",
    "dclean=get_dicts()\n",
    "tweet=[tweet]\n",
    "clean_tweet= get_clean_docs(tweet)\n",
    "dictionary = corpora.Dictionary(dclean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_tweet]\n",
    "topics = ldamodel.show_topics(num_topics=5, num_words=10, log=False, formatted=True)\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "label_list=[max(\n",
    "        ldamodel.get_document_topics(doc, minimum_probability=None, minimum_phi_value=None, per_word_topics=False),\n",
    "key=lambda item: item[1])[0] for doc in doc_term_matrix]\n",
    "\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 1}\n"
     ]
    }
   ],
   "source": [
    "topic_count={}\n",
    "for label in label_list:\n",
    "    if label in topic_count:\n",
    "        topic_count[label]=topic_count[label] + 1\n",
    "    else:\n",
    "        topic_count[label]=1\n",
    "se = pd.Series(label_list)\n",
    "print (topic_count)\n",
    "#df = pd.read_csv('Tweets.csv')\n",
    "df=pd.DataFrame({'text':tweet})\n",
    "df['label'] = se\n",
    "#df.to_csv('modified_tweets5.csv')\n",
    "df.to_csv('../data/LDAOutput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    processed_Traindata = []\n",
    "    wordlist = []  \n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    \n",
    "    def initialize(self, csv_file, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            self.data_model = pd.read_csv(from_cached)\n",
    "            return\n",
    "        self.processed_Traindata = pd.read_csv(csv_file, usecols=[0,1])\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginiaAirlines your customer service is ver...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text\n",
       "0           0  @VirginiaAirlines your customer service is ver..."
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Pre-processing for Sentiment Analysis\n",
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"../data/LDAOutput.csv\")\n",
    "data.processed_Traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\haris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Data Preprocessing\n",
    "nltk.download('words')\n",
    "word_dictionary = list(set(words.words()))\n",
    "\n",
    "for alphabet in \"bcdefghjklmnopqrstuvwxyzBCDEFGHJKLMNOPQRSTUVWXYZ\":\n",
    "    word_dictionary.remove(alphabet)\n",
    "class DataPreprocessing:\n",
    "    def iterate(self):\n",
    "        for preprocessingMethod in [self.replaceProcessedHashtags,\n",
    "                                   self.removeUrls,\n",
    "                                   self.removeUsernames,\n",
    "                                   self.removeElongatedWords,\n",
    "                                   self.removeNa,\n",
    "                                   self.replaceSlangWords,\n",
    "                                   self.removeSpecialChars,\n",
    "                                   self.removeNumbers]:\n",
    "            yield preprocessingMethod\n",
    "    \n",
    "    @staticmethod\n",
    "    def removeByRegex(tweets, regExp):\n",
    "        tweets.loc[:, \"text\"].replace(regExp, \"\", inplace=True)\n",
    "        return tweets\n",
    "    \n",
    "    def removeUrls(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "    \n",
    "    def removeNa(self, tweets):\n",
    "        return tweets[tweets[\"text\"] != \"\"]\n",
    "    \n",
    "    def removeSpecialChars(self, tweets):\n",
    "        for remove in map(lambda r: regex.compile(regex.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\"\n",
    "                                                                    \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                    \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                    \"!\", \"?\", \".\", \"'\",\n",
    "                                                                    \"--\", \"---\",\"#\"]):\n",
    "            tweets.loc[:, \"text\"].replace(remove, \"\", inplace=True)\n",
    "        return tweets\n",
    "    \n",
    "    def removeUsernames(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "    \n",
    "    def removeElongatedWords(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"(.)\\1+', r'\\1\\1\"))\n",
    "    \n",
    "    def removeNumbers(self, tweets):\n",
    "        #print(tweets)\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))\n",
    "    \n",
    "    def replaceSlangWords(self, tweets):\n",
    "        with open('../data/slang.txt') as file:\n",
    "            slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "            for line in file if line.strip())\n",
    "            #print(tweets[\"text\"])\n",
    "            #print(\"-----------------------------------------END\")\n",
    "            for index,word in tweets['text'].iteritems():\n",
    "                #print(index)\n",
    "                for i in word.split():\n",
    "                    isUpperCase = i.isupper()\n",
    "                    i = i.lower()\n",
    "                    if i in slang_map.keys():\n",
    "                        word = word.replace(i, slang_map[i])\n",
    "                        tweets.loc[(index),\"text\"] = word\n",
    "                if isUpperCase:\n",
    "                    i = i.upper()\n",
    "        #print(tweets.loc[:,\"text\"])\n",
    "        return tweets\n",
    "\n",
    "    # print(split_tweets)\n",
    "    @staticmethod\n",
    "    def removeDigitsFromHashtag(tag):\n",
    "        tag = regex.sub(r\"\\s?[0-9]+\\.?[0-9]*\", \"\", tag)\n",
    "        return tag\n",
    "\n",
    "    @staticmethod\n",
    "    def collect_hashtags_in_tweet(wordList):\n",
    "        hashtags = []\n",
    "        for word in wordList:\n",
    "            index = word.find('#')\n",
    "            if index != -1:\n",
    "                if word[index + 1:] != '':\n",
    "                    hashtags.append(word[index + 1:])\n",
    "        return hashtags\n",
    "\n",
    "    @staticmethod\n",
    "    def split_hashtag_to_words_all_possibilities(hashtag):\n",
    "        all_possibilities = []\n",
    "\n",
    "        split_posibility = [hashtag[:i] in word_dictionary for i in reversed(range(len(hashtag) + 1))]\n",
    "        possible_split_positions = [i for i, x in enumerate(split_posibility) if x == True]\n",
    "\n",
    "        for split_pos in possible_split_positions:\n",
    "            split_words = []\n",
    "            word_1, word_2 = hashtag[:len(hashtag) - split_pos], hashtag[len(hashtag) - split_pos:]\n",
    "\n",
    "            if word_2 in word_dictionary:\n",
    "                split_words.append(word_1)\n",
    "                split_words.append(word_2)\n",
    "                all_possibilities.append(split_words)\n",
    "\n",
    "                another_round = DataPreprocessing.split_hashtag_to_words_all_possibilities(word_2)\n",
    "\n",
    "                if len(another_round) > 0:\n",
    "                    all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in\n",
    "                                                             zip([word_1] * len(another_round), another_round)]\n",
    "            else:\n",
    "                another_round = DataPreprocessing.split_hashtag_to_words_all_possibilities(word_2)\n",
    "\n",
    "                if len(another_round) > 0:\n",
    "                    all_possibilities = all_possibilities + [[a1] + a2 for a1, a2, in\n",
    "                                                             zip([word_1] * len(another_round), another_round)]\n",
    "\n",
    "        return all_possibilities\n",
    "\n",
    "    @staticmethod\n",
    "    def process_all_hashtags_in_tweet(hashtags):\n",
    "        all_words = []\n",
    "        for tag in hashtags:\n",
    "            split_hashtag = DataPreprocessing.split_hashtag_to_words_all_possibilities(DataPreprocessing.removeDigitsFromHashtag(tag))\n",
    "            if split_hashtag:\n",
    "                all_words = all_words + split_hashtag[0]\n",
    "            else:\n",
    "                all_words.append(tag)\n",
    "        return all_words \n",
    "    \n",
    "    def replaceProcessedHashtags(self, tweets):\n",
    "            for index,word in tweets['text'].iteritems():\n",
    "                word=word.split()\n",
    "                collectHashtags=DataPreprocessing.collect_hashtags_in_tweet(word)\n",
    "                allHashtags=DataPreprocessing.process_all_hashtags_in_tweet(collectHashtags)\n",
    "                collectHashtags = [\"#\" + tag for tag in collectHashtags]\n",
    "                if allHashtags:\n",
    "                    word = list(set(word) - set(collectHashtags))\n",
    "                    word = word + allHashtags\n",
    "                word = \" \".join(word)\n",
    "                tweets.loc[(index),\"text\"] = word\n",
    "                #print(tweets.loc[(index),\"text\"])\n",
    "           # print(tweets)\n",
    "            return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning the Training Data\n",
    "class CleanTrainingData(TwitterData_Initialize):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        #self.processed_Testdata = previous.processed_Testdata\n",
    "    \n",
    "    def cleaningData(self, cleaner):\n",
    "        train = self.processed_Traindata\n",
    "        #test = self.processed_Testdata\n",
    "        \n",
    "        for cleanerMethod in cleaner.iterate():\n",
    "            train = cleanerMethod(train)\n",
    "            #test = cleanerMethod(test)\n",
    "        self.processed_Traindata = train\n",
    "        #self.processed_Testdata = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>your customer service is very bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                               text\n",
       "0           0  your customer service is very bad"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CleanTrainingData(data)\n",
    "data.cleaningData(DataPreprocessing())\n",
    "data.processed_Traindata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenizing and Stemming the data\n",
    "class TokenizationStemming(CleanTrainingData):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        #self.processed_TestData = previous.processed_TestData\n",
    "    \n",
    "    def stem(self, stemmer = nltk.PorterStemmer()):\n",
    "        def stemJoin(row):\n",
    "            row[\"text\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"text\"]))\n",
    "            return row\n",
    "    \n",
    "        self.processed_Traindata = self.processed_Traindata.apply(stemJoin, axis=1)\n",
    "    \n",
    "    def tokenize(self, tokenizer = nltk.word_tokenize):\n",
    "        def tokenizeRow(row):\n",
    "            row[\"text\"] = tokenizer(row[\"text\"])\n",
    "            row[\"tokenizedText\"] = [] + row[\"text\"]\n",
    "            return row\n",
    "        \n",
    "        self.processed_Traindata = self.processed_Traindata.apply(tokenizeRow, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 5), ('r', 4), ('e', 4), ('s', 3), ('y', 2)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building Wordlist\n",
    "#Un-filtered version without removing stopwords\n",
    "words = collections.Counter()\n",
    "for idx in data.processed_Traindata.index:\n",
    "    words.update(data.processed_Traindata.loc[idx, \"text\"])\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 5), ('r', 4), ('e', 4), ('u', 2), ('c', 2)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "whitelist = [\"n't\", \"not\"]\n",
    "for idx, stop_word in enumerate(stopwords):\n",
    "    if stop_word not in whitelist:\n",
    "        del words[stop_word]\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating the final wordlist\n",
    "class WordList(TokenizationStemming):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "    \n",
    "    whitelist = [\"n't\", \"not\"]\n",
    "    wordlist = []\n",
    "    \n",
    "    def buildWordlist(self, min_occurrences=3, max_occurences=3000, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                     whitelist=None):\n",
    "        self.wordlist = []\n",
    "        whitelist = self.whitelist if whitelist is None else whitelist\n",
    "        import os\n",
    "        if os.path.isfile('../data/wordlist.csv'):\n",
    "            word_df = pd.read_csv('../data/wordlist.csv', encoding = \"ISO-8859-1\")\n",
    "            word_df = word_df[word_df[\"occurrences\"] > min_occurrences]\n",
    "            self.wordlist = list(word_df.loc[:, \"word\"])\n",
    "            return\n",
    "        words = collections.Counter()\n",
    "        for idx in self.processed_Traindata.index:\n",
    "            words.update(self.processed_Traindata.loc[idx, \"text\"])\n",
    "        \n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "        \n",
    "        word_df = pd.DataFrame(data={\"word\" : [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                    \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                              columns = [\"word\", \"occurrences\"])\n",
    "        \n",
    "        word_df.to_csv(\"../data/wordlist.csv\", index_label=\"idx\", encoding = \"utf8\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = WordList(data)\n",
    "data.buildWordlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transforming into Bag-of-Words\n",
    "class BagOfWords(WordList):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        self.wordlist = previous.wordlist\n",
    "    \n",
    "    def buildDataModel(self):\n",
    "        columns = list(\n",
    "            map(lambda w: w + \"_bow\", self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        \n",
    "        for idx in self.processed_Traindata.index:\n",
    "            currentRow = []\n",
    "            \n",
    "            tokens = set(self.processed_Traindata.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                currentRow.append(1 if word in tokens else 0)\n",
    "            \n",
    "            rows.append(currentRow)\n",
    "        \n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        \n",
    "        return self.data_model, self.data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour_bow</th>\n",
       "      <th>cancel_bow</th>\n",
       "      <th>help_bow</th>\n",
       "      <th>servic_bow</th>\n",
       "      <th>delay_bow</th>\n",
       "      <th>time_bow</th>\n",
       "      <th>custom_bow</th>\n",
       "      <th>bag_bow</th>\n",
       "      <th>call_bow</th>\n",
       "      <th>im_bow</th>\n",
       "      <th>...</th>\n",
       "      <th>gorgeou_bow</th>\n",
       "      <th>woohoo_bow</th>\n",
       "      <th>thousand_bow</th>\n",
       "      <th>understat_bow</th>\n",
       "      <th>furiou_bow</th>\n",
       "      <th>manual_bow</th>\n",
       "      <th>smell_bow</th>\n",
       "      <th>ber_bow</th>\n",
       "      <th>charleston_bow</th>\n",
       "      <th>nrt_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 2414 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour_bow  cancel_bow  help_bow  servic_bow  delay_bow  time_bow  \\\n",
       "0         0           0         0           0          0         0   \n",
       "\n",
       "   custom_bow  bag_bow  call_bow  im_bow   ...     gorgeou_bow  woohoo_bow  \\\n",
       "0           0        0         0       0   ...               0           0   \n",
       "\n",
       "   thousand_bow  understat_bow  furiou_bow  manual_bow  smell_bow  ber_bow  \\\n",
       "0             0              0           0           0          0        0   \n",
       "\n",
       "   charleston_bow  nrt_bow  \n",
       "0               0        0  \n",
       "\n",
       "[1 rows x 2414 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = BagOfWords(data)\n",
    "bow, labels = data.buildDataModel()\n",
    "bow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Detecting Emoticons\n",
    "class EmoticonDetector:\n",
    "    emoticons = {}\n",
    "    \n",
    "    def __init__(self, emoticon_file=\"../data/emoticons.txt\"):\n",
    "        from pathlib import Path\n",
    "        content = Path(emoticon_file).read_text()\n",
    "        positive = True\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if \"positive\" in line.lower():\n",
    "                positive = True\n",
    "                continue\n",
    "            elif \"negative\" in line.lower():\n",
    "                positive = False\n",
    "                continue\n",
    "            \n",
    "            self.emoticons[line] = positive\n",
    "    \n",
    "    def is_positive(self, emoticon):\n",
    "        if emoticon in self.emoticons:\n",
    "            return self.emoticons[emoticon]\n",
    "        return False\n",
    "    \n",
    "    def is_emoticon(self, to_check):\n",
    "        return to_check in self.emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtraFeatures(WordList):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_data_model(self):\n",
    "        extra_columns = [col for col in self.processed_Traindata.columns if col.startswith(\"number_of\")]\n",
    "        columns = extra_columns + list(\n",
    "                map(lambda w: w + \"_bow\", self.wordlist))\n",
    "        \n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_Traindata.index:\n",
    "            current_row = []\n",
    "        \n",
    "            for _,col in enumerate(extra_columns):\n",
    "                current_row.append(self.processed_Traindata.loc[idx, col])\n",
    "        \n",
    "        #adding bag-of-words\n",
    "            tokens = set(self.processed_Traindata.loc[idx, \"text\"])\n",
    "            for _,word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "        \n",
    "            rows.append(current_row)\n",
    "        \n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        \n",
    "        return self.data_model, self.data_labels\n",
    "    \n",
    "    def add_column(self, column_name, column_content):\n",
    "        self.processed_Traindata.loc[:, column_name] = pd.Series(column_content, index=self.processed_Traindata.index)\n",
    "\n",
    "    def build_features(self):\n",
    "        def count_by_lambda(expression, word_array):\n",
    "            return len(list(filter(expression, word_array)))\n",
    "        \n",
    "        def count_occurences(character, word_array):\n",
    "            counter = 0\n",
    "            for j, word in enumerate(word_array):\n",
    "                for char in word:\n",
    "                    if char == character:\n",
    "                        counter += 1\n",
    "            return counter\n",
    "        \n",
    "        def count_interjections(wordArray):\n",
    "            interjections = []\n",
    "            interjectionCount = 0\n",
    "            with open('../data/interjections.txt') as file:\n",
    "                interjections = file.read().splitlines()\n",
    "            for word in wordArray:\n",
    "                if word in interjections:\n",
    "                    interjectionCount += 1\n",
    "            return interjectionCount \n",
    "\n",
    "        def count_by_regex(regex, plain_text):\n",
    "            return len(regex.findall(plain_text))\n",
    "        \n",
    "        self.add_column(\"splitted_text\", map(lambda txt: txt.split(\" \"), self.processed_Traindata[\"text\"]))\n",
    "        \n",
    "        #Number of uppercase words\n",
    "        uppercase = list(map(lambda txt: count_by_lambda(lambda word: word == word.upper(), txt),\n",
    "                                                        self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_uppercase\", uppercase)\n",
    "        \n",
    "        #number of !\n",
    "        exclamations = list(map(lambda txt: count_occurences(\"!\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_exclamation\", exclamations)\n",
    "        \n",
    "        #number of ?\n",
    "        questions = list(map(lambda txt: count_occurences(\"?\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_question\", questions)\n",
    "        \n",
    "        #number of ...\n",
    "        ellipsis = list(map(lambda txt: count_by_regex(regex.compile(r\"\\.\\s?\\.\\s?\\.\"), txt),\n",
    "                           self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_ellipsis\", ellipsis)\n",
    "        \n",
    "        #number of hashtags\n",
    "        hashtags = list(map(lambda txt: count_occurences(\"#\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_hashtags\", hashtags)\n",
    "        \n",
    "        #number of mentions\n",
    "        mentions = list(map(lambda txt: count_occurences(\"@\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_mentions\", mentions)\n",
    "        \n",
    "        #number of quotes\n",
    "        quotes = list(map(lambda plain_text: int(count_occurences(\"'\", [plain_text.strip(\"'\").strip('\"')]) / 2 +\n",
    "                                                 count_occurences('\"', [plain_text.strip(\"'\").strip('\"')]) / 2),\n",
    "                          self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_quotes\", quotes)\n",
    "        \n",
    "        #number of urls\n",
    "        urls = list(map(lambda txt: count_by_regex(regex.compile(r\"http.?://[^\\s]+[\\s]?\"), txt),\n",
    "                             self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_urls\", urls)\n",
    "        \n",
    "        #number of positive emoticons\n",
    "        ed = EmoticonDetector()\n",
    "        positive_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and ed.is_positive(word), txt), \n",
    "                   self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_positive_emo\", positive_emo)\n",
    "        \n",
    "        #number of negative emoticons\n",
    "        negative_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and not ed.is_positive(word), txt), \n",
    "                   self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_negative_emo\", negative_emo)\n",
    "        \n",
    "        #number of interjections\n",
    "        interjections = list(map(lambda txt: count_interjections(txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_interjections\", interjections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_uppercase</th>\n",
       "      <th>number_of_exclamation</th>\n",
       "      <th>number_of_question</th>\n",
       "      <th>number_of_ellipsis</th>\n",
       "      <th>number_of_hashtags</th>\n",
       "      <th>number_of_mentions</th>\n",
       "      <th>number_of_quotes</th>\n",
       "      <th>number_of_urls</th>\n",
       "      <th>number_of_positive_emo</th>\n",
       "      <th>number_of_negative_emo</th>\n",
       "      <th>...</th>\n",
       "      <th>gorgeou_bow</th>\n",
       "      <th>woohoo_bow</th>\n",
       "      <th>thousand_bow</th>\n",
       "      <th>understat_bow</th>\n",
       "      <th>furiou_bow</th>\n",
       "      <th>manual_bow</th>\n",
       "      <th>smell_bow</th>\n",
       "      <th>ber_bow</th>\n",
       "      <th>charleston_bow</th>\n",
       "      <th>nrt_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 2425 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_uppercase  number_of_exclamation  number_of_question  \\\n",
       "0                    0                      0                   0   \n",
       "\n",
       "   number_of_ellipsis  number_of_hashtags  number_of_mentions  \\\n",
       "0                   0                   0                   1   \n",
       "\n",
       "   number_of_quotes  number_of_urls  number_of_positive_emo  \\\n",
       "0                 0               0                       0   \n",
       "\n",
       "   number_of_negative_emo   ...     gorgeou_bow  woohoo_bow  thousand_bow  \\\n",
       "0                       0   ...               0           0             0   \n",
       "\n",
       "   understat_bow  furiou_bow  manual_bow  smell_bow  ber_bow  charleston_bow  \\\n",
       "0              0           0           0          0        0               0   \n",
       "\n",
       "   nrt_bow  \n",
       "0        0  \n",
       "\n",
       "[1 rows x 2425 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ExtraFeatures()\n",
    "data.initialize(\"../data/LDAOutput.csv\")\n",
    "data.build_features()\n",
    "data.cleaningData(DataPreprocessing())\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.buildWordlist()\n",
    "data_model, labels = data.build_data_model()\n",
    "\n",
    "data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Naive-Bayes Prediction :  ['negative']\n",
      "Random Forest Prediction :  ['negative']\n"
     ]
    }
   ],
   "source": [
    "#Load Naive-Bayes Library\n",
    "with open('../model/NaiveBayesClassifier.pkl', 'rb') as nid:\n",
    "    nb_loaded = cPickle.load(nid)\n",
    "with open('../model/RandomForestClassifier.pkl', 'rb') as rid:\n",
    "    rf_loaded = cPickle.load(rid)\n",
    "\n",
    "result_nb = nb_loaded.predict(data_model)\n",
    "print(type(result_nb))\n",
    "print(\"Naive-Bayes Prediction : \",result_nb)\n",
    "result_rf = rf_loaded.predict(data_model)\n",
    "print(\"Random Forest Prediction : \",result_rf)\n",
    "df_csv = pd.read_csv(\"../data/LDAOutput.csv\")\n",
    "df_csv['NaiveBayesSentiment'] = pd.DataFrame(result_nb)\n",
    "df_csv['RandomForestSentiment'] = pd.DataFrame(result_rf)\n",
    "\n",
    "df_csv.to_csv(\"../data/LDAOutput.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
