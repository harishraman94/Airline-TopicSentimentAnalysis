{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning:\n",
      "\n",
      "detected Windows; aliasing chunkize to chunkize_serial\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "#from emoticons import EmoticonDetector\n",
    "import re as regex\n",
    "import collections\n",
    "import numpy as np\n",
    "import plotly\n",
    "from plotly import graph_objs\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from time import time\n",
    "import gensim\n",
    "\n",
    "#plotly configuration\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    data = []\n",
    "    processed_Traindata = []\n",
    "    processed_Testdata = []\n",
    "    wordlist = []\n",
    "    \n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    \n",
    "    def initialize(self, csv_file, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            self.data_model = pd.read_csv(from_cached)\n",
    "            return\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file, usecols=[0,1,5,10,15])\n",
    "        train, test = train_test_split(self.data, test_size=0.2)\n",
    "        self.processed_Traindata = train\n",
    "        self.processed_Testdata = test\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11944</th>\n",
       "      <td>5.702890e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir - Please find my bag!! In Singapo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>5.688890e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir please....can I have the last ti...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>5.690000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways It says to call. Before connecting,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>5.680720e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir Ahhhh! Sorry, just followed.</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10722</th>\n",
       "      <td>5.689400e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways / @AmericanAir are incompetent.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10898</th>\n",
       "      <td>5.687930e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways Thank you for valuing my feedback. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6908</th>\n",
       "      <td>5.700410e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue Are all of your flights out of Charle...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8172</th>\n",
       "      <td>5.686360e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue I'm sitting on the plane. Too many pr...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>5.699220e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united there are a lot of unhappy cold people...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4545</th>\n",
       "      <td>5.700420e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir crew on flight 206 is awesome! T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7306</th>\n",
       "      <td>5.696750e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue the amount of money I spent on hotels...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5580</th>\n",
       "      <td>5.688840e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir   Happily, flight 1625 was not d...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4323</th>\n",
       "      <td>5.675950e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united and don't hope for me having a nicer f...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5870</th>\n",
       "      <td>5.685310e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir @Imaginedragons please help me a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>5.696670e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united strikes again! Why board anyone if the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>5.699480e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Delta</td>\n",
       "      <td>I hate you all. RT @JetBlue: Our fleet's on fl...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>5.698850e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united my PQMs and PQDs are no longer showing...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>5.695720e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united I'm not sure how you can help. Your fl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>5.694160e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>United</td>\n",
       "      <td>@united what's with the layover in Canada from...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>5.681660e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>@united thank you! Love united!! Have 4 flight...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5.700000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica Will flights be leaving Dallas ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14395</th>\n",
       "      <td>5.696220e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir that's not what I asked :)) I'm l...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>5.696810e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>@united They let us know in advance of the reb...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088</th>\n",
       "      <td>5.699390e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Delta</td>\n",
       "      <td>ðŸ˜’ \"@JetBlue: Our fleet's on fleek. http://t.co...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6934</th>\n",
       "      <td>5.700080e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Delta</td>\n",
       "      <td>Alright... Someone has to stop this! RT @JetBl...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>5.700700e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir yes....twice...for two different ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>5.688210e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united uh - I booked it through the UA websit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13408</th>\n",
       "      <td>5.698700e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir I need to be back in Cleveland as...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7115</th>\n",
       "      <td>5.699330e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Delta</td>\n",
       "      <td>can you not? RT @JetBlue Our fleet's on fleek....</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5095</th>\n",
       "      <td>5.693480e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir are you hiring for flight attend...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5077</th>\n",
       "      <td>5.693630e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir better travel photos:\\nMy Kindle...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10168</th>\n",
       "      <td>5.695100e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways Thank you glad to be home. There we...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14177</th>\n",
       "      <td>5.696580e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir I'm on 1024 into Dallas. Do I hav...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>5.696300e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue cool! Are there stairs at JFK? I can ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>5.685740e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united I'm confused. After your @Dulles_Airpo...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12594</th>\n",
       "      <td>5.700950e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir - you broke my sick wife's luggag...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14368</th>\n",
       "      <td>5.696270e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir most horrendous service ever !! O...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>5.702080e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways nope, they just announced we're hea...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14359</th>\n",
       "      <td>5.696280e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir 3231DTW to LAG at 4:45. Flight Ca...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>5.682830e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united  very disappointed by the service star...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>5.681150e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united every time I check a bag with your air...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>5.679280e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united, wtf is your username and/or email add...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11604</th>\n",
       "      <td>5.678900e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways Thank you!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12114</th>\n",
       "      <td>5.702850e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir what's the status of flight 1357 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9123</th>\n",
       "      <td>5.702040e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways what's wrong with the plane? Flt 581</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>5.696850e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir Tough I can take. Zero meaningfu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12092</th>\n",
       "      <td>5.702960e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir thanks keep me updated just hope ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11611</th>\n",
       "      <td>5.678810e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways thanks! It's hectic for everyone bu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2510</th>\n",
       "      <td>5.691530e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united I sent my details to the customer care...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6523</th>\n",
       "      <td>5.678030e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir had to rebook through Atlanta &amp;a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13707</th>\n",
       "      <td>5.697400e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir Finally got a call from AA, but i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8226</th>\n",
       "      <td>5.685890e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue that's what they told me half an hour...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14293</th>\n",
       "      <td>5.696410e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir a phone call or email when you Ca...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11439</th>\n",
       "      <td>5.681140e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways I followed you complaint procedure ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3560</th>\n",
       "      <td>5.682690e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united I have submitted my complaint. I need ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10238</th>\n",
       "      <td>5.694690e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways truly been the worst experience thi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3124</th>\n",
       "      <td>5.686540e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united well. As of yet, our checked bag has a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11496</th>\n",
       "      <td>5.680700e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways through, can you confirm that I hav...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13268</th>\n",
       "      <td>5.699010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir still no response from AA. great ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>5.682290e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united can you DM me? Been on hold with custo...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11712 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id airline_sentiment         airline  \\\n",
       "11944  5.702890e+17          negative        American   \n",
       "5570   5.688890e+17           neutral       Southwest   \n",
       "10657  5.690000e+17          negative      US Airways   \n",
       "6302   5.680720e+17           neutral       Southwest   \n",
       "10722  5.689400e+17          negative      US Airways   \n",
       "10898  5.687930e+17           neutral      US Airways   \n",
       "6908   5.700410e+17           neutral           Delta   \n",
       "8172   5.686360e+17          negative           Delta   \n",
       "1118   5.699220e+17          negative          United   \n",
       "4545   5.700420e+17          positive       Southwest   \n",
       "7306   5.696750e+17          negative           Delta   \n",
       "5580   5.688840e+17          positive       Southwest   \n",
       "4323   5.675950e+17          negative          United   \n",
       "5870   5.685310e+17           neutral       Southwest   \n",
       "1551   5.696670e+17          negative          United   \n",
       "7045   5.699480e+17          negative           Delta   \n",
       "1211   5.698850e+17          negative          United   \n",
       "1814   5.695720e+17          negative          United   \n",
       "2059   5.694160e+17           neutral          United   \n",
       "3729   5.681660e+17          positive          United   \n",
       "54     5.700000e+17           neutral  Virgin America   \n",
       "14395  5.696220e+17          negative        American   \n",
       "1528   5.696810e+17          positive          United   \n",
       "7088   5.699390e+17           neutral           Delta   \n",
       "6934   5.700080e+17           neutral           Delta   \n",
       "12663  5.700700e+17           neutral        American   \n",
       "2942   5.688210e+17          negative          United   \n",
       "13408  5.698700e+17           neutral        American   \n",
       "7115   5.699330e+17          negative           Delta   \n",
       "5095   5.693480e+17           neutral       Southwest   \n",
       "...             ...               ...             ...   \n",
       "5077   5.693630e+17           neutral       Southwest   \n",
       "10168  5.695100e+17          negative      US Airways   \n",
       "14177  5.696580e+17           neutral        American   \n",
       "7385   5.696300e+17          positive           Delta   \n",
       "3243   5.685740e+17          negative          United   \n",
       "12594  5.700950e+17          negative        American   \n",
       "14368  5.696270e+17          negative        American   \n",
       "9118   5.702080e+17          negative      US Airways   \n",
       "14359  5.696280e+17          negative        American   \n",
       "3536   5.682830e+17          negative          United   \n",
       "3800   5.681150e+17          negative          United   \n",
       "3947   5.679280e+17          negative          United   \n",
       "11604  5.678900e+17          positive      US Airways   \n",
       "12114  5.702850e+17           neutral        American   \n",
       "9123   5.702040e+17          negative      US Airways   \n",
       "4842   5.696850e+17          negative       Southwest   \n",
       "12092  5.702960e+17          positive        American   \n",
       "11611  5.678810e+17          positive      US Airways   \n",
       "2510   5.691530e+17          negative          United   \n",
       "6523   5.678030e+17          negative       Southwest   \n",
       "13707  5.697400e+17          negative        American   \n",
       "8226   5.685890e+17          negative           Delta   \n",
       "14293  5.696410e+17          negative        American   \n",
       "11439  5.681140e+17          negative      US Airways   \n",
       "3560   5.682690e+17          negative          United   \n",
       "10238  5.694690e+17          negative      US Airways   \n",
       "3124   5.686540e+17          negative          United   \n",
       "11496  5.680700e+17           neutral      US Airways   \n",
       "13268  5.699010e+17          negative        American   \n",
       "3623   5.682290e+17          negative          United   \n",
       "\n",
       "                                                    text  topic_label  \n",
       "11944  @AmericanAir - Please find my bag!! In Singapo...            4  \n",
       "5570   @SouthwestAir please....can I have the last ti...            5  \n",
       "10657  @USAirways It says to call. Before connecting,...            8  \n",
       "6302          @SouthwestAir Ahhhh! Sorry, just followed.            8  \n",
       "10722         @USAirways / @AmericanAir are incompetent.            0  \n",
       "10898  @USAirways Thank you for valuing my feedback. ...            0  \n",
       "6908   @JetBlue Are all of your flights out of Charle...            8  \n",
       "8172   @JetBlue I'm sitting on the plane. Too many pr...            8  \n",
       "1118   @united there are a lot of unhappy cold people...            2  \n",
       "4545   @SouthwestAir crew on flight 206 is awesome! T...            0  \n",
       "7306   @JetBlue the amount of money I spent on hotels...            4  \n",
       "5580   @SouthwestAir   Happily, flight 1625 was not d...            6  \n",
       "4323   @united and don't hope for me having a nicer f...            2  \n",
       "5870   @SouthwestAir @Imaginedragons please help me a...            1  \n",
       "1551   @united strikes again! Why board anyone if the...            4  \n",
       "7045   I hate you all. RT @JetBlue: Our fleet's on fl...            5  \n",
       "1211   @united my PQMs and PQDs are no longer showing...            9  \n",
       "1814   @united I'm not sure how you can help. Your fl...            2  \n",
       "2059   @united what's with the layover in Canada from...            2  \n",
       "3729   @united thank you! Love united!! Have 4 flight...            4  \n",
       "54     @VirginAmerica Will flights be leaving Dallas ...            8  \n",
       "14395  @AmericanAir that's not what I asked :)) I'm l...            4  \n",
       "1528   @united They let us know in advance of the reb...            6  \n",
       "7088   ðŸ˜’ \"@JetBlue: Our fleet's on fleek. http://t.co...            5  \n",
       "6934   Alright... Someone has to stop this! RT @JetBl...            5  \n",
       "12663  @AmericanAir yes....twice...for two different ...            4  \n",
       "2942   @united uh - I booked it through the UA websit...            1  \n",
       "13408  @AmericanAir I need to be back in Cleveland as...            8  \n",
       "7115   can you not? RT @JetBlue Our fleet's on fleek....            5  \n",
       "5095   @SouthwestAir are you hiring for flight attend...            7  \n",
       "...                                                  ...          ...  \n",
       "5077   @SouthwestAir better travel photos:\\nMy Kindle...            5  \n",
       "10168  @USAirways Thank you glad to be home. There we...            9  \n",
       "14177  @AmericanAir I'm on 1024 into Dallas. Do I hav...            3  \n",
       "7385   @JetBlue cool! Are there stairs at JFK? I can ...            2  \n",
       "3243   @united I'm confused. After your @Dulles_Airpo...            7  \n",
       "12594  @AmericanAir - you broke my sick wife's luggag...            9  \n",
       "14368  @AmericanAir most horrendous service ever !! O...            4  \n",
       "9118   @USAirways nope, they just announced we're hea...            6  \n",
       "14359  @AmericanAir 3231DTW to LAG at 4:45. Flight Ca...            4  \n",
       "3536   @united  very disappointed by the service star...            9  \n",
       "3800   @united every time I check a bag with your air...            9  \n",
       "3947   @united, wtf is your username and/or email add...            4  \n",
       "11604                              @USAirways Thank you!            0  \n",
       "12114  @AmericanAir what's the status of flight 1357 ...            1  \n",
       "9123     @USAirways what's wrong with the plane? Flt 581            6  \n",
       "4842   @SouthwestAir Tough I can take. Zero meaningfu...            1  \n",
       "12092  @AmericanAir thanks keep me updated just hope ...            7  \n",
       "11611  @USAirways thanks! It's hectic for everyone bu...            0  \n",
       "2510   @united I sent my details to the customer care...            0  \n",
       "6523   @SouthwestAir had to rebook through Atlanta &a...            4  \n",
       "13707  @AmericanAir Finally got a call from AA, but i...            0  \n",
       "8226   @JetBlue that's what they told me half an hour...            8  \n",
       "14293  @AmericanAir a phone call or email when you Ca...            8  \n",
       "11439  @USAirways I followed you complaint procedure ...            2  \n",
       "3560   @united I have submitted my complaint. I need ...            7  \n",
       "10238  @USAirways truly been the worst experience thi...            4  \n",
       "3124   @united well. As of yet, our checked bag has a...            4  \n",
       "11496  @USAirways through, can you confirm that I hav...            9  \n",
       "13268  @AmericanAir still no response from AA. great ...            0  \n",
       "3623   @united can you DM me? Been on hold with custo...            8  \n",
       "\n",
       "[11712 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"../data/Tweets.csv\")\n",
    "data.processed_Traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "class DataPreprocessing:\n",
    "    def iterate(self):\n",
    "        for preprocessingMethod in [self.removeUrls,\n",
    "                                   self.removeUsernames,\n",
    "                                   self.removeElongatedWords,\n",
    "                                   self.removeNa,\n",
    "                                   self.replaceSlangWords,\n",
    "                                   self.removeSpecialChars,\n",
    "                                   self.removeNumbers]:\n",
    "            yield preprocessingMethod\n",
    "    \n",
    "    @staticmethod\n",
    "    def removeByRegex(tweets, regExp):\n",
    "        tweets.loc[:, \"text\"].replace(regExp, \"\", inplace=True)\n",
    "        return tweets\n",
    "    \n",
    "    def removeUrls(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "    \n",
    "    def removeNa(self, tweets):\n",
    "        return tweets[tweets[\"text\"] != \"\"]\n",
    "    \n",
    "    def removeSpecialChars(self, tweets):\n",
    "        for remove in map(lambda r: regex.compile(regex.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\"\n",
    "                                                                    \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                    \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                    \"!\", \"?\", \".\", \"'\",\n",
    "                                                                    \"--\", \"---\", \"#\"]):\n",
    "            tweets.loc[:, \"text\"].replace(remove, \"\", inplace=True)\n",
    "        return tweets\n",
    "    \n",
    "    def removeUsernames(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "    \n",
    "    def removeElongatedWords(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"(.)\\1+', r'\\1\\1\"))\n",
    "    \n",
    "    def removeNumbers(self, tweets):\n",
    "        #print(tweets)\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))\n",
    "    \n",
    "    def replaceSlangWords(self, tweets):\n",
    "        with open('../data/slang.txt') as file:\n",
    "            slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "            for line in file if line.strip())\n",
    "            #print(tweets[\"text\"])\n",
    "            #print(\"-----------------------------------------END\")\n",
    "            for index,word in tweets['text'].iteritems():\n",
    "                #print(index)\n",
    "                for i in word.split():\n",
    "                    isUpperCase = i.isupper()\n",
    "                    i = i.lower()\n",
    "                    if i in slang_map.keys():\n",
    "                        word = word.replace(i, slang_map[i])\n",
    "                        tweets.loc[(index),\"text\"] = word\n",
    "                if isUpperCase:\n",
    "                    i = i.upper()\n",
    "        #print(tweets.loc[:,\"text\"])\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning the Training Data\n",
    "class CleanTrainingData(TwitterData_Initialize):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        #self.processed_Testdata = previous.processed_Testdata\n",
    "    \n",
    "    def cleaningData(self, cleaner):\n",
    "        train = self.processed_Traindata\n",
    "        #test = self.processed_Testdata\n",
    "        \n",
    "        for cleanerMethod in cleaner.iterate():\n",
    "            train = cleanerMethod(train)\n",
    "            #test = cleanerMethod(test)\n",
    "        self.processed_Traindata = train\n",
    "        #self.processed_Testdata = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11944</th>\n",
       "      <td>5.702890e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>Please find my bag In Singapore for three day...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>5.688890e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>pleasecan I have the last tickets for me and m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>5.690000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>It says to call Before connecting get song dan...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302</th>\n",
       "      <td>5.680720e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>Ahhhh Sorry just followed</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10722</th>\n",
       "      <td>5.689400e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>are incompetent</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id airline_sentiment     airline  \\\n",
       "11944  5.702890e+17          negative    American   \n",
       "5570   5.688890e+17           neutral   Southwest   \n",
       "10657  5.690000e+17          negative  US Airways   \n",
       "6302   5.680720e+17           neutral   Southwest   \n",
       "10722  5.689400e+17          negative  US Airways   \n",
       "\n",
       "                                                    text  topic_label  \n",
       "11944   Please find my bag In Singapore for three day...            4  \n",
       "5570   pleasecan I have the last tickets for me and m...            5  \n",
       "10657  It says to call Before connecting get song dan...            8  \n",
       "6302                           Ahhhh Sorry just followed            8  \n",
       "10722                                    are incompetent            0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CleanTrainingData(data)\n",
    "data.cleaningData(DataPreprocessing())\n",
    "data.processed_Traindata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenizing and Stemming the data\n",
    "class TokenizationStemming(CleanTrainingData):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        #self.processed_TestData = previous.processed_TestData\n",
    "    \n",
    "    def stem(self, stemmer = nltk.PorterStemmer()):\n",
    "        def stemJoin(row):\n",
    "            row[\"text\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"text\"]))\n",
    "            return row\n",
    "    \n",
    "        self.processed_Traindata = self.processed_Traindata.apply(stemJoin, axis=1)\n",
    "    \n",
    "    def tokenize(self, tokenizer = nltk.word_tokenize):\n",
    "        def tokenizeRow(row):\n",
    "            row[\"text\"] = tokenizer(row[\"text\"])\n",
    "            row[\"tokenizedText\"] = [] + row[\"text\"]\n",
    "            return row\n",
    "        \n",
    "        self.processed_Traindata = self.processed_Traindata.apply(tokenizeRow, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_label</th>\n",
       "      <th>tokenizedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>5.688190e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>[i, wa, not, look, for, the, fare, to, be, ret...</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, was, not, looking, for, the, fare, to, be,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>5.677670e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>[it, and, no, power, outlet, at, the, seat, on...</td>\n",
       "      <td>8</td>\n",
       "      <td>[its, and, no, power, outlets, at, the, seats,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3895</th>\n",
       "      <td>5.680380e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>[sorri, to, hear, outsourc, plan, bois, is, be...</td>\n",
       "      <td>7</td>\n",
       "      <td>[sorry, to, hear, outsourcing, plan, Boise, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14373</th>\n",
       "      <td>5.696250e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>[that, unaccept, they, should, allow, me, to, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[thats, unacceptable, They, should, allow, me,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9961</th>\n",
       "      <td>5.696000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>[it, pretti, ridicul, that, at, phx, sky, harb...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Its, pretty, ridiculous, that, at, PHX, sky, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id airline_sentiment     airline  \\\n",
       "2946   5.688190e+17          negative      United   \n",
       "4225   5.677670e+17          negative      United   \n",
       "3895   5.680380e+17          positive      United   \n",
       "14373  5.696250e+17          negative    American   \n",
       "9961   5.696000e+17          negative  US Airways   \n",
       "\n",
       "                                                    text  topic_label  \\\n",
       "2946   [i, wa, not, look, for, the, fare, to, be, ret...            1   \n",
       "4225   [it, and, no, power, outlet, at, the, seat, on...            8   \n",
       "3895   [sorri, to, hear, outsourc, plan, bois, is, be...            7   \n",
       "14373  [that, unaccept, they, should, allow, me, to, ...            1   \n",
       "9961   [it, pretti, ridicul, that, at, phx, sky, harb...            4   \n",
       "\n",
       "                                           tokenizedText  \n",
       "2946   [I, was, not, looking, for, the, fare, to, be,...  \n",
       "4225   [its, and, no, power, outlets, at, the, seats,...  \n",
       "3895   [sorry, to, hear, outsourcing, plan, Boise, is...  \n",
       "14373  [thats, unacceptable, They, should, allow, me,...  \n",
       "9961   [Its, pretty, ridiculous, that, at, PHX, sky, ...  "
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TokenizationStemming(data)\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.processed_Traindata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 6898), ('the', 4837), ('i', 4340), ('flight', 3822), ('a', 3557)]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building Wordlist\n",
    "#Un-filtered version without removing stopwords\n",
    "words = collections.Counter()\n",
    "for idx in data.processed_Traindata.index:\n",
    "    words.update(data.processed_Traindata.loc[idx, \"text\"])\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flight', 3822), ('thank', 1367), ('wa', 1287), ('get', 1287), ('not', 1284)]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "whitelist = [\"n't\", \"not\"]\n",
    "for idx, stop_word in enumerate(stopwords):\n",
    "    if stop_word not in whitelist:\n",
    "        del words[stop_word]\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating the final wordlist\n",
    "class WordList(TokenizationStemming):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "    \n",
    "    whitelist = [\"n't\", \"not\"]\n",
    "    wordlist = []\n",
    "    \n",
    "    def buildWordlist(self, min_occurrences=3, max_occurences=3000, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                     whitelist=None):\n",
    "        self.wordlist = []\n",
    "        whitelist = self.whitelist if whitelist is None else whitelist\n",
    "        import os\n",
    "        if os.path.isfile('../data/wordlist.csv'):\n",
    "            word_df = pd.read_csv('../data/wordlist.csv', encoding = \"ISO-8859-1\")\n",
    "            word_df = word_df[word_df[\"occurrences\"] > min_occurrences]\n",
    "            self.wordlist = list(word_df.loc[:, \"word\"])\n",
    "            return\n",
    "        words = collections.Counter()\n",
    "        for idx in self.processed_Traindata.index:\n",
    "            words.update(self.processed_Traindata.loc[idx, \"text\"])\n",
    "        \n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "        \n",
    "        word_df = pd.DataFrame(data={\"word\" : [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                    \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                              columns = [\"word\", \"occurrences\"])\n",
    "        \n",
    "        word_df.to_csv(\"../data/wordlist.csv\", index_label=\"idx\", encoding = \"utf8\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = WordList(data)\n",
    "data.buildWordlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transforming into Bag-of-Words\n",
    "class BagOfWords(WordList):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        self.wordlist = previous.wordlist\n",
    "    \n",
    "    def buildDataModel(self):\n",
    "        labelColumn = [\"label\"]\n",
    "        columns = labelColumn + list(\n",
    "            map(lambda w: w + \"_bow\", self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        \n",
    "        for idx in self.processed_Traindata.index:\n",
    "            currentRow = []\n",
    "            currentLabel = self.processed_Traindata.loc[idx, \"airline_sentiment\"]\n",
    "            labels.append(currentLabel)\n",
    "            currentRow.append(currentLabel)\n",
    "            \n",
    "            tokens = set(self.processed_Traindata.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                currentRow.append(1 if word in tokens else 0)\n",
    "            \n",
    "            rows.append(currentRow)\n",
    "        \n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        \n",
    "        return self.data_model, self.data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>hour_bow</th>\n",
       "      <th>cancel_bow</th>\n",
       "      <th>help_bow</th>\n",
       "      <th>servic_bow</th>\n",
       "      <th>delay_bow</th>\n",
       "      <th>time_bow</th>\n",
       "      <th>custom_bow</th>\n",
       "      <th>bag_bow</th>\n",
       "      <th>call_bow</th>\n",
       "      <th>...</th>\n",
       "      <th>gorgeou_bow</th>\n",
       "      <th>woohoo_bow</th>\n",
       "      <th>thousand_bow</th>\n",
       "      <th>understat_bow</th>\n",
       "      <th>furiou_bow</th>\n",
       "      <th>manual_bow</th>\n",
       "      <th>smell_bow</th>\n",
       "      <th>ber_bow</th>\n",
       "      <th>charleston_bow</th>\n",
       "      <th>nrt_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2415 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  hour_bow  cancel_bow  help_bow  servic_bow  delay_bow  time_bow  \\\n",
       "0  negative         0           0         0           0          0         0   \n",
       "1   neutral         0           0         0           0          0         0   \n",
       "2   neutral         1           0         0           0          0         0   \n",
       "3  positive         0           0         0           0          0         0   \n",
       "4  negative         0           0         1           0          0         0   \n",
       "\n",
       "   custom_bow  bag_bow  call_bow   ...     gorgeou_bow  woohoo_bow  \\\n",
       "0           0        0         0   ...               0           0   \n",
       "1           0        0         0   ...               0           0   \n",
       "2           0        0         0   ...               0           0   \n",
       "3           0        0         0   ...               0           0   \n",
       "4           0        0         0   ...               0           0   \n",
       "\n",
       "   thousand_bow  understat_bow  furiou_bow  manual_bow  smell_bow  ber_bow  \\\n",
       "0             0              0           0           0          0        0   \n",
       "1             0              0           0           0          0        0   \n",
       "2             0              0           0           0          0        0   \n",
       "3             0              0           0           0          0        0   \n",
       "4             0              0           0           0          0        0   \n",
       "\n",
       "   charleston_bow  nrt_bow  \n",
       "0               0        0  \n",
       "1               0        0  \n",
       "2               0        0  \n",
       "3               0        0  \n",
       "4               0        0  \n",
       "\n",
       "[5 rows x 2415 columns]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = BagOfWords(data)\n",
    "bow, labels = data.buildDataModel()\n",
    "bow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 666\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Utility function to train the classifier and show F1, Precision, recall and Accuracy values\n",
    "\n",
    "def test_classifier(X_train, y_train, X_test, y_test, classifier):\n",
    "    log(\"\")\n",
    "    log(\"==================================================\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    log(\"Testing \" + classifier_name)\n",
    "    now = time()\n",
    "    list_of_labels = sorted(list(set(y_train)))\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    log(\"Learning time {0}s\".format(time() - now))\n",
    "    now = time()\n",
    "    predictions = model.predict(X_test)\n",
    "    log(\"Predicting time {0}s\".format(time() - now))\n",
    "    \n",
    "    precision = precision_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    recall = recall_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    log(\"==================Results=======================\")\n",
    "    log(\"            Negative     Neutral    Positive\")\n",
    "    log(\"F1         \" + str(f1))\n",
    "    log(\"Precision  \" + str(precision))\n",
    "    log(\"Recall     \" + str(recall))\n",
    "    log(\"Accuracy   \" + str(accuracy))\n",
    "    log(\"================================================\")\n",
    "    \n",
    "    return precision, recall, accuracy, f1\n",
    "\n",
    "def log(x):\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing BernoulliNB\n",
      "Learning time 0.8397293090820312s\n",
      "Predicting time 0.2986288070678711s\n",
      "==================Results=======================\n",
      "            Negative     Neutral    Positive\n",
      "F1         [0.8519774  0.57836066 0.58070501]\n",
      "Precision  [0.84340045 0.57198444 0.61614173]\n",
      "Recall     [0.86073059 0.58488064 0.54912281]\n",
      "Accuracy   0.750996015936255\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "#Classifier : BagOfWords + NaiveBayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:, 1:], bow.iloc[:, 0],\n",
    "                                                   train_size = 0.7, stratify=bow.iloc[:, 0],\n",
    "                                                   random_state = seed)\n",
    "\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NaiveBayes with 8 fold cross-validation\n",
    "\n",
    "def cv(classifier, X_train, y_train):\n",
    "    log(\"===============================================\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    now = time()\n",
    "    log(\"Crossvalidating \" + classifier_name + \"...\")\n",
    "    accuracy = [cross_val_score(classifier, X_train, y_train, cv=8, n_jobs=-1)]\n",
    "    log(\"Crosvalidation completed in {0}s\".format(time() - now))\n",
    "    log(\"Accuracy: \" + str(accuracy[0]))\n",
    "    log(\"Average accuracy: \" + str(np.array(accuracy[0]).mean()))\n",
    "    log(\"===============================================\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Crossvalidating BernoulliNB...\n",
      "Crosvalidation completed in 21.083032608032227s\n",
      "Accuracy: [0.77883959 0.74539249 0.76382253 0.74726776 0.74795082 0.76281613\n",
      " 0.75393028 0.75529733]\n",
      "Average accuracy: 0.7569146165589327\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "nb_acc = cv(BernoulliNB(), bow.iloc[:,1:], bow.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Addtion of extra features:\n",
    "\n",
    "# Number of Uppercase - tend to express postive/negative emotions by using uppercase words\n",
    "# Number of !         - exclamation marks are likely to increase strength of opinion\n",
    "# Number of ?         - might distinguish neutral tweets - seeking information\n",
    "# Number of positive  - positive emoji will most likely occur in positive tweets\n",
    "# emoticons\n",
    "# Number of negative  - Inverse to the one above\n",
    "# emoticons\n",
    "# Number of ...       - commonly used in commenting something\n",
    "# Number of quotations- same as above\n",
    "# Number of mentions  - Lots of mentions on positive tweets, to share something good/bad\n",
    "# Number of urls      - similar to number of mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Detecting Emoticons\n",
    "class EmoticonDetector:\n",
    "    emoticons = {}\n",
    "    \n",
    "    def __init__(self, emoticon_file=\"../data/emoticons.txt\"):\n",
    "        from pathlib import Path\n",
    "        content = Path(emoticon_file).read_text()\n",
    "        positive = True\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if \"positive\" in line.lower():\n",
    "                positive = True\n",
    "                continue\n",
    "            elif \"negative\" in line.lower():\n",
    "                positive = False\n",
    "                continue\n",
    "            \n",
    "            self.emoticons[line] = positive\n",
    "    \n",
    "    def is_positive(self, emoticon):\n",
    "        if emoticon in self.emoticons:\n",
    "            return self.emoticons[emoticon]\n",
    "        return False\n",
    "    \n",
    "    def is_emoticon(self, to_check):\n",
    "        return to_check in self.emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtraFeatures(WordList):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_data_model(self):\n",
    "        extra_columns = [col for col in self.processed_Traindata.columns if col.startswith(\"number_of\")]\n",
    "        label_column = [\"label\"]\n",
    "        columns = label_column + extra_columns + list(\n",
    "                map(lambda w: w + \"_bow\", self.wordlist))\n",
    "        \n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_Traindata.index:\n",
    "            current_row = []\n",
    "            current_label = self.processed_Traindata.loc[idx, \"airline_sentiment\"]\n",
    "            labels.append(current_label)\n",
    "            current_row.append(current_label)\n",
    "        \n",
    "            for _,col in enumerate(extra_columns):\n",
    "                current_row.append(self.processed_Traindata.loc[idx, col])\n",
    "        \n",
    "        #adding bad-of-words\n",
    "            tokens = set(self.processed_Traindata.loc[idx, \"text\"])\n",
    "            for _,word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "        \n",
    "            rows.append(current_row)\n",
    "        \n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        \n",
    "        return self.data_model, self.data_labels\n",
    "    \n",
    "    def add_column(self, column_name, column_content):\n",
    "        self.processed_Traindata.loc[:, column_name] = pd.Series(column_content, index=self.processed_Traindata.index)\n",
    "\n",
    "    def build_features(self):\n",
    "        def count_by_lambda(expression, word_array):\n",
    "            return len(list(filter(expression, word_array)))\n",
    "        \n",
    "        def count_occurences(character, word_array):\n",
    "            counter = 0\n",
    "            for j, word in enumerate(word_array):\n",
    "                for char in word:\n",
    "                    if char == character:\n",
    "                        counter += 1\n",
    "            return counter\n",
    "        \n",
    "        def count_by_regex(regex, plain_text):\n",
    "            return len(regex.findall(plain_text))\n",
    "        \n",
    "        self.add_column(\"splitted_text\", map(lambda txt: txt.split(\" \"), self.processed_Traindata[\"text\"]))\n",
    "        \n",
    "        #Number of uppercase words\n",
    "        uppercase = list(map(lambda txt: count_by_lambda(lambda word: word == word.upper(), txt),\n",
    "                                                        self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_uppercase\", uppercase)\n",
    "        \n",
    "        #number of !\n",
    "        exclamations = list(map(lambda txt: count_occurences(\"!\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_exclamation\", exclamations)\n",
    "        \n",
    "        #number of ?\n",
    "        questions = list(map(lambda txt: count_occurences(\"?\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_question\", questions)\n",
    "        \n",
    "        #number of ...\n",
    "        ellipsis = list(map(lambda txt: count_by_regex(regex.compile(r\"\\.\\s?\\.\\s?\\.\"), txt),\n",
    "                           self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_ellipsis\", ellipsis)\n",
    "        \n",
    "        #number of hashtags\n",
    "        hashtags = list(map(lambda txt: count_occurences(\"#\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_hashtags\", hashtags)\n",
    "        \n",
    "        #number of mentions\n",
    "        mentions = list(map(lambda txt: count_occurences(\"@\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_mentions\", mentions)\n",
    "        \n",
    "        #number of quotes\n",
    "        quotes = list(map(lambda plain_text: int(count_occurences(\"'\", [plain_text.strip(\"'\").strip('\"')]) / 2 +\n",
    "                                                 count_occurences('\"', [plain_text.strip(\"'\").strip('\"')]) / 2),\n",
    "                          self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_quotes\", quotes)\n",
    "        \n",
    "        #number of urls\n",
    "        urls = list(map(lambda txt: count_by_regex(regex.compile(r\"http.?://[^\\s]+[\\s]?\"), txt),\n",
    "                             self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_urls\", urls)\n",
    "        \n",
    "        #number of positive emoticons\n",
    "        ed = EmoticonDetector()\n",
    "        positive_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and ed.is_positive(word), txt), \n",
    "                   self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_positive_emo\", positive_emo)\n",
    "        \n",
    "        #number of negative emoticons\n",
    "        negative_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and not ed.is_positive(word), txt), \n",
    "                   self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_negative_emo\", negative_emo)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:337: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3813: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = ExtraFeatures()\n",
    "data.initialize(\"../data/Tweets.csv\")\n",
    "data.build_features()\n",
    "data.cleaningData(DataPreprocessing())\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.buildWordlist()\n",
    "data_model, labels = data.build_data_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing RandomForestClassifier\n",
      "Learning time 20.246517419815063s\n",
      "Predicting time 0.4646904468536377s\n",
      "==================Results=======================\n",
      "            Negative     Neutral    Positive\n",
      "F1         [0.84963691 0.53918495 0.62803738]\n",
      "Precision  [0.80558931 0.6405959  0.66141732]\n",
      "Recall     [0.89877994 0.46549391 0.59786477]\n",
      "Accuracy   0.7595332953898691\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "#Extended Features + Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_model.iloc[:, 1:], data_model.iloc[:, 0],\n",
    "                                                    train_size=0.7, stratify=data_model.iloc[:, 0],\n",
    "                                                    random_state=seed)\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, RandomForestClassifier(random_state=seed,n_estimators=403,n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Crossvalidating RandomForestClassifier...\n",
      "Crosvalidation completed in 235.91511368751526s\n",
      "Accuracy: [0.7774744  0.77663934 0.76639344 0.7670765  0.76571038 0.73019126\n",
      " 0.75546448 0.7518797 ]\n",
      "Average accuracy: 0.7613536889768202\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "#Crosvalidation\n",
    "rf_acc = cv(RandomForestClassifier(n_estimators=403,n_jobs=-1, random_state=seed),data_model.iloc[:, 1:], data_model.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing BernoulliNB\n",
      "Learning time 1.0232014656066895s\n",
      "Predicting time 0.3078019618988037s\n",
      "==================Results=======================\n",
      "            Negative     Neutral    Positive\n",
      "F1         [0.8519774  0.57836066 0.58070501]\n",
      "Precision  [0.84340045 0.57198444 0.61614173]\n",
      "Recall     [0.86073059 0.58488064 0.54912281]\n",
      "Accuracy   0.750996015936255\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:, 1:], bow.iloc[:, 0],\n",
    "                                                   train_size = 0.7, stratify=bow.iloc[:, 0],\n",
    "                                                   random_state = seed)\n",
    "\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, BernoulliNB())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
