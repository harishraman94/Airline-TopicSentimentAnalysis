{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "#from emoticons import EmoticonDetector\n",
    "import re as regex\n",
    "import collections\n",
    "import numpy as np\n",
    "import plotly\n",
    "from plotly import graph_objs\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from time import time\n",
    "import gensim\n",
    "\n",
    "#plotly configuration\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    data = []\n",
    "    processed_Traindata = []\n",
    "    processed_Testdata = []\n",
    "    wordlist = []\n",
    "    \n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    \n",
    "    def initialize(self, csv_file, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            self.data_model = pd.read_csv(from_cached)\n",
    "            return\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file, usecols=[0,1,5,10,15])\n",
    "        train, test = train_test_split(self.data, test_size=0.2)\n",
    "        self.processed_Traindata = train\n",
    "        self.processed_Testdata = test\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>5.688190e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united I was not looking for the fare to be r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>5.677670e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united its 2015 and no power outlets at the s...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3895</th>\n",
       "      <td>5.680380e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>@united sorry to hear outsourcing plan. Boise ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14373</th>\n",
       "      <td>5.696250e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir that's unacceptable. They should ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9961</th>\n",
       "      <td>5.696000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways Its pretty ridiculous that at PHX s...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2546</th>\n",
       "      <td>5.690980e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>United</td>\n",
       "      <td>@united lets do that can you email boarding pass?</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6143</th>\n",
       "      <td>5.681830e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir thx - fingers crossed they are f...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>5.692360e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>United</td>\n",
       "      <td>@united I'm flying UA but *G with A3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3279</th>\n",
       "      <td>5.685580e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>@united Fair enough. I don't usually rant, but...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5679</th>\n",
       "      <td>5.688130e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir can you show me some luv?Its 9 d...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488</th>\n",
       "      <td>5.689490e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir @SwagglikeBean take me here</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14507</th>\n",
       "      <td>5.696060e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir i dont believe it, it has been im...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>5.684500e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>@united Joni did a great job on flight 5653 to...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>5.696930e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>United</td>\n",
       "      <td>@united @baftz rcvd promo if i booked flight s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>5.681660e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>@united thank you! Love united!! Have 4 flight...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11717</th>\n",
       "      <td>5.678010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways I have been rebooked but It has bee...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4218</th>\n",
       "      <td>5.677680e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united who can tell me where they are?</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>5.696050e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@United You were doing so well until the PHL-S...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12463</th>\n",
       "      <td>5.701780e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir Super Spring Tides and “Tide of T...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11232</th>\n",
       "      <td>5.683870e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways I'm delayed in TYS #5015, likely go...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10432</th>\n",
       "      <td>5.693130e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways waited for 3 hours NO LUGGAGE line ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>5.700230e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways I lost an ID on your plane and havi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4515</th>\n",
       "      <td>5.700880e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir wifi stays connected about the l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>5.695550e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>@united amazing hospitality and helpfulness fr...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>5.695870e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4585</th>\n",
       "      <td>5.699960e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir I enjoyed a call from my good fr...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>5.698250e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>United</td>\n",
       "      <td>@united Any news about the departure of the fl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7541</th>\n",
       "      <td>5.695380e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Delta</td>\n",
       "      <td>“@JetBlue: @2littlebirds Beautiful shot.. Than...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>5.700680e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united agent helps the person in front of me....</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13248</th>\n",
       "      <td>5.699070e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir She could even see that I had tri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6285</th>\n",
       "      <td>5.680820e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir  goes to court to gain access to...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7341</th>\n",
       "      <td>5.696600e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue Still no response from CEO. I guess h...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10185</th>\n",
       "      <td>5.695010e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways you got to be \"F\"ing kidding me Usa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4026</th>\n",
       "      <td>5.678670e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>United</td>\n",
       "      <td>@united how about changing RNO -DEN non-stop I...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12935</th>\n",
       "      <td>5.699760e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir when changing dest city on an awa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4340</th>\n",
       "      <td>5.703020e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir poor performance all around! Pai...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13741</th>\n",
       "      <td>5.697300e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir all good! I'm catching the 11:10p...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14300</th>\n",
       "      <td>5.696400e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir delayed.....wow</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11417</th>\n",
       "      <td>5.681340e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways nope, done. Was supposed to get hom...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11642</th>\n",
       "      <td>5.678560e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways I'm on the flight, finally in the a...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14357</th>\n",
       "      <td>5.696280e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir the ticket is a poor gesture of g...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>5.686040e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir link doesn't work</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11786</th>\n",
       "      <td>5.677600e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways Haha - that will indeed be a great ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>5.696820e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united Instead of sending inaccurate emails a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3052</th>\n",
       "      <td>5.687700e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united worst airline ever. Thanks for the sle...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11956</th>\n",
       "      <td>5.702840e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir good care of their customers if a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8594</th>\n",
       "      <td>5.681330e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue When I checked yesterday it looked li...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>5.702480e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>United</td>\n",
       "      <td>@united is there an email address I can reach?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8499</th>\n",
       "      <td>5.682050e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue #jetblue #sofly #wish #firststari see...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>5.691020e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united Yes, myself and about 200 other people...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11918</th>\n",
       "      <td>5.702980e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir if by near the gate you mean sitt...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14095</th>\n",
       "      <td>5.696680e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir who's your pick for best picture?...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12437</th>\n",
       "      <td>5.701880e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir so great AA1103 sitting for an ho...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>5.681300e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>@united why is my flight from mke to ord getti...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13169</th>\n",
       "      <td>5.699220e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>American</td>\n",
       "      <td>Yessir RT “@AmericanAir: @yourlocalnyer Good m...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7776</th>\n",
       "      <td>5.692540e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue what's the status of flight 1272 dive...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7149</th>\n",
       "      <td>5.699250e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Delta</td>\n",
       "      <td>But, are your flight attendants fucking tho? R...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10887</th>\n",
       "      <td>5.688020e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways has no idea what customer service m...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>5.684620e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>@SouthwestAir Why doesn't mean TSA PreCheck sh...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8820</th>\n",
       "      <td>5.678180e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue @EllaHenderson the highlight of being...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11712 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id airline_sentiment     airline  \\\n",
       "2946   5.688190e+17          negative      United   \n",
       "4225   5.677670e+17          negative      United   \n",
       "3895   5.680380e+17          positive      United   \n",
       "14373  5.696250e+17          negative    American   \n",
       "9961   5.696000e+17          negative  US Airways   \n",
       "2546   5.690980e+17           neutral      United   \n",
       "6143   5.681830e+17          positive   Southwest   \n",
       "2382   5.692360e+17           neutral      United   \n",
       "3279   5.685580e+17          positive      United   \n",
       "5679   5.688130e+17           neutral   Southwest   \n",
       "5488   5.689490e+17           neutral   Southwest   \n",
       "14507  5.696060e+17          negative    American   \n",
       "3429   5.684500e+17          positive      United   \n",
       "1508   5.696930e+17           neutral      United   \n",
       "3729   5.681660e+17          positive      United   \n",
       "11717  5.678010e+17          negative  US Airways   \n",
       "4218   5.677680e+17          negative      United   \n",
       "1750   5.696050e+17          negative      United   \n",
       "12463  5.701780e+17           neutral    American   \n",
       "11232  5.683870e+17          negative  US Airways   \n",
       "10432  5.693130e+17          negative  US Airways   \n",
       "9250   5.700230e+17          negative  US Airways   \n",
       "4515   5.700880e+17          negative   Southwest   \n",
       "1853   5.695550e+17          positive      United   \n",
       "14638  5.695870e+17          negative    American   \n",
       "4585   5.699960e+17           neutral   Southwest   \n",
       "1328   5.698250e+17           neutral      United   \n",
       "7541   5.695380e+17          positive       Delta   \n",
       "814    5.700680e+17          negative      United   \n",
       "13248  5.699070e+17          negative    American   \n",
       "...             ...               ...         ...   \n",
       "6285   5.680820e+17           neutral   Southwest   \n",
       "7341   5.696600e+17          negative       Delta   \n",
       "10185  5.695010e+17          negative  US Airways   \n",
       "4026   5.678670e+17           neutral      United   \n",
       "12935  5.699760e+17           neutral    American   \n",
       "4340   5.703020e+17          negative   Southwest   \n",
       "13741  5.697300e+17          positive    American   \n",
       "14300  5.696400e+17          negative    American   \n",
       "11417  5.681340e+17          negative  US Airways   \n",
       "11642  5.678560e+17          negative  US Airways   \n",
       "14357  5.696280e+17          negative    American   \n",
       "5789   5.686040e+17          negative   Southwest   \n",
       "11786  5.677600e+17          positive  US Airways   \n",
       "1525   5.696820e+17          negative      United   \n",
       "3052   5.687700e+17          negative      United   \n",
       "11956  5.702840e+17          negative    American   \n",
       "8594   5.681330e+17           neutral       Delta   \n",
       "617    5.702480e+17           neutral      United   \n",
       "8499   5.682050e+17           neutral       Delta   \n",
       "2543   5.691020e+17          negative      United   \n",
       "11918  5.702980e+17          negative    American   \n",
       "14095  5.696680e+17           neutral    American   \n",
       "12437  5.701880e+17          negative    American   \n",
       "3783   5.681300e+17          negative      United   \n",
       "13169  5.699220e+17           neutral    American   \n",
       "7776   5.692540e+17           neutral       Delta   \n",
       "7149   5.699250e+17          negative       Delta   \n",
       "10887  5.688020e+17          negative  US Airways   \n",
       "5955   5.684620e+17          negative   Southwest   \n",
       "8820   5.678180e+17          negative       Delta   \n",
       "\n",
       "                                                    text  topic_label  \n",
       "2946   @united I was not looking for the fare to be r...            1  \n",
       "4225   @united its 2015 and no power outlets at the s...            8  \n",
       "3895   @united sorry to hear outsourcing plan. Boise ...            7  \n",
       "14373  @AmericanAir that's unacceptable. They should ...            1  \n",
       "9961   @USAirways Its pretty ridiculous that at PHX s...            4  \n",
       "2546   @united lets do that can you email boarding pass?            6  \n",
       "6143   @SouthwestAir thx - fingers crossed they are f...            8  \n",
       "2382                @united I'm flying UA but *G with A3            9  \n",
       "3279   @united Fair enough. I don't usually rant, but...            7  \n",
       "5679   @SouthwestAir can you show me some luv?Its 9 d...            3  \n",
       "5488           @SouthwestAir @SwagglikeBean take me here            7  \n",
       "14507  @AmericanAir i dont believe it, it has been im...            4  \n",
       "3429   @united Joni did a great job on flight 5653 to...            9  \n",
       "1508   @united @baftz rcvd promo if i booked flight s...            0  \n",
       "3729   @united thank you! Love united!! Have 4 flight...            4  \n",
       "11717  @USAirways I have been rebooked but It has bee...            4  \n",
       "4218             @united who can tell me where they are?            7  \n",
       "1750   @United You were doing so well until the PHL-S...            9  \n",
       "12463  @AmericanAir Super Spring Tides and “Tide of T...            5  \n",
       "11232  @USAirways I'm delayed in TYS #5015, likely go...            4  \n",
       "10432  @USAirways waited for 3 hours NO LUGGAGE line ...            8  \n",
       "9250   @USAirways I lost an ID on your plane and havi...            7  \n",
       "4515   @SouthwestAir wifi stays connected about the l...            5  \n",
       "1853   @united amazing hospitality and helpfulness fr...            7  \n",
       "14638  @AmericanAir you have my money, you change my ...            8  \n",
       "4585   @SouthwestAir I enjoyed a call from my good fr...            4  \n",
       "1328   @united Any news about the departure of the fl...            4  \n",
       "7541   “@JetBlue: @2littlebirds Beautiful shot.. Than...            5  \n",
       "814    @united agent helps the person in front of me....            8  \n",
       "13248  @AmericanAir She could even see that I had tri...            1  \n",
       "...                                                  ...          ...  \n",
       "6285   @SouthwestAir  goes to court to gain access to...            5  \n",
       "7341   @JetBlue Still no response from CEO. I guess h...            8  \n",
       "10185  @USAirways you got to be \"F\"ing kidding me Usa...            1  \n",
       "4026   @united how about changing RNO -DEN non-stop I...            4  \n",
       "12935  @AmericanAir when changing dest city on an awa...            4  \n",
       "4340   @SouthwestAir poor performance all around! Pai...            4  \n",
       "13741  @AmericanAir all good! I'm catching the 11:10p...            9  \n",
       "14300                       @AmericanAir delayed.....wow            3  \n",
       "11417  @USAirways nope, done. Was supposed to get hom...            8  \n",
       "11642  @USAirways I'm on the flight, finally in the a...            7  \n",
       "14357  @AmericanAir the ticket is a poor gesture of g...            4  \n",
       "5789                     @SouthwestAir link doesn't work            7  \n",
       "11786  @USAirways Haha - that will indeed be a great ...            0  \n",
       "1525   @united Instead of sending inaccurate emails a...            8  \n",
       "3052   @united worst airline ever. Thanks for the sle...            2  \n",
       "11956  @AmericanAir good care of their customers if a...            2  \n",
       "8594   @JetBlue When I checked yesterday it looked li...            4  \n",
       "617    @united is there an email address I can reach?...            0  \n",
       "8499   @JetBlue #jetblue #sofly #wish #firststari see...            5  \n",
       "2543   @united Yes, myself and about 200 other people...            6  \n",
       "11918  @AmericanAir if by near the gate you mean sitt...            8  \n",
       "14095  @AmericanAir who's your pick for best picture?...            6  \n",
       "12437  @AmericanAir so great AA1103 sitting for an ho...            4  \n",
       "3783   @united why is my flight from mke to ord getti...            9  \n",
       "13169  Yessir RT “@AmericanAir: @yourlocalnyer Good m...            7  \n",
       "7776   @JetBlue what's the status of flight 1272 dive...            8  \n",
       "7149   But, are your flight attendants fucking tho? R...            5  \n",
       "10887  @USAirways has no idea what customer service m...            2  \n",
       "5955   @SouthwestAir Why doesn't mean TSA PreCheck sh...            7  \n",
       "8820   @JetBlue @EllaHenderson the highlight of being...            3  \n",
       "\n",
       "[11712 rows x 5 columns]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"../data/Tweets.csv\")\n",
    "data.processed_Traindata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "class DataPreprocessing:\n",
    "    def iterate(self):\n",
    "        for preprocessingMethod in [self.removeUrls,\n",
    "                                   self.removeUsernames,\n",
    "                                   self.removeElongatedWords,\n",
    "                                   self.removeNa,\n",
    "                                   self.replaceSlangWords,\n",
    "                                   self.removeSpecialChars,\n",
    "                                   self.removeNumbers]:\n",
    "            yield preprocessingMethod\n",
    "    \n",
    "    @staticmethod\n",
    "    def removeByRegex(tweets, regExp):\n",
    "        tweets.loc[:, \"text\"].replace(regExp, \"\", inplace=True)\n",
    "        return tweets\n",
    "    \n",
    "    def removeUrls(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "    \n",
    "    def removeNa(self, tweets):\n",
    "        return tweets[tweets[\"text\"] != \"\"]\n",
    "    \n",
    "    def removeSpecialChars(self, tweets):\n",
    "        for remove in map(lambda r: regex.compile(regex.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\"\n",
    "                                                                    \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                    \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                    \"!\", \"?\", \".\", \"'\",\n",
    "                                                                    \"--\", \"---\", \"#\"]):\n",
    "            tweets.loc[:, \"text\"].replace(remove, \"\", inplace=True)\n",
    "        return tweets\n",
    "    \n",
    "    def removeUsernames(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "    \n",
    "    def removeElongatedWords(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"(.)\\1+', r'\\1\\1\"))\n",
    "    \n",
    "    def removeNumbers(self, tweets):\n",
    "        #print(tweets)\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))\n",
    "    \n",
    "    def replaceSlangWords(self, tweets):\n",
    "        with open('../data/slang.txt') as file:\n",
    "            slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "            for line in file if line.strip())\n",
    "            #print(tweets[\"text\"])\n",
    "            #print(\"-----------------------------------------END\")\n",
    "            for index,word in tweets['text'].iteritems():\n",
    "                #print(index)\n",
    "                for i in word.split():\n",
    "                    isUpperCase = i.isupper()\n",
    "                    i = i.lower()\n",
    "                    if i in slang_map.keys():\n",
    "                        word = word.replace(i, slang_map[i])\n",
    "                        tweets.loc[(index),\"text\"] = word\n",
    "                if isUpperCase:\n",
    "                    i = i.upper()\n",
    "        #print(tweets.loc[:,\"text\"])\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the Training Data\n",
    "class CleanTrainingData(TwitterData_Initialize):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        #self.processed_Testdata = previous.processed_Testdata\n",
    "    \n",
    "    def cleaningData(self, cleaner):\n",
    "        train = self.processed_Traindata\n",
    "        #test = self.processed_Testdata\n",
    "        \n",
    "        for cleanerMethod in cleaner.iterate():\n",
    "            train = cleanerMethod(train)\n",
    "            #test = cleanerMethod(test)\n",
    "        self.processed_Traindata = train\n",
    "        #self.processed_Testdata = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-398-f22aa1699131>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCleanTrainingData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleaningData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataPreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessed_Traindata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-397-3598b7bc2f37>\u001b[0m in \u001b[0;36mcleaningData\u001b[1;34m(self, cleaner)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcleanerMethod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcleaner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleanerMethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[1;31m#test = cleanerMethod(test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessed_Traindata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-396-6b6aca1ace63>\u001b[0m in \u001b[0;36mreplaceSlangWords\u001b[1;34m(self, tweets)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;31m#print(index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                     \u001b[0misUpperCase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "data = CleanTrainingData(data)\n",
    "data.cleaningData(DataPreprocessing())\n",
    "data.processed_Traindata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenizing and Stemming the data\n",
    "class TokenizationStemming(CleanTrainingData):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        #self.processed_TestData = previous.processed_TestData\n",
    "    \n",
    "    def stem(self, stemmer = nltk.PorterStemmer()):\n",
    "        def stemJoin(row):\n",
    "            row[\"text\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"text\"]))\n",
    "            return row\n",
    "    \n",
    "        self.processed_Traindata = self.processed_Traindata.apply(stemJoin, axis=1)\n",
    "    \n",
    "    def tokenize(self, tokenizer = nltk.word_tokenize):\n",
    "        def tokenizeRow(row):\n",
    "            row[\"text\"] = tokenizer(row[\"text\"])\n",
    "            row[\"tokenizedText\"] = [] + row[\"text\"]\n",
    "            return row\n",
    "        \n",
    "        self.processed_Traindata = self.processed_Traindata.apply(tokenizeRow, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_label</th>\n",
       "      <th>tokenizedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2946</th>\n",
       "      <td>5.688190e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>[i, wa, not, look, for, the, fare, to, be, ret...</td>\n",
       "      <td>1</td>\n",
       "      <td>[I, was, not, looking, for, the, fare, to, be,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>5.677670e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>[it, and, no, power, outlet, at, the, seat, on...</td>\n",
       "      <td>8</td>\n",
       "      <td>[its, and, no, power, outlets, at, the, seats,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3895</th>\n",
       "      <td>5.680380e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>[sorri, to, hear, outsourc, plan, bois, is, be...</td>\n",
       "      <td>7</td>\n",
       "      <td>[sorry, to, hear, outsourcing, plan, Boise, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14373</th>\n",
       "      <td>5.696250e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>[that, unaccept, they, should, allow, me, to, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[thats, unacceptable, They, should, allow, me,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9961</th>\n",
       "      <td>5.696000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>[it, pretti, ridicul, that, at, phx, sky, harb...</td>\n",
       "      <td>4</td>\n",
       "      <td>[Its, pretty, ridiculous, that, at, PHX, sky, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id airline_sentiment     airline  \\\n",
       "2946   5.688190e+17          negative      United   \n",
       "4225   5.677670e+17          negative      United   \n",
       "3895   5.680380e+17          positive      United   \n",
       "14373  5.696250e+17          negative    American   \n",
       "9961   5.696000e+17          negative  US Airways   \n",
       "\n",
       "                                                    text  topic_label  \\\n",
       "2946   [i, wa, not, look, for, the, fare, to, be, ret...            1   \n",
       "4225   [it, and, no, power, outlet, at, the, seat, on...            8   \n",
       "3895   [sorri, to, hear, outsourc, plan, bois, is, be...            7   \n",
       "14373  [that, unaccept, they, should, allow, me, to, ...            1   \n",
       "9961   [it, pretti, ridicul, that, at, phx, sky, harb...            4   \n",
       "\n",
       "                                           tokenizedText  \n",
       "2946   [I, was, not, looking, for, the, fare, to, be,...  \n",
       "4225   [its, and, no, power, outlets, at, the, seats,...  \n",
       "3895   [sorry, to, hear, outsourcing, plan, Boise, is...  \n",
       "14373  [thats, unacceptable, They, should, allow, me,...  \n",
       "9961   [Its, pretty, ridiculous, that, at, PHX, sky, ...  "
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TokenizationStemming(data)\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.processed_Traindata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 6898), ('the', 4837), ('i', 4340), ('flight', 3822), ('a', 3557)]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building Wordlist\n",
    "#Un-filtered version without removing stopwords\n",
    "words = collections.Counter()\n",
    "for idx in data.processed_Traindata.index:\n",
    "    words.update(data.processed_Traindata.loc[idx, \"text\"])\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flight', 3822), ('thank', 1367), ('wa', 1287), ('get', 1287), ('not', 1284)]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "whitelist = [\"n't\", \"not\"]\n",
    "for idx, stop_word in enumerate(stopwords):\n",
    "    if stop_word not in whitelist:\n",
    "        del words[stop_word]\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating the final wordlist\n",
    "class WordList(TokenizationStemming):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "    \n",
    "    whitelist = [\"n't\", \"not\"]\n",
    "    wordlist = []\n",
    "    \n",
    "    def buildWordlist(self, min_occurrences=3, max_occurences=3000, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                     whitelist=None):\n",
    "        self.wordlist = []\n",
    "        whitelist = self.whitelist if whitelist is None else whitelist\n",
    "        import os\n",
    "        if os.path.isfile('../data/wordlist.csv'):\n",
    "            word_df = pd.read_csv('../data/wordlist.csv', encoding = \"ISO-8859-1\")\n",
    "            word_df = word_df[word_df[\"occurrences\"] > min_occurrences]\n",
    "            self.wordlist = list(word_df.loc[:, \"word\"])\n",
    "            return\n",
    "        words = collections.Counter()\n",
    "        for idx in self.processed_Traindata.index:\n",
    "            words.update(self.processed_Traindata.loc[idx, \"text\"])\n",
    "        \n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "        \n",
    "        word_df = pd.DataFrame(data={\"word\" : [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                    \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                              columns = [\"word\", \"occurrences\"])\n",
    "        \n",
    "        word_df.to_csv(\"../data/wordlist.csv\", index_label=\"idx\", encoding = \"utf8\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = WordList(data)\n",
    "data.buildWordlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transforming into Bag-of-Words\n",
    "class BagOfWords(WordList):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        self.wordlist = previous.wordlist\n",
    "    \n",
    "    def buildDataModel(self):\n",
    "        labelColumn = [\"label\"]\n",
    "        columns = labelColumn + list(\n",
    "            map(lambda w: w + \"_bow\", self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        \n",
    "        for idx in self.processed_Traindata.index:\n",
    "            currentRow = []\n",
    "            currentLabel = self.processed_Traindata.loc[idx, \"airline_sentiment\"]\n",
    "            labels.append(currentLabel)\n",
    "            currentRow.append(currentLabel)\n",
    "            \n",
    "            tokens = set(self.processed_Traindata.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                currentRow.append(1 if word in tokens else 0)\n",
    "            \n",
    "            rows.append(currentRow)\n",
    "        \n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        \n",
    "        return self.data_model, self.data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>hour_bow</th>\n",
       "      <th>cancel_bow</th>\n",
       "      <th>help_bow</th>\n",
       "      <th>servic_bow</th>\n",
       "      <th>delay_bow</th>\n",
       "      <th>time_bow</th>\n",
       "      <th>custom_bow</th>\n",
       "      <th>bag_bow</th>\n",
       "      <th>call_bow</th>\n",
       "      <th>...</th>\n",
       "      <th>gorgeou_bow</th>\n",
       "      <th>woohoo_bow</th>\n",
       "      <th>thousand_bow</th>\n",
       "      <th>understat_bow</th>\n",
       "      <th>furiou_bow</th>\n",
       "      <th>manual_bow</th>\n",
       "      <th>smell_bow</th>\n",
       "      <th>ber_bow</th>\n",
       "      <th>charleston_bow</th>\n",
       "      <th>nrt_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2415 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  hour_bow  cancel_bow  help_bow  servic_bow  delay_bow  time_bow  \\\n",
       "0  negative         0           0         0           0          0         0   \n",
       "1   neutral         0           0         0           0          0         0   \n",
       "2   neutral         1           0         0           0          0         0   \n",
       "3  positive         0           0         0           0          0         0   \n",
       "4  negative         0           0         1           0          0         0   \n",
       "\n",
       "   custom_bow  bag_bow  call_bow   ...     gorgeou_bow  woohoo_bow  \\\n",
       "0           0        0         0   ...               0           0   \n",
       "1           0        0         0   ...               0           0   \n",
       "2           0        0         0   ...               0           0   \n",
       "3           0        0         0   ...               0           0   \n",
       "4           0        0         0   ...               0           0   \n",
       "\n",
       "   thousand_bow  understat_bow  furiou_bow  manual_bow  smell_bow  ber_bow  \\\n",
       "0             0              0           0           0          0        0   \n",
       "1             0              0           0           0          0        0   \n",
       "2             0              0           0           0          0        0   \n",
       "3             0              0           0           0          0        0   \n",
       "4             0              0           0           0          0        0   \n",
       "\n",
       "   charleston_bow  nrt_bow  \n",
       "0               0        0  \n",
       "1               0        0  \n",
       "2               0        0  \n",
       "3               0        0  \n",
       "4               0        0  \n",
       "\n",
       "[5 rows x 2415 columns]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = BagOfWords(data)\n",
    "bow, labels = data.buildDataModel()\n",
    "bow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 666\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Utility function to train the classifier and show F1, Precision, recall and Accuracy values\n",
    "\n",
    "def test_classifier(X_train, y_train, X_test, y_test, classifier):\n",
    "    log(\"\")\n",
    "    log(\"==================================================\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    log(\"Testing \" + classifier_name)\n",
    "    now = time()\n",
    "    list_of_labels = sorted(list(set(y_train)))\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    log(\"Learning time {0}s\".format(time() - now))\n",
    "    now = time()\n",
    "    predictions = model.predict(X_test)\n",
    "    log(\"Predicting time {0}s\".format(time() - now))\n",
    "    \n",
    "    precision = precision_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    recall = recall_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    log(\"==================Results=======================\")\n",
    "    log(\"            Negative     Neutral    Positive\")\n",
    "    log(\"F1         \" + str(f1))\n",
    "    log(\"Precision  \" + str(precision))\n",
    "    log(\"Recall     \" + str(recall))\n",
    "    log(\"Accuracy   \" + str(accuracy))\n",
    "    log(\"================================================\")\n",
    "    \n",
    "    return precision, recall, accuracy, f1\n",
    "\n",
    "def log(x):\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing BernoulliNB\n",
      "Learning time 0.8397293090820312s\n",
      "Predicting time 0.2986288070678711s\n",
      "==================Results=======================\n",
      "            Negative     Neutral    Positive\n",
      "F1         [0.8519774  0.57836066 0.58070501]\n",
      "Precision  [0.84340045 0.57198444 0.61614173]\n",
      "Recall     [0.86073059 0.58488064 0.54912281]\n",
      "Accuracy   0.750996015936255\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "#Classifier : BagOfWords + NaiveBayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:, 1:], bow.iloc[:, 0],\n",
    "                                                   train_size = 0.7, stratify=bow.iloc[:, 0],\n",
    "                                                   random_state = seed)\n",
    "\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NaiveBayes with 8 fold cross-validation\n",
    "\n",
    "def cv(classifier, X_train, y_train):\n",
    "    log(\"===============================================\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    now = time()\n",
    "    log(\"Crossvalidating \" + classifier_name + \"...\")\n",
    "    accuracy = [cross_val_score(classifier, X_train, y_train, cv=8, n_jobs=-1)]\n",
    "    log(\"Crosvalidation completed in {0}s\".format(time() - now))\n",
    "    log(\"Accuracy: \" + str(accuracy[0]))\n",
    "    log(\"Average accuracy: \" + str(np.array(accuracy[0]).mean()))\n",
    "    log(\"===============================================\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Crossvalidating BernoulliNB...\n",
      "Crosvalidation completed in 21.083032608032227s\n",
      "Accuracy: [0.77883959 0.74539249 0.76382253 0.74726776 0.74795082 0.76281613\n",
      " 0.75393028 0.75529733]\n",
      "Average accuracy: 0.7569146165589327\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "nb_acc = cv(BernoulliNB(), bow.iloc[:,1:], bow.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Addtion of extra features:\n",
    "\n",
    "# Number of Uppercase - tend to express postive/negative emotions by using uppercase words\n",
    "# Number of !         - exclamation marks are likely to increase strength of opinion\n",
    "# Number of ?         - might distinguish neutral tweets - seeking information\n",
    "# Number of positive  - positive emoji will most likely occur in positive tweets\n",
    "# emoticons\n",
    "# Number of negative  - Inverse to the one above\n",
    "# emoticons\n",
    "# Number of ...       - commonly used in commenting something\n",
    "# Number of quotations- same as above\n",
    "# Number of mentions  - Lots of mentions on positive tweets, to share something good/bad\n",
    "# Number of urls      - similar to number of mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Detecting Emoticons\n",
    "class EmoticonDetector:\n",
    "    emoticons = {}\n",
    "    \n",
    "    def __init__(self, emoticon_file=\"../data/emoticons.txt\"):\n",
    "        from pathlib import Path\n",
    "        content = Path(emoticon_file).read_text()\n",
    "        positive = True\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if \"positive\" in line.lower():\n",
    "                positive = True\n",
    "                continue\n",
    "            elif \"negative\" in line.lower():\n",
    "                positive = False\n",
    "                continue\n",
    "            \n",
    "            self.emoticons[line] = positive\n",
    "    \n",
    "    def is_positive(self, emoticon):\n",
    "        if emoticon in self.emoticons:\n",
    "            return self.emoticons[emoticon]\n",
    "        return False\n",
    "    \n",
    "    def is_emoticon(self, to_check):\n",
    "        return to_check in self.emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtraFeatures(WordList):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_data_model(self):\n",
    "        extra_columns = [col for col in self.processed_Traindata.columns if col.startswith(\"number_of\")]\n",
    "        label_column = [\"label\"]\n",
    "        columns = label_column + extra_columns + list(\n",
    "                map(lambda w: w + \"_bow\", self.wordlist))\n",
    "        \n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_Traindata.index:\n",
    "            current_row = []\n",
    "            current_label = self.processed_Traindata.loc[idx, \"airline_sentiment\"]\n",
    "            labels.append(current_label)\n",
    "            current_row.append(current_label)\n",
    "        \n",
    "            for _,col in enumerate(extra_columns):\n",
    "                current_row.append(self.processed_Traindata.loc[idx, col])\n",
    "        \n",
    "        #adding bad-of-words\n",
    "            tokens = set(self.processed_Traindata.loc[idx, \"text\"])\n",
    "            for _,word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "        \n",
    "            rows.append(current_row)\n",
    "        \n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        \n",
    "        return self.data_model, self.data_labels\n",
    "    \n",
    "    def add_column(self, column_name, column_content):\n",
    "        self.processed_Traindata.loc[:, column_name] = pd.Series(column_content, index=self.processed_Traindata.index)\n",
    "\n",
    "    def build_features(self):\n",
    "        def count_by_lambda(expression, word_array):\n",
    "            return len(list(filter(expression, word_array)))\n",
    "        \n",
    "        def count_occurences(character, word_array):\n",
    "            counter = 0\n",
    "            for j, word in enumerate(word_array):\n",
    "                for char in word:\n",
    "                    if char == character:\n",
    "                        counter += 1\n",
    "            return counter\n",
    "        \n",
    "        def count_by_regex(regex, plain_text):\n",
    "            return len(regex.findall(plain_text))\n",
    "        \n",
    "        self.add_column(\"splitted_text\", map(lambda txt: txt.split(\" \"), self.processed_Traindata[\"text\"]))\n",
    "        \n",
    "        #Number of uppercase words\n",
    "        uppercase = list(map(lambda txt: count_by_lambda(lambda word: word == word.upper(), txt),\n",
    "                                                        self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_uppercase\", uppercase)\n",
    "        \n",
    "        #number of !\n",
    "        exclamations = list(map(lambda txt: count_occurences(\"!\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_exclamation\", exclamations)\n",
    "        \n",
    "        #number of ?\n",
    "        questions = list(map(lambda txt: count_occurences(\"?\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_question\", questions)\n",
    "        \n",
    "        #number of ...\n",
    "        ellipsis = list(map(lambda txt: count_by_regex(regex.compile(r\"\\.\\s?\\.\\s?\\.\"), txt),\n",
    "                           self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_ellipsis\", ellipsis)\n",
    "        \n",
    "        #number of hashtags\n",
    "        hashtags = list(map(lambda txt: count_occurences(\"#\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_hashtags\", hashtags)\n",
    "        \n",
    "        #number of mentions\n",
    "        mentions = list(map(lambda txt: count_occurences(\"@\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_mentions\", mentions)\n",
    "        \n",
    "        #number of quotes\n",
    "        quotes = list(map(lambda plain_text: int(count_occurences(\"'\", [plain_text.strip(\"'\").strip('\"')]) / 2 +\n",
    "                                                 count_occurences('\"', [plain_text.strip(\"'\").strip('\"')]) / 2),\n",
    "                          self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_quotes\", quotes)\n",
    "        \n",
    "        #number of urls\n",
    "        urls = list(map(lambda txt: count_by_regex(regex.compile(r\"http.?://[^\\s]+[\\s]?\"), txt),\n",
    "                             self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_urls\", urls)\n",
    "        \n",
    "        #number of positive emoticons\n",
    "        ed = EmoticonDetector()\n",
    "        positive_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and ed.is_positive(word), txt), \n",
    "                   self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_positive_emo\", positive_emo)\n",
    "        \n",
    "        #number of negative emoticons\n",
    "        negative_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and not ed.is_positive(word), txt), \n",
    "                   self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_negative_emo\", negative_emo)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:337: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3813: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = ExtraFeatures()\n",
    "data.initialize(\"../data/Tweets.csv\")\n",
    "data.build_features()\n",
    "data.cleaningData(DataPreprocessing())\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.buildWordlist()\n",
    "data_model, labels = data.build_data_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing RandomForestClassifier\n",
      "Learning time 20.246517419815063s\n",
      "Predicting time 0.4646904468536377s\n",
      "==================Results=======================\n",
      "            Negative     Neutral    Positive\n",
      "F1         [0.84963691 0.53918495 0.62803738]\n",
      "Precision  [0.80558931 0.6405959  0.66141732]\n",
      "Recall     [0.89877994 0.46549391 0.59786477]\n",
      "Accuracy   0.7595332953898691\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "#Extended Features + Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_model.iloc[:, 1:], data_model.iloc[:, 0],\n",
    "                                                    train_size=0.7, stratify=data_model.iloc[:, 0],\n",
    "                                                    random_state=seed)\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, RandomForestClassifier(random_state=seed,n_estimators=403,n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Crossvalidating RandomForestClassifier...\n",
      "Crosvalidation completed in 235.91511368751526s\n",
      "Accuracy: [0.7774744  0.77663934 0.76639344 0.7670765  0.76571038 0.73019126\n",
      " 0.75546448 0.7518797 ]\n",
      "Average accuracy: 0.7613536889768202\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "#Crosvalidation\n",
    "rf_acc = cv(RandomForestClassifier(n_estimators=403,n_jobs=-1, random_state=seed),data_model.iloc[:, 1:], data_model.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing BernoulliNB\n",
      "Learning time 1.0232014656066895s\n",
      "Predicting time 0.3078019618988037s\n",
      "==================Results=======================\n",
      "            Negative     Neutral    Positive\n",
      "F1         [0.8519774  0.57836066 0.58070501]\n",
      "Precision  [0.84340045 0.57198444 0.61614173]\n",
      "Recall     [0.86073059 0.58488064 0.54912281]\n",
      "Accuracy   0.750996015936255\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:, 1:], bow.iloc[:, 0],\n",
    "                                                   train_size = 0.7, stratify=bow.iloc[:, 0],\n",
    "                                                   random_state = seed)\n",
    "\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, BernoulliNB())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
