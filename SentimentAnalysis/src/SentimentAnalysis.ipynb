{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "#from emoticons import EmoticonDetector\n",
    "import re as regex\n",
    "import collections\n",
    "import numpy as np\n",
    "import plotly\n",
    "from plotly import graph_objs\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from time import time\n",
    "import gensim\n",
    "\n",
    "#plotly configuration\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    data = []\n",
    "    processed_Traindata = []\n",
    "    processed_Testdata = []\n",
    "    wordlist = []\n",
    "    \n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    \n",
    "    def initialize(self, csv_file, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            self.data_model = pd.read_csv(from_cached)\n",
    "            return\n",
    "        \n",
    "        self.data = pd.read_csv(csv_file, usecols=[0,1,5,10,15])\n",
    "        train, test = train_test_split(self.data, test_size=0.2)\n",
    "        self.processed_Traindata = train\n",
    "        self.processed_Testdata = test\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9925</th>\n",
       "      <td>5.696130e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways despite mechanical issues and many ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13713</th>\n",
       "      <td>5.697390e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir stuck in airplane both on way out...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12015</th>\n",
       "      <td>5.702690e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>@AmericanAir any idea on what the wait time is...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7781</th>\n",
       "      <td>5.692500e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Delta</td>\n",
       "      <td>@JetBlue you can't beat jetblue in space's mat...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10769</th>\n",
       "      <td>5.688950e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>@USAirways I'm trying to Request Missed Mileag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id airline_sentiment     airline  \\\n",
       "9925   5.696130e+17          positive  US Airways   \n",
       "13713  5.697390e+17          negative    American   \n",
       "12015  5.702690e+17          negative    American   \n",
       "7781   5.692500e+17          positive       Delta   \n",
       "10769  5.688950e+17          negative  US Airways   \n",
       "\n",
       "                                                    text  topic_label  \n",
       "9925   @USAirways despite mechanical issues and many ...            8  \n",
       "13713  @AmericanAir stuck in airplane both on way out...            7  \n",
       "12015  @AmericanAir any idea on what the wait time is...            8  \n",
       "7781   @JetBlue you can't beat jetblue in space's mat...            9  \n",
       "10769  @USAirways I'm trying to Request Missed Mileag...            1  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"Tweets.csv\")\n",
    "data.processed_Traindata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          "positive",
          "negative",
          "neutral"
         ],
         "y": [
          1924,
          7292,
          2496
         ]
        }
       ],
       "layout": {
        "title": "Sentiment type distribution in training set"
       }
      },
      "text/html": [
       "<div id=\"622e0d8f-e838-4b7c-8425-b9a5209ddfcf\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"622e0d8f-e838-4b7c-8425-b9a5209ddfcf\", [{\"type\": \"bar\", \"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [1924, 7292, 2496]}], {\"title\": \"Sentiment type distribution in training set\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"622e0d8f-e838-4b7c-8425-b9a5209ddfcf\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"622e0d8f-e838-4b7c-8425-b9a5209ddfcf\", [{\"type\": \"bar\", \"x\": [\"positive\", \"negative\", \"neutral\"], \"y\": [1924, 7292, 2496]}], {\"title\": \"Sentiment type distribution in training set\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the training data\n",
    "df = data.processed_Traindata\n",
    "negative = len(df[df[\"airline_sentiment\"] == \"negative\"])\n",
    "positive = len(df[df[\"airline_sentiment\"] == \"positive\"])\n",
    "neutral = len(df[df[\"airline_sentiment\"] == \"neutral\"])\n",
    "\n",
    "dist = [\n",
    "    graph_objs.Bar(\n",
    "        x = [\"positive\", \"negative\", \"neutral\"],\n",
    "        y = [positive, negative, neutral],\n",
    "    )]\n",
    "plotly.offline.iplot({\"data\":dist, \"layout\":graph_objs.Layout(title=\"Sentiment type distribution in training set\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Preprocessing\n",
    "class DataPreprocessing:\n",
    "    def iterate(self):\n",
    "        for preprocessingMethod in [self.removeUrls,\n",
    "                                   self.removeUsernames,\n",
    "                                   self.removeElongatedWords,\n",
    "                                   self.removeNa,\n",
    "                                   self.removeSpecialChars,\n",
    "                                   self.removeNumbers]:\n",
    "            yield preprocessingMethod\n",
    "    \n",
    "    @staticmethod\n",
    "    def removeByRegex(tweets, regExp):\n",
    "        tweets.loc[:, \"text\"].replace(regExp, \"\", inplace=True)\n",
    "        return tweets\n",
    "    \n",
    "    def removeUrls(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"http.?://[^\\s]+[\\s]?\"))\n",
    "    \n",
    "    def removeNa(self, tweets):\n",
    "        return tweets[tweets[\"text\"] != \"\"]\n",
    "    \n",
    "    def removeSpecialChars(self, tweets):\n",
    "        for remove in map(lambda r: regex.compile(regex.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\"\n",
    "                                                                    \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n",
    "                                                                    \"[\", \"]\", \"|\", \"/\", \"\\\\\", \">\", \"<\", \"-\",\n",
    "                                                                    \"!\", \"?\", \".\", \"'\",\n",
    "                                                                    \"--\", \"---\", \"#\"]):\n",
    "            tweets.loc[:, \"text\"].replace(remove, \"\", inplace=True)\n",
    "        return tweets\n",
    "    \n",
    "    def removeUsernames(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"@[^\\s]+[\\s]?\"))\n",
    "    \n",
    "    def removeElongatedWords(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"(.)\\1+', r'\\1\\1\"))\n",
    "    \n",
    "    def removeNumbers(self, tweets):\n",
    "        return DataPreprocessing.removeByRegex(tweets, regex.compile(r\"\\s?[0-9]+\\.?[0-9]*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning the Training Data\n",
    "class CleanTrainingData(TwitterData_Initialize):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        #self.processed_Testdata = previous.processed_Testdata\n",
    "    \n",
    "    def cleaningData(self, cleaner):\n",
    "        train = self.processed_Traindata\n",
    "        #test = self.processed_Testdata\n",
    "        \n",
    "        for cleanerMethod in cleaner.iterate():\n",
    "            train = cleanerMethod(train)\n",
    "            #test = cleanerMethod(test)\n",
    "        self.processed_Traindata = train\n",
    "        #self.processed_Testdata = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9925</th>\n",
       "      <td>5.696130e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>despite mechanical issues and many delays foll...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13713</th>\n",
       "      <td>5.697390e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>stuck in airplane both on way out of PHL and a...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12015</th>\n",
       "      <td>5.702690e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>any idea on what the wait time is for refunds ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7781</th>\n",
       "      <td>5.692500e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Delta</td>\n",
       "      <td>you cant beat jetblue in spaces matter</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10769</th>\n",
       "      <td>5.688950e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>Im trying to Request Missed Mileage and it kee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id airline_sentiment     airline  \\\n",
       "9925   5.696130e+17          positive  US Airways   \n",
       "13713  5.697390e+17          negative    American   \n",
       "12015  5.702690e+17          negative    American   \n",
       "7781   5.692500e+17          positive       Delta   \n",
       "10769  5.688950e+17          negative  US Airways   \n",
       "\n",
       "                                                    text  topic_label  \n",
       "9925   despite mechanical issues and many delays foll...            8  \n",
       "13713  stuck in airplane both on way out of PHL and a...            7  \n",
       "12015  any idea on what the wait time is for refunds ...            8  \n",
       "7781             you cant beat jetblue in spaces matter             9  \n",
       "10769  Im trying to Request Missed Mileage and it kee...            1  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CleanTrainingData(data)\n",
    "data.cleaningData(DataPreprocessing())\n",
    "data.processed_Traindata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenizing and Stemming the data\n",
    "class TokenizationStemming(CleanTrainingData):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        #self.processed_TestData = previous.processed_TestData\n",
    "    \n",
    "    def stem(self, stemmer = nltk.PorterStemmer()):\n",
    "        def stemJoin(row):\n",
    "            row[\"text\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"text\"]))\n",
    "            return row\n",
    "    \n",
    "        self.processed_Traindata = self.processed_Traindata.apply(stemJoin, axis=1)\n",
    "    \n",
    "    def tokenize(self, tokenizer = nltk.word_tokenize):\n",
    "        def tokenizeRow(row):\n",
    "            row[\"text\"] = tokenizer(row[\"text\"])\n",
    "            row[\"tokenizedText\"] = [] + row[\"text\"]\n",
    "            return row\n",
    "        \n",
    "        self.processed_Traindata = self.processed_Traindata.apply(tokenizeRow, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>text</th>\n",
       "      <th>topic_label</th>\n",
       "      <th>tokenizedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9925</th>\n",
       "      <td>5.696130e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>[despit, mechan, issu, and, mani, delay, follo...</td>\n",
       "      <td>8</td>\n",
       "      <td>[despite, mechanical, issues, and, many, delay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13713</th>\n",
       "      <td>5.697390e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>[stuck, in, airplan, both, on, way, out, of, p...</td>\n",
       "      <td>7</td>\n",
       "      <td>[stuck, in, airplane, both, on, way, out, of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12015</th>\n",
       "      <td>5.702690e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>American</td>\n",
       "      <td>[ani, idea, on, what, the, wait, time, is, for...</td>\n",
       "      <td>8</td>\n",
       "      <td>[any, idea, on, what, the, wait, time, is, for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7781</th>\n",
       "      <td>5.692500e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Delta</td>\n",
       "      <td>[you, cant, beat, jetblu, in, space, matter]</td>\n",
       "      <td>9</td>\n",
       "      <td>[you, cant, beat, jetblue, in, spaces, matter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10769</th>\n",
       "      <td>5.688950e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>[im, tri, to, request, miss, mileag, and, it, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Im, trying, to, Request, Missed, Mileage, and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_id airline_sentiment     airline  \\\n",
       "9925   5.696130e+17          positive  US Airways   \n",
       "13713  5.697390e+17          negative    American   \n",
       "12015  5.702690e+17          negative    American   \n",
       "7781   5.692500e+17          positive       Delta   \n",
       "10769  5.688950e+17          negative  US Airways   \n",
       "\n",
       "                                                    text  topic_label  \\\n",
       "9925   [despit, mechan, issu, and, mani, delay, follo...            8   \n",
       "13713  [stuck, in, airplan, both, on, way, out, of, p...            7   \n",
       "12015  [ani, idea, on, what, the, wait, time, is, for...            8   \n",
       "7781        [you, cant, beat, jetblu, in, space, matter]            9   \n",
       "10769  [im, tri, to, request, miss, mileag, and, it, ...            1   \n",
       "\n",
       "                                           tokenizedText  \n",
       "9925   [despite, mechanical, issues, and, many, delay...  \n",
       "13713  [stuck, in, airplane, both, on, way, out, of, ...  \n",
       "12015  [any, idea, on, what, the, wait, time, is, for...  \n",
       "7781      [you, cant, beat, jetblue, in, spaces, matter]  \n",
       "10769  [Im, trying, to, Request, Missed, Mileage, and...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TokenizationStemming(data)\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.processed_Traindata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 6902), ('the', 4772), ('i', 4346), ('flight', 3829), ('a', 3588)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building Wordlist\n",
    "#Un-filtered version without removing stopwords\n",
    "words = collections.Counter()\n",
    "for idx in data.processed_Traindata.index:\n",
    "    words.update(data.processed_Traindata.loc[idx, \"text\"])\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flight', 3829), ('thank', 1342), ('not', 1325), ('get', 1318), ('wa', 1296)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "whitelist = [\"n't\", \"not\"]\n",
    "for idx, stop_word in enumerate(stopwords):\n",
    "    if stop_word not in whitelist:\n",
    "        del words[stop_word]\n",
    "\n",
    "words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Generating the final wordlist\n",
    "class WordList(TokenizationStemming):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "    \n",
    "    whitelist = [\"n't\", \"not\"]\n",
    "    wordlist = []\n",
    "    \n",
    "    def buildWordlist(self, min_occurrences=3, max_occurences=1000, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                     whitelist=None):\n",
    "        self.wordlist = []\n",
    "        whitelist = self.whitelist if whitelist is None else whitelist\n",
    "        import os\n",
    "        if os.path.isfile('wordlist.csv'):\n",
    "            word_df = pd.read_csv('wordlist.csv', encoding = \"ISO-8859-1\")\n",
    "            word_df = word_df[word_df[\"occurrences\"] > min_occurrences]\n",
    "            self.wordlist = list(word_df.loc[:, \"word\"])\n",
    "            return\n",
    "        words = collections.Counter()\n",
    "        for idx in self.processed_Traindata.index:\n",
    "            words.update(self.processed_Traindata.loc[idx, \"text\"])\n",
    "        \n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "        \n",
    "        word_df = pd.DataFrame(data={\"word\" : [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                    \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                              columns = [\"word\", \"occurrences\"])\n",
    "        \n",
    "        word_df.to_csv(\"wordlist.csv\", index_label=\"idx\", encoding = \"utf8\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = WordList(data)\n",
    "data.buildWordlist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "orientation": "h",
         "type": "bar",
         "x": [
          585,
          588,
          617,
          620,
          746,
          752,
          771,
          779,
          822,
          851,
          925
         ],
         "y": [
          "wait",
          "im",
          "call",
          "bag",
          "custom",
          "time",
          "delay",
          "servic",
          "help",
          "cancel",
          "hour"
         ]
        }
       ],
       "layout": {
        "title": "Top Words in the built Wordlist"
       }
      },
      "text/html": [
       "<div id=\"7e59bd87-244a-492e-8546-c1228abad12a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"7e59bd87-244a-492e-8546-c1228abad12a\", [{\"type\": \"bar\", \"x\": [585, 588, 617, 620, 746, 752, 771, 779, 822, 851, 925], \"y\": [\"wait\", \"im\", \"call\", \"bag\", \"custom\", \"time\", \"delay\", \"servic\", \"help\", \"cancel\", \"hour\"], \"orientation\": \"h\"}], {\"title\": \"Top Words in the built Wordlist\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"7e59bd87-244a-492e-8546-c1228abad12a\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"7e59bd87-244a-492e-8546-c1228abad12a\", [{\"type\": \"bar\", \"x\": [585, 588, 617, 620, 746, 752, 771, 779, 822, 851, 925], \"y\": [\"wait\", \"im\", \"call\", \"bag\", \"custom\", \"time\", \"delay\", \"servic\", \"help\", \"cancel\", \"hour\"], \"orientation\": \"h\"}], {\"title\": \"Top Words in the built Wordlist\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = pd.read_csv(\"wordlist.csv\", encoding = \"ISO-8859-1\")\n",
    "x_words = list(words.loc[0:10, \"word\"])\n",
    "x_words.reverse()\n",
    "y_occ = list(words.loc[0:10, \"occurrences\"])\n",
    "y_occ.reverse()\n",
    "\n",
    "dist = [\n",
    "    graph_objs.Bar(\n",
    "        x = y_occ,\n",
    "        y = x_words,\n",
    "        orientation = \"h\"\n",
    "    )]\n",
    "plotly.offline.iplot({\"data\":dist, \"layout\":graph_objs.Layout(title=\"Top Words in the built Wordlist\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transforming into Bag-of-Words\n",
    "class BagOfWords(WordList):\n",
    "    def __init__(self, previous):\n",
    "        self.processed_Traindata = previous.processed_Traindata\n",
    "        self.wordlist = previous.wordlist\n",
    "    \n",
    "    def buildDataModel(self):\n",
    "        labelColumn = [\"label\"]\n",
    "        columns = labelColumn + list(\n",
    "            map(lambda w: w + \"_bow\", self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        \n",
    "        for idx in self.processed_Traindata.index:\n",
    "            currentRow = []\n",
    "            currentLabel = self.processed_Traindata.loc[idx, \"airline_sentiment\"]\n",
    "            labels.append(currentLabel)\n",
    "            currentRow.append(currentLabel)\n",
    "            \n",
    "            tokens = set(self.processed_Traindata.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                currentRow.append(1 if word in tokens else 0)\n",
    "            \n",
    "            rows.append(currentRow)\n",
    "        \n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        \n",
    "        return self.data_model, self.data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>hour_bow</th>\n",
       "      <th>cancel_bow</th>\n",
       "      <th>help_bow</th>\n",
       "      <th>servic_bow</th>\n",
       "      <th>delay_bow</th>\n",
       "      <th>time_bow</th>\n",
       "      <th>custom_bow</th>\n",
       "      <th>bag_bow</th>\n",
       "      <th>call_bow</th>\n",
       "      <th>...</th>\n",
       "      <th>gorgeou_bow</th>\n",
       "      <th>woohoo_bow</th>\n",
       "      <th>thousand_bow</th>\n",
       "      <th>understat_bow</th>\n",
       "      <th>furiou_bow</th>\n",
       "      <th>manual_bow</th>\n",
       "      <th>smell_bow</th>\n",
       "      <th>ber_bow</th>\n",
       "      <th>charleston_bow</th>\n",
       "      <th>nrt_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2415 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  hour_bow  cancel_bow  help_bow  servic_bow  delay_bow  time_bow  \\\n",
       "0  negative         0           1         0           0          0         0   \n",
       "1  negative         0           0         0           0          0         0   \n",
       "2  negative         0           0         0           0          0         0   \n",
       "3  negative         0           0         0           0          0         0   \n",
       "4  negative         0           0         0           0          0         0   \n",
       "\n",
       "   custom_bow  bag_bow  call_bow   ...     gorgeou_bow  woohoo_bow  \\\n",
       "0           0        0         1   ...               0           0   \n",
       "1           0        0         0   ...               0           0   \n",
       "2           0        0         0   ...               0           0   \n",
       "3           0        0         0   ...               0           0   \n",
       "4           1        0         0   ...               0           0   \n",
       "\n",
       "   thousand_bow  understat_bow  furiou_bow  manual_bow  smell_bow  ber_bow  \\\n",
       "0             0              0           0           0          0        0   \n",
       "1             0              0           0           0          0        0   \n",
       "2             0              0           0           0          0        0   \n",
       "3             0              0           0           0          0        0   \n",
       "4             0              0           0           0          0        0   \n",
       "\n",
       "   charleston_bow  nrt_bow  \n",
       "0               0        0  \n",
       "1               0        0  \n",
       "2               0        0  \n",
       "3               0        0  \n",
       "4               0        0  \n",
       "\n",
       "[5 rows x 2415 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = BagOfWords(data)\n",
    "bow, labels = data.buildDataModel()\n",
    "bow.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "name": "positive",
         "type": "bar",
         "x": [
          "great",
          "love",
          "help",
          "servic",
          "guy",
          "custom",
          "fli",
          "pleas",
          "need",
          "ani",
          "dm",
          "go",
          "hour",
          "cancel",
          "delay",
          "time"
         ],
         "y": [
          182,
          131,
          126,
          122,
          99,
          96,
          95,
          41,
          19,
          26,
          27,
          47,
          18,
          31,
          38,
          86
         ]
        },
        {
         "name": "neutral",
         "type": "bar",
         "x": [
          "great",
          "love",
          "help",
          "servic",
          "guy",
          "custom",
          "fli",
          "pleas",
          "need",
          "ani",
          "dm",
          "go",
          "hour",
          "cancel",
          "delay",
          "time"
         ],
         "y": [
          13,
          41,
          123,
          53,
          40,
          28,
          112,
          155,
          146,
          134,
          103,
          99,
          25,
          80,
          34,
          86
         ]
        },
        {
         "name": "negative",
         "type": "bar",
         "x": [
          "great",
          "love",
          "help",
          "servic",
          "guy",
          "custom",
          "fli",
          "pleas",
          "need",
          "ani",
          "dm",
          "go",
          "hour",
          "cancel",
          "delay",
          "time"
         ],
         "y": [
          68,
          60,
          559,
          598,
          200,
          585,
          315,
          248,
          371,
          188,
          71,
          338,
          810,
          695,
          651,
          546
         ]
        }
       ],
       "layout": {
        "title": "'Most Common words across sentiments"
       }
      },
      "text/html": [
       "<div id=\"e92930fd-df4f-44c1-a1a2-aae75537ecc6\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e92930fd-df4f-44c1-a1a2-aae75537ecc6\", [{\"type\": \"bar\", \"x\": [\"great\", \"love\", \"help\", \"servic\", \"guy\", \"custom\", \"fli\", \"pleas\", \"need\", \"ani\", \"dm\", \"go\", \"hour\", \"cancel\", \"delay\", \"time\"], \"y\": [182, 131, 126, 122, 99, 96, 95, 41, 19, 26, 27, 47, 18, 31, 38, 86], \"name\": \"positive\"}, {\"type\": \"bar\", \"x\": [\"great\", \"love\", \"help\", \"servic\", \"guy\", \"custom\", \"fli\", \"pleas\", \"need\", \"ani\", \"dm\", \"go\", \"hour\", \"cancel\", \"delay\", \"time\"], \"y\": [13, 41, 123, 53, 40, 28, 112, 155, 146, 134, 103, 99, 25, 80, 34, 86], \"name\": \"neutral\"}, {\"type\": \"bar\", \"x\": [\"great\", \"love\", \"help\", \"servic\", \"guy\", \"custom\", \"fli\", \"pleas\", \"need\", \"ani\", \"dm\", \"go\", \"hour\", \"cancel\", \"delay\", \"time\"], \"y\": [68, 60, 559, 598, 200, 585, 315, 248, 371, 188, 71, 338, 810, 695, 651, 546], \"name\": \"negative\"}], {\"title\": \"'Most Common words across sentiments\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"e92930fd-df4f-44c1-a1a2-aae75537ecc6\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e92930fd-df4f-44c1-a1a2-aae75537ecc6\", [{\"type\": \"bar\", \"x\": [\"great\", \"love\", \"help\", \"servic\", \"guy\", \"custom\", \"fli\", \"pleas\", \"need\", \"ani\", \"dm\", \"go\", \"hour\", \"cancel\", \"delay\", \"time\"], \"y\": [182, 131, 126, 122, 99, 96, 95, 41, 19, 26, 27, 47, 18, 31, 38, 86], \"name\": \"positive\"}, {\"type\": \"bar\", \"x\": [\"great\", \"love\", \"help\", \"servic\", \"guy\", \"custom\", \"fli\", \"pleas\", \"need\", \"ani\", \"dm\", \"go\", \"hour\", \"cancel\", \"delay\", \"time\"], \"y\": [13, 41, 123, 53, 40, 28, 112, 155, 146, 134, 103, 99, 25, 80, 34, 86], \"name\": \"neutral\"}, {\"type\": \"bar\", \"x\": [\"great\", \"love\", \"help\", \"servic\", \"guy\", \"custom\", \"fli\", \"pleas\", \"need\", \"ani\", \"dm\", \"go\", \"hour\", \"cancel\", \"delay\", \"time\"], \"y\": [68, 60, 559, 598, 200, 585, 315, 248, 371, 188, 71, 338, 810, 695, 651, 546], \"name\": \"negative\"}], {\"title\": \"'Most Common words across sentiments\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grouped = bow.groupby([\"label\"]).sum()\n",
    "words_to_visualize = []\n",
    "sentiments = [\"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "#get the most 7 common words for every sentiment\n",
    "for sentiment in sentiments:\n",
    "    words = grouped.loc[sentiment,:]\n",
    "    words.sort_values(inplace=True, ascending=False)\n",
    "    for w in words.index[:7]:\n",
    "        if w not in words_to_visualize:\n",
    "            words_to_visualize.append(w)\n",
    "\n",
    "#visualizing the words\n",
    "plot_data = []\n",
    "for sentiment in sentiments:\n",
    "    plot_data.append(graph_objs.Bar(\n",
    "            x = [w.split(\"_\")[0] for w in words_to_visualize],\n",
    "            y = [grouped.loc[sentiment,w] for w in words_to_visualize],\n",
    "            name = sentiment\n",
    "    ))\n",
    "\n",
    "plotly.offline.iplot({\n",
    "    \"data\": plot_data,\n",
    "    \"layout\": graph_objs.Layout(title=\"'Most Common words across sentiments\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 666\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Utility function to train the classifier and show F1, Precision, recall and Accuracy values\n",
    "\n",
    "def test_classifier(X_train, y_train, X_test, y_test, classifier):\n",
    "    log(\"\")\n",
    "    log(\"==================================================\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    log(\"Testing \" + classifier_name)\n",
    "    now = time()\n",
    "    list_of_labels = sorted(list(set(y_train)))\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "    log(\"Learning time {0}s\".format(time() - now))\n",
    "    now = time()\n",
    "    predictions = model.predict(X_test)\n",
    "    log(\"Predicting time {0}s\".format(time() - now))\n",
    "    \n",
    "    precision = precision_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    recall = recall_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions, average=None, pos_label=None, labels=list_of_labels)\n",
    "    log(\"==================Results=======================\")\n",
    "    log(\"            Negative     Neutral    Positive\")\n",
    "    log(\"F1         \" + str(f1))\n",
    "    log(\"Precision  \" + str(precision))\n",
    "    log(\"Recall     \" + str(recall))\n",
    "    log(\"Accuracy   \" + str(accuracy))\n",
    "    log(\"================================================\")\n",
    "    \n",
    "    return precision, recall, accuracy, f1\n",
    "\n",
    "def log(x):\n",
    "    print(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing BernoulliNB\n",
      "Learning time 0.735954999923706s\n",
      "Predicting time 0.2744331359863281s\n",
      "==================Results=======================\n",
      "            Negative     Neutral    Positive\n",
      "F1         [0.85962955 0.57930108 0.60245515]\n",
      "Precision  [0.8458498  0.5731383  0.65773196]\n",
      "Recall     [0.8738657  0.58559783 0.55574913]\n",
      "Accuracy   0.7615253272623791\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "#Classifier : BagOfWords + NaiveBayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:, 1:], bow.iloc[:, 0],\n",
    "                                                   train_size = 0.7, stratify=bow.iloc[:, 0],\n",
    "                                                   random_state = seed)\n",
    "\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NaiveBayes with 8 fold cross-validation\n",
    "\n",
    "def cv(classifier, X_train, y_train):\n",
    "    log(\"===============================================\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    now = time()\n",
    "    log(\"Crossvalidating \" + classifier_name + \"...\")\n",
    "    accuracy = [cross_val_score(classifier, X_train, y_train, cv=8, n_jobs=-1)]\n",
    "    log(\"Crosvalidation completed in {0}s\".format(time() - now))\n",
    "    log(\"Accuracy: \" + str(accuracy[0]))\n",
    "    log(\"Average accuracy: \" + str(np.array(accuracy[0]).mean()))\n",
    "    log(\"===============================================\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Crossvalidating BernoulliNB...\n",
      "Crosvalidation completed in 10.928978681564331s\n",
      "Accuracy: [0.7665529  0.75767918 0.77269625 0.75767918 0.75734792 0.74641148\n",
      " 0.74914559 0.75803144]\n",
      "Average accuracy: 0.7581929925651858\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "nb_acc = cv(BernoulliNB(), bow.iloc[:,1:], bow.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Addtion of extra features:\n",
    "\n",
    "# Number of Uppercase - tend to express postive/negative emotions by using uppercase words\n",
    "# Number of !         - exclamation marks are likely to increase strength of opinion\n",
    "# Number of ?         - might distinguish neutral tweets - seeking information\n",
    "# Number of positive  - positive emoji will most likely occur in positive tweets\n",
    "# emoticons\n",
    "# Number of negative  - Inverse to the one above\n",
    "# emoticons\n",
    "# Number of ...       - commonly used in commenting something\n",
    "# Number of quotations- same as above\n",
    "# Number of mentions  - Lots of mentions on positive tweets, to share something good/bad\n",
    "# Number of urls      - similar to number of mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Detecting Emoticons\n",
    "class EmoticonDetector:\n",
    "    emoticons = {}\n",
    "    \n",
    "    def __init__(self, emoticon_file=\".\\\\emoticons.txt\"):\n",
    "        from pathlib import Path\n",
    "        content = Path(emoticon_file).read_text()\n",
    "        positive = True\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if \"positive\" in line.lower():\n",
    "                positive = True\n",
    "                continue\n",
    "            elif \"negative\" in line.lower():\n",
    "                positive = False\n",
    "                continue\n",
    "            \n",
    "            self.emoticons[line] = positive\n",
    "    \n",
    "    def is_positive(self, emoticon):\n",
    "        if emoticon in self.emoticons:\n",
    "            return self.emoticons[emoticon]\n",
    "        return False\n",
    "    \n",
    "    def is_emoticon(self, to_check):\n",
    "        return to_check in self.emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtraFeatures(WordList):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_data_model(self):\n",
    "        extra_columns = [col for col in self.processed_Traindata.columns if col.startswith(\"number_of\")]\n",
    "        label_column = [\"label\"]\n",
    "        columns = label_column + extra_columns + list(\n",
    "                map(lambda w: w + \"_bow\", self.wordlist))\n",
    "        \n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_Traindata.index:\n",
    "            current_row = []\n",
    "            current_label = self.processed_Traindata.loc[idx, \"airline_sentiment\"]\n",
    "            labels.append(current_label)\n",
    "            current_row.append(current_label)\n",
    "        \n",
    "            for _,col in enumerate(extra_columns):\n",
    "                current_row.append(self.processed_Traindata.loc[idx, col])\n",
    "        \n",
    "        #adding bad-of-words\n",
    "            tokens = set(self.processed_Traindata.loc[idx, \"text\"])\n",
    "            for _,word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "        \n",
    "            rows.append(current_row)\n",
    "        \n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        \n",
    "        return self.data_model, self.data_labels\n",
    "    \n",
    "    def add_column(self, column_name, column_content):\n",
    "        self.processed_Traindata.loc[:, column_name] = pd.Series(column_content, index=self.processed_Traindata.index)\n",
    "\n",
    "    def build_features(self):\n",
    "        def count_by_lambda(expression, word_array):\n",
    "            return len(list(filter(expression, word_array)))\n",
    "        \n",
    "        def count_occurences(character, word_array):\n",
    "            counter = 0\n",
    "            for j, word in enumerate(word_array):\n",
    "                for char in word:\n",
    "                    if char == character:\n",
    "                        counter += 1\n",
    "            return counter\n",
    "        \n",
    "        def count_by_regex(regex, plain_text):\n",
    "            return len(regex.findall(plain_text))\n",
    "        \n",
    "        self.add_column(\"splitted_text\", map(lambda txt: txt.split(\" \"), self.processed_Traindata[\"text\"]))\n",
    "        \n",
    "        #Number of uppercase words\n",
    "        uppercase = list(map(lambda txt: count_by_lambda(lambda word: word == word.upper(), txt),\n",
    "                                                        self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_uppercase\", uppercase)\n",
    "        \n",
    "        #number of !\n",
    "        exclamations = list(map(lambda txt: count_occurences(\"!\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_exclamation\", exclamations)\n",
    "        \n",
    "        #number of ?\n",
    "        questions = list(map(lambda txt: count_occurences(\"?\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_question\", questions)\n",
    "        \n",
    "        #number of ...\n",
    "        ellipsis = list(map(lambda txt: count_by_regex(regex.compile(r\"\\.\\s?\\.\\s?\\.\"), txt),\n",
    "                           self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_ellipsis\", ellipsis)\n",
    "        \n",
    "        #number of hashtags\n",
    "        hashtags = list(map(lambda txt: count_occurences(\"#\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_hashtags\", hashtags)\n",
    "        \n",
    "        #number of mentions\n",
    "        mentions = list(map(lambda txt: count_occurences(\"@\", txt),\n",
    "                               self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_mentions\", mentions)\n",
    "        \n",
    "        #number of quotes\n",
    "        quotes = list(map(lambda plain_text: int(count_occurences(\"'\", [plain_text.strip(\"'\").strip('\"')]) / 2 +\n",
    "                                                 count_occurences('\"', [plain_text.strip(\"'\").strip('\"')]) / 2),\n",
    "                          self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_quotes\", quotes)\n",
    "        \n",
    "        #number of urls\n",
    "        urls = list(map(lambda txt: count_by_regex(regex.compile(r\"http.?://[^\\s]+[\\s]?\"), txt),\n",
    "                             self.processed_Traindata[\"text\"]))\n",
    "        self.add_column(\"number_of_urls\", urls)\n",
    "        \n",
    "        #number of positive emoticons\n",
    "        ed = EmoticonDetector()\n",
    "        positive_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and ed.is_positive(word), txt), \n",
    "                   self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_positive_emo\", positive_emo)\n",
    "        \n",
    "        #number of negative emoticons\n",
    "        negative_emo = list(\n",
    "            map(lambda txt: count_by_lambda(lambda word: ed.is_emoticon(word) and not ed.is_positive(word), txt), \n",
    "                   self.processed_Traindata[\"splitted_text\"]))\n",
    "        self.add_column(\"number_of_negative_emo\", negative_emo)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:337: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3813: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = ExtraFeatures()\n",
    "data.initialize(\"Tweets.csv\")\n",
    "data.build_features()\n",
    "data.cleaningData(DataPreprocessing())\n",
    "data.tokenize()\n",
    "data.stem()\n",
    "data.buildWordlist()\n",
    "data_model, labels = data.build_data_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          "positive",
          "neutral",
          "negative"
         ],
         "y": [
          115,
          39,
          19
         ]
        }
       ],
       "layout": {
        "title": "How feature \"number_of_positive_emo\" separates the tweets"
       }
      },
      "text/html": [
       "<div id=\"d1b09d5c-f603-461a-9f4d-ef39dc5c96d5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d1b09d5c-f603-461a-9f4d-ef39dc5c96d5\", [{\"type\": \"bar\", \"x\": [\"positive\", \"neutral\", \"negative\"], \"y\": [115, 39, 19]}], {\"title\": \"How feature \\\"number_of_positive_emo\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"d1b09d5c-f603-461a-9f4d-ef39dc5c96d5\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"d1b09d5c-f603-461a-9f4d-ef39dc5c96d5\", [{\"type\": \"bar\", \"x\": [\"positive\", \"neutral\", \"negative\"], \"y\": [115, 39, 19]}], {\"title\": \"How feature \\\"number_of_positive_emo\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          "positive",
          "neutral",
          "negative"
         ],
         "y": [
          5,
          15,
          68
         ]
        }
       ],
       "layout": {
        "title": "How feature \"number_of_negative_emo\" separates the tweets"
       }
      },
      "text/html": [
       "<div id=\"988bcbd4-28bc-47fa-ada1-dfabc68a7d48\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"988bcbd4-28bc-47fa-ada1-dfabc68a7d48\", [{\"type\": \"bar\", \"x\": [\"positive\", \"neutral\", \"negative\"], \"y\": [5, 15, 68]}], {\"title\": \"How feature \\\"number_of_negative_emo\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"988bcbd4-28bc-47fa-ada1-dfabc68a7d48\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"988bcbd4-28bc-47fa-ada1-dfabc68a7d48\", [{\"type\": \"bar\", \"x\": [\"positive\", \"neutral\", \"negative\"], \"y\": [5, 15, 68]}], {\"title\": \"How feature \\\"number_of_negative_emo\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          "positive",
          "neutral",
          "negative"
         ],
         "y": [
          970,
          368,
          1254
         ]
        }
       ],
       "layout": {
        "title": "How feature \"number_of_exclamation\" separates the tweets"
       }
      },
      "text/html": [
       "<div id=\"229c047f-0bd9-4be5-bf96-2907a0d283ef\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"229c047f-0bd9-4be5-bf96-2907a0d283ef\", [{\"type\": \"bar\", \"x\": [\"positive\", \"neutral\", \"negative\"], \"y\": [970, 368, 1254]}], {\"title\": \"How feature \\\"number_of_exclamation\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"229c047f-0bd9-4be5-bf96-2907a0d283ef\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"229c047f-0bd9-4be5-bf96-2907a0d283ef\", [{\"type\": \"bar\", \"x\": [\"positive\", \"neutral\", \"negative\"], \"y\": [970, 368, 1254]}], {\"title\": \"How feature \\\"number_of_exclamation\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          "positive",
          "neutral",
          "negative"
         ],
         "y": [
          350,
          351,
          1286
         ]
        }
       ],
       "layout": {
        "title": "How feature \"number_of_hashtags\" separates the tweets"
       }
      },
      "text/html": [
       "<div id=\"f893e0f9-9847-4697-b178-b29f8863e7d3\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f893e0f9-9847-4697-b178-b29f8863e7d3\", [{\"type\": \"bar\", \"x\": [\"positive\", \"neutral\", \"negative\"], \"y\": [350, 351, 1286]}], {\"title\": \"How feature \\\"number_of_hashtags\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"f893e0f9-9847-4697-b178-b29f8863e7d3\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"f893e0f9-9847-4697-b178-b29f8863e7d3\", [{\"type\": \"bar\", \"x\": [\"positive\", \"neutral\", \"negative\"], \"y\": [350, 351, 1286]}], {\"title\": \"How feature \\\"number_of_hashtags\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "bar",
         "x": [
          "positive",
          "neutral",
          "negative"
         ],
         "y": [
          85,
          968,
          1891
         ]
        }
       ],
       "layout": {
        "title": "How feature \"number_of_question\" separates the tweets"
       }
      },
      "text/html": [
       "<div id=\"7286d8a0-ce11-49b6-874c-d6336b18221c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"7286d8a0-ce11-49b6-874c-d6336b18221c\", [{\"type\": \"bar\", \"x\": [\"positive\", \"neutral\", \"negative\"], \"y\": [85, 968, 1891]}], {\"title\": \"How feature \\\"number_of_question\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"7286d8a0-ce11-49b6-874c-d6336b18221c\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"7286d8a0-ce11-49b6-874c-d6336b18221c\", [{\"type\": \"bar\", \"x\": [\"positive\", \"neutral\", \"negative\"], \"y\": [85, 968, 1891]}], {\"title\": \"How feature \\\"number_of_question\\\" separates the tweets\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentiments = [\"positive\", \"neutral\", \"negative\"]\n",
    "plots_data_ef = []\n",
    "\n",
    "for what in map(lambda o: \"number_of_\"+o,[\"positive_emo\",\"negative_emo\",\"exclamation\",\"hashtags\",\"question\"]):\n",
    "    ef_grouped = data_model[data_model[what]>=1].groupby([\"label\"]).count()\n",
    "    plots_data_ef.append({\"data\":[graph_objs.Bar(\n",
    "            x = sentiments,\n",
    "            y = [ef_grouped.loc[s,:][0] for s in sentiments],\n",
    "    )], \"title\":\"How feature \\\"\"+what+\"\\\" separates the tweets\"})\n",
    "    \n",
    "for plot_data_ef in plots_data_ef:\n",
    "    plotly.offline.iplot({\n",
    "            \"data\":plot_data_ef[\"data\"],\n",
    "            \"layout\":graph_objs.Layout(title=plot_data_ef[\"title\"])\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing RandomForestClassifier\n",
      "Learning time 19.51445460319519s\n",
      "Predicting time 0.40056657791137695s\n",
      "==================Results=======================\n",
      "            Negative     Neutral    Positive\n",
      "F1         [0.85174044 0.5046729  0.6293578 ]\n",
      "Precision  [0.80964052 0.6        0.65209125]\n",
      "Recall     [0.89845875 0.43548387 0.60815603]\n",
      "Accuracy   0.7538417757541264\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "#Extended Features + Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_model.iloc[:, 1:], data_model.iloc[:, 0],\n",
    "                                                    train_size=0.7, stratify=data_model.iloc[:, 0],\n",
    "                                                    random_state=seed)\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, RandomForestClassifier(random_state=seed,n_estimators=403,n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "Crossvalidating RandomForestClassifier...\n",
      "Crosvalidation completed in 221.5852508544922s\n",
      "Accuracy: [0.75290102 0.74931694 0.7670765  0.76434426 0.75546448 0.7636612\n",
      " 0.76502732 0.76486671]\n",
      "Average accuracy: 0.760332305813554\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "#Crosvalidation\n",
    "rf_acc = cv(RandomForestClassifier(n_estimators=403,n_jobs=-1, random_state=seed),data_model.iloc[:, 1:], data_model.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing BernoulliNB\n",
      "Learning time 0.9169924259185791s\n",
      "Predicting time 0.36939239501953125s\n",
      "==================Results=======================\n",
      "            Negative     Neutral    Positive\n",
      "F1         [0.85962955 0.57930108 0.60245515]\n",
      "Precision  [0.8458498  0.5731383  0.65773196]\n",
      "Recall     [0.8738657  0.58559783 0.55574913]\n",
      "Accuracy   0.7615253272623791\n",
      "================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:, 1:], bow.iloc[:, 0],\n",
    "                                                   train_size = 0.7, stratify=bow.iloc[:, 0],\n",
    "                                                   random_state = seed)\n",
    "\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, BernoulliNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tweet_id airline_sentiment         airline  \\\n",
      "5704   5.687940e+17           neutral       Southwest   \n",
      "8341   5.684710e+17           neutral           Delta   \n",
      "982    5.699760e+17          negative          United   \n",
      "9058   5.702630e+17          negative      US Airways   \n",
      "13171  5.699210e+17          negative        American   \n",
      "1271   5.698590e+17          negative          United   \n",
      "4467   5.702280e+17           neutral       Southwest   \n",
      "3472   5.684130e+17          negative          United   \n",
      "2881   5.688510e+17          negative          United   \n",
      "13614  5.697990e+17          negative        American   \n",
      "8461   5.682400e+17           neutral           Delta   \n",
      "4380   5.702810e+17          negative       Southwest   \n",
      "12608  5.700880e+17          negative        American   \n",
      "10983  5.686390e+17          negative      US Airways   \n",
      "8001   5.688900e+17           neutral           Delta   \n",
      "13285  5.698960e+17          negative        American   \n",
      "12978  5.699700e+17          negative        American   \n",
      "5288   5.692120e+17          negative       Southwest   \n",
      "5937   5.684710e+17          positive       Southwest   \n",
      "9511   5.698980e+17          negative      US Airways   \n",
      "112    5.698740e+17           neutral  Virgin America   \n",
      "12884  5.699920e+17          negative        American   \n",
      "14279  5.696430e+17          negative        American   \n",
      "8390   5.684300e+17          negative           Delta   \n",
      "13301  5.698940e+17          negative        American   \n",
      "12577  5.701000e+17          negative        American   \n",
      "2019   5.694720e+17          positive          United   \n",
      "1411   5.697310e+17          positive          United   \n",
      "1762   5.695980e+17          positive          United   \n",
      "4799   5.697200e+17          negative       Southwest   \n",
      "...             ...               ...             ...   \n",
      "110    5.698740e+17           neutral  Virgin America   \n",
      "9739   5.696860e+17          negative      US Airways   \n",
      "2060   5.694150e+17          negative          United   \n",
      "1118   5.699220e+17          negative          United   \n",
      "5229   5.692480e+17          negative       Southwest   \n",
      "2895   5.688430e+17           neutral          United   \n",
      "1748   5.696050e+17          negative          United   \n",
      "11447  5.681100e+17          negative      US Airways   \n",
      "2064   5.694040e+17          negative          United   \n",
      "1025   5.699610e+17          negative          United   \n",
      "2410   5.692220e+17          negative          United   \n",
      "11349  5.681780e+17          negative      US Airways   \n",
      "11005  5.686210e+17          negative      US Airways   \n",
      "2850   5.688690e+17          negative          United   \n",
      "12420  5.701940e+17          negative        American   \n",
      "633    5.702360e+17          negative          United   \n",
      "2427   5.692130e+17           neutral          United   \n",
      "13184  5.699190e+17          positive        American   \n",
      "11063  5.685450e+17           neutral      US Airways   \n",
      "2071   5.693910e+17           neutral          United   \n",
      "5764   5.686300e+17          negative       Southwest   \n",
      "3921   5.679680e+17           neutral          United   \n",
      "5091   5.693550e+17          negative       Southwest   \n",
      "9386   5.699600e+17          negative      US Airways   \n",
      "6505   5.678130e+17          negative       Southwest   \n",
      "14546  5.696010e+17           neutral        American   \n",
      "2441   5.692050e+17          negative          United   \n",
      "1497   5.696980e+17          negative          United   \n",
      "14525  5.696040e+17          negative        American   \n",
      "7740   5.692870e+17           neutral           Delta   \n",
      "\n",
      "                                                    text  topic_label  \\\n",
      "5704   [who, can, i, contact, about, request, a, char...            2   \n",
      "8341   [ever, consid, come, to, memphi, need, some, t...            2   \n",
      "982    [you, are, easili, the, worst, compani, i, hav...            2   \n",
      "9058   [day, no, coffe, amp, work, toilet, then, i, r...            2   \n",
      "13171  [last, flight, ever, on, aa, you, botch, thi, ...            2   \n",
      "1271   [no, thank, im, sick, of, your, compani, lousi...            2   \n",
      "4467   [stepup, makeitright, re, you, best, or, just,...            2   \n",
      "3472   [sure, youv, been, say, that, for, as, long, a...            2   \n",
      "2881   [after, sever, trial, and, sever, hour, of, wa...            2   \n",
      "13614  [thank, you, for, the, worst, experi, ever, on...            2   \n",
      "8461   [“, jetblu, relat, flightd, thi, time, ”, no, ...            2   \n",
      "4380                         [your, phone, servic, suck]            2   \n",
      "12608  [the, last, wa, dfw, to, ord, not, weather, re...            2   \n",
      "10983            [offload, the, plane, thi, is, ridicul]            2   \n",
      "8001      [im, fli, your, airlin, just, out, of, lga, 😷]            2   \n",
      "13285  [contact, me, or, do, someth, to, allevi, thi,...            2   \n",
      "12978  [from, the, rude, ticket, counter, employe, to...            2   \n",
      "5288   [i, fli, back, to, omaha, for, my, grandmoth, ...            2   \n",
      "5937   [now, fli, non, stop, cmhoak, ha, me, daydream...            2   \n",
      "9511   [thank, for, a, subpar, travel, experi, and, i...            2   \n",
      "112    [ha, getaway, deal, through, may, from, $, one...            2   \n",
      "12884  [prob, the, worst, custom, servic, ever, repli...            2   \n",
      "14279  [they, could, be, seen, if, your, horribl, cus...            2   \n",
      "8390   [claim, w, custom, protect, they, will, notifi...            2   \n",
      "13301  [as, of, now, will, do, my, best, to, never, f...            2   \n",
      "12577  [how, about, connect, me, with, a, custom, ser...            2   \n",
      "2019                 [i, still, like, you, unit, airlin]            2   \n",
      "1411   [your, my, earli, frontrunn, for, best, airlin...            2   \n",
      "1762      [thi, is, whi, i, fli, never, have, ani, issu]            2   \n",
      "4799   [custom, servic, ha, been, veri, passiv, in, t...            2   \n",
      "...                                                  ...          ...   \n",
      "110    [ha, getaway, deal, through, may, from, $, one...            2   \n",
      "9739   [worst, servic, at, reagan, int, i, rare, post...            2   \n",
      "2060   [an, inconveni, is, a, weather, delay, your, p...            2   \n",
      "1118   [there, are, a, lot, of, unhappi, cold, peopl,...            2   \n",
      "5229   [ha, the, absolut, worst, custom, servic, when...            2   \n",
      "2895   [it, just, goe, round, amp, round, it, be, go,...            2   \n",
      "1748   [i, wa, rebook, but, had, to, take, bu, from, ...            2   \n",
      "11447  [are, you, an, incompet, airlin, everi, day, o...            2   \n",
      "2064   [aftermin, of, avoid, nonweath, delay, that, a...            2   \n",
      "1025   [pedophil, airlin, split, myself, and, my, yr,...            2   \n",
      "2410   [well, if, you, dont, want, your, custom, to, ...            2   \n",
      "11349                     [just, lost, a, faith, custom]            2   \n",
      "11005  [i, bet, you, dont, but, im, the, pay, custom,...            2   \n",
      "2850   [thank, for, remind, me, how, much, easier, it...            2   \n",
      "12420  [requir, custom, to, fill, out, all, the, cont...            2   \n",
      "633    [by, the, way, a, simpl, apolog, goe, a, long,...            2   \n",
      "2427   [i, take, it, as, a, compliment, that, i, wa, ...            2   \n",
      "13184  [it, realli, the, small, thingsth, detailsthat...            2   \n",
      "11063  [vuelo, vuelos, sell, too, cheap, madnyc, with...            2   \n",
      "2071   [i, have, $, and, ill, draw, you, a, super, sw...            2   \n",
      "5764   [whi, do, airlin, chang, ticket, price, in, th...            2   \n",
      "3921   [speak, of, my, flight, skateboard, are, allow...            2   \n",
      "5091   [weather, delay, arent, your, fault, today, bu...            2   \n",
      "9386   [i, find, it, funni, that, respond, but, you, ...            2   \n",
      "6505   [i, told, myself, last, time, i, would, never,...            2   \n",
      "14546  [how, realist, is, it, to, make, an, minut, do...            2   \n",
      "2441   [rememb, in, busi, a, pleasant, experi, mean, ...            2   \n",
      "1497   [what, about, the, poor, custom, servic, at, c...            2   \n",
      "14525  [more, of, the, insan, treatment, by, your, cu...            2   \n",
      "7740                                   [boston, gate, c]            2   \n",
      "\n",
      "                                           splitted_text  number_of_uppercase  \\\n",
      "5704   [@SouthwestAir, Who, can, I, contact, about, r...                    1   \n",
      "8341   [@JetBlue, Ever, consider, coming, to, Memphis...                    1   \n",
      "982    [@united, you, are, easily, the, worst, compan...                    1   \n",
      "9058   [@USAirways, 2day:, no, coffee, &amp;, 1, work...                    6   \n",
      "13171  [@AmericanAir, #last, #flight, #ever, on, #AA....                    1   \n",
      "1271   [@united, No,, thanks., I'm, sick, of, your, c...                    0   \n",
      "4467   [@SouthwestAir, #stepup, #makeitright, re, you...                    0   \n",
      "3472   [@united, sure..., You've, been, saying, that,...                    1   \n",
      "2881   [@united, after, several, trials, and, several...                    1   \n",
      "13614  [@AmericanAir, Thank, you, for, the, worst, ex...                    1   \n",
      "8461   [“@JetBlue:, @KyleComer_, JetBlue, reLate, Fli...                    1   \n",
      "4380        [@SouthwestAir, your, phone, service, sucks]                    0   \n",
      "12608  [@AmericanAir, the, last, was, 2339, dfw, to, ...                    1   \n",
      "10983  [@USAirways, offloading, the, plane?!?!?!, Thi...                    0   \n",
      "8001   [@JetBlue, I'm, flying, your, airline, just, o...                    2   \n",
      "13285  [@AmericanAir, @united, contact, me,, or, do, ...                    1   \n",
      "12978  [@AmericanAir, from, the, rude, ticket, counte...                    0   \n",
      "5288   [@SouthwestAir, I, fly, back, to, Omaha, for, ...                    1   \n",
      "5937   [@SouthwestAir, now, flying, non, stop, CMH-OA...                    1   \n",
      "9511   [@USAirways, thanks, for, a, subpar, travel, e...                    0   \n",
      "112    [@VirginAmerica, has, getaway, deals, through,...                    1   \n",
      "12884  [@AmericanAir, probs, the, worst, customer, se...                    2   \n",
      "14279  [@AmericanAir, they, could, be, seen, if, your...                    0   \n",
      "8390   [@JetBlue, claims, w/, customer, protection, t...                    0   \n",
      "13301  [@AmericanAir, as, of, now,, will, do, my, bes...                    0   \n",
      "12577  [.@AmericanAir, how, about, connecting, me, wi...                    0   \n",
      "2019   [@united, @suntoshi, I, still, like, you, unit...                    1   \n",
      "1411   [@united, you're, my, early, frontrunner, for,...                    0   \n",
      "1762   [@united, this, is, why, I, fly, @SouthwestAir...                    2   \n",
      "4799   [@SouthwestAir, @jparkermastin, , customer, se...                    3   \n",
      "...                                                  ...                  ...   \n",
      "110    [@VirginAmerica, has, getaway, deals, through,...                    1   \n",
      "9739   [@USAirways, worst, service, at, Reagan, Int.,...                    2   \n",
      "2060   [@united, an, inconvenience, is, a, weather, d...                    1   \n",
      "1118   [@united, there, are, a, lot, of, unhappy, col...                    0   \n",
      "5229   [@SouthwestAir, has, the, absolute, worst, cus...                    0   \n",
      "2895   [@united, it, just, goes, round, &amp;, round,...                    0   \n",
      "1748   [@united, I, was, rebooked, but, had, to, take...                    1   \n",
      "11447  [@USAirways, \\nAre, you, an, incompetent, airl...                    0   \n",
      "2064   [@united, after, 443min, of, avoidable, non-we...                    0   \n",
      "1025   [@united, pedophile, airline., Split, myself, ...                    2   \n",
      "2410   [@united, well, if, you, don't, want, your, cu...                    0   \n",
      "11349   [@USAirways, just, lost, a, faithful, customer.]                    0   \n",
      "11005  [@USAirways, -, I, bet, you, don't,, but, I'm,...                    3   \n",
      "2850   [@united, thanks, for, reminding, me, how, muc...                    2   \n",
      "12420  [@AmericanAir, Requiring, customers, to, fill,...                    0   \n",
      "633    [@united, By, the, way,, a, simple, apology, g...                    0   \n",
      "2427   [@united, I, take, it, as, a, compliment, that...                    3   \n",
      "13184  [.@AmericanAir, @TyWinter, it's, really, the, ...                    0   \n",
      "11063  [@USAirways, http://t.co/AXRYeIWzh0,, vuelo24....                    1   \n",
      "2071   [@united, I, have, $20, and, I'll, draw, you, ...                    3   \n",
      "5764   [@SouthwestAir, Why, do, airlines, change, tic...                    0   \n",
      "3921   [@united, speaking, of, my, flight,, skateboar...                    0   \n",
      "5091   [@SouthwestAir, weather, delays, aren't, your,...                    0   \n",
      "9386   [@USAirways, I, find, it, funny, that, @PHLAir...                    1   \n",
      "6505   [@SouthwestAir, I, told, myself, last, time, I...                    4   \n",
      "14546  [@AmericanAir, how, realistic, is, it, to, mak...                    3   \n",
      "2441   [@united, remember, in, business,, a, pleasant...                    1   \n",
      "1497   [@united, what, about, the, poor, customer, se...                    1   \n",
      "14525  [@AmericanAir, more, of, the, insane, treatmen...                    0   \n",
      "7740                       [@JetBlue, Boston, gate, C12]                    1   \n",
      "\n",
      "       number_of_exclamation  number_of_question  number_of_ellipsis  \\\n",
      "5704                       0                   1                   0   \n",
      "8341                       2                   1                   0   \n",
      "982                        1                   0                   1   \n",
      "9058                       0                   1                   0   \n",
      "13171                      0                   0                   0   \n",
      "1271                       0                   0                   0   \n",
      "4467                       0                   2                   0   \n",
      "3472                       0                   0                   1   \n",
      "2881                       0                   0                   0   \n",
      "13614                      0                   0                   0   \n",
      "8461                       0                   2                   1   \n",
      "4380                       0                   0                   0   \n",
      "12608                      0                   0                   1   \n",
      "10983                      7                   3                   0   \n",
      "8001                       0                   0                   0   \n",
      "13285                      0                   0                   0   \n",
      "12978                      0                   0                   0   \n",
      "5288                       0                   0                   0   \n",
      "5937                       0                   0                   1   \n",
      "9511                       0                   0                   0   \n",
      "112                        0                   0                   0   \n",
      "12884                      1                   0                   1   \n",
      "14279                      2                   0                   0   \n",
      "8390                       0                   0                   0   \n",
      "13301                      0                   0                   0   \n",
      "12577                      1                   1                   0   \n",
      "2019                       0                   0                   0   \n",
      "1411                       1                   0                   0   \n",
      "1762                       0                   0                   1   \n",
      "4799                       0                   0                   0   \n",
      "...                      ...                 ...                 ...   \n",
      "110                        0                   0                   0   \n",
      "9739                       0                   0                   0   \n",
      "2060                       0                   0                   0   \n",
      "1118                       0                   0                   0   \n",
      "5229                       0                   0                   0   \n",
      "2895                       0                   0                   0   \n",
      "1748                       0                   0                   0   \n",
      "11447                      0                   1                   0   \n",
      "2064                       0                   0                   0   \n",
      "1025                       0                   0                   0   \n",
      "2410                       2                   0                   0   \n",
      "11349                      0                   0                   0   \n",
      "11005                      0                   0                   0   \n",
      "2850                       2                   0                   0   \n",
      "12420                      0                   0                   0   \n",
      "633                        0                   0                   0   \n",
      "2427                       0                   0                   1   \n",
      "13184                      0                   0                   0   \n",
      "11063                      0                   1                   0   \n",
      "2071                       0                   1                   0   \n",
      "5764                       0                   0                   0   \n",
      "3921                       0                   2                   0   \n",
      "5091                       0                   0                   0   \n",
      "9386                       0                   0                   0   \n",
      "6505                       0                   0                   0   \n",
      "14546                      0                   1                   0   \n",
      "2441                       0                   0                   0   \n",
      "1497                       1                   5                   0   \n",
      "14525                      0                   0                   0   \n",
      "7740                       0                   0                   0   \n",
      "\n",
      "       number_of_hashtags  number_of_mentions  number_of_quotes  \\\n",
      "5704                    1                   1                 0   \n",
      "8341                    0                   1                 0   \n",
      "982                     0                   1                 1   \n",
      "9058                    1                   1                 0   \n",
      "13171                   4                   2                 0   \n",
      "1271                    0                   1                 1   \n",
      "4467                    3                   1                 0   \n",
      "3472                    0                   1                 1   \n",
      "2881                    0                   1                 0   \n",
      "13614                   0                   1                 0   \n",
      "8461                    0                   2                 1   \n",
      "4380                    0                   1                 0   \n",
      "12608                   0                   1                 0   \n",
      "10983                   0                   1                 0   \n",
      "8001                    1                   1                 0   \n",
      "13285                   0                   2                 0   \n",
      "12978                   0                   1                 0   \n",
      "5288                    0                   1                 0   \n",
      "5937                    1                   1                 0   \n",
      "9511                    1                   1                 0   \n",
      "112                     2                   1                 0   \n",
      "12884                   0                   1                 0   \n",
      "14279                   0                   1                 0   \n",
      "8390                    1                   1                 1   \n",
      "13301                   1                   1                 0   \n",
      "12577                   0                   1                 0   \n",
      "2019                    0                   2                 0   \n",
      "1411                    1                   1                 0   \n",
      "1762                    0                   2                 0   \n",
      "4799                    0                   2                 0   \n",
      "...                   ...                 ...               ...   \n",
      "110                     2                   1                 0   \n",
      "9739                    0                   1                 0   \n",
      "2060                    0                   1                 0   \n",
      "1118                    0                   1                 0   \n",
      "5229                    0                   1                 0   \n",
      "2895                    0                   1                 0   \n",
      "1748                    0                   1                 0   \n",
      "11447                   0                   1                 1   \n",
      "2064                    2                   1                 0   \n",
      "1025                    0                   1                 0   \n",
      "2410                    1                   1                 0   \n",
      "11349                   0                   1                 0   \n",
      "11005                   0                   1                 1   \n",
      "2850                    0                   1                 0   \n",
      "12420                   0                   1                 0   \n",
      "633                     0                   1                 1   \n",
      "2427                    0                   1                 0   \n",
      "13184                   0                   2                 0   \n",
      "11063                   0                   1                 0   \n",
      "2071                    0                   1                 1   \n",
      "5764                    1                   1                 0   \n",
      "3921                    0                   1                 0   \n",
      "5091                    0                   1                 0   \n",
      "9386                    1                   2                 0   \n",
      "6505                    0                   1                 0   \n",
      "14546                   0                   1                 0   \n",
      "2441                    0                   1                 0   \n",
      "1497                    0                   1                 0   \n",
      "14525                   0                   1                 0   \n",
      "7740                    0                   1                 0   \n",
      "\n",
      "       number_of_urls  number_of_positive_emo  number_of_negative_emo  \\\n",
      "5704                0                       0                       0   \n",
      "8341                0                       0                       0   \n",
      "982                 0                       0                       0   \n",
      "9058                0                       0                       0   \n",
      "13171               0                       0                       0   \n",
      "1271                0                       0                       0   \n",
      "4467                0                       0                       0   \n",
      "3472                0                       0                       0   \n",
      "2881                0                       0                       0   \n",
      "13614               0                       0                       0   \n",
      "8461                0                       0                       0   \n",
      "4380                0                       0                       0   \n",
      "12608               0                       0                       0   \n",
      "10983               0                       0                       0   \n",
      "8001                0                       0                       0   \n",
      "13285               0                       0                       0   \n",
      "12978               0                       0                       0   \n",
      "5288                0                       0                       0   \n",
      "5937                0                       0                       0   \n",
      "9511                0                       0                       0   \n",
      "112                 1                       0                       0   \n",
      "12884               0                       0                       0   \n",
      "14279               0                       0                       0   \n",
      "8390                1                       0                       0   \n",
      "13301               0                       0                       0   \n",
      "12577               0                       0                       0   \n",
      "2019                0                       0                       0   \n",
      "1411                0                       0                       0   \n",
      "1762                0                       0                       0   \n",
      "4799                0                       0                       0   \n",
      "...               ...                     ...                     ...   \n",
      "110                 1                       0                       0   \n",
      "9739                0                       0                       0   \n",
      "2060                0                       0                       0   \n",
      "1118                0                       0                       0   \n",
      "5229                0                       0                       0   \n",
      "2895                0                       0                       0   \n",
      "1748                0                       0                       0   \n",
      "11447               0                       0                       0   \n",
      "2064                0                       0                       0   \n",
      "1025                0                       0                       0   \n",
      "2410                0                       0                       0   \n",
      "11349               0                       0                       0   \n",
      "11005               0                       0                       0   \n",
      "2850                0                       0                       0   \n",
      "12420               0                       0                       0   \n",
      "633                 0                       0                       0   \n",
      "2427                0                       1                       0   \n",
      "13184               0                       0                       0   \n",
      "11063               1                       0                       0   \n",
      "2071                0                       0                       0   \n",
      "5764                0                       0                       0   \n",
      "3921                0                       0                       0   \n",
      "5091                0                       0                       0   \n",
      "9386                0                       0                       0   \n",
      "6505                0                       0                       0   \n",
      "14546               0                       0                       0   \n",
      "2441                0                       0                       0   \n",
      "1497                0                       0                       0   \n",
      "14525               0                       0                       0   \n",
      "7740                0                       0                       0   \n",
      "\n",
      "                                           tokenizedText  \n",
      "5704   [Who, can, I, contact, about, requesting, a, c...  \n",
      "8341   [Ever, consider, coming, to, Memphis, Need, so...  \n",
      "982    [you, are, easily, the, worst, company, I, hav...  \n",
      "9058   [day, no, coffee, amp, working, toilet, Then, ...  \n",
      "13171  [last, flight, ever, on, AA, You, botched, thi...  \n",
      "1271   [No, thanks, Im, sick, of, your, companys, lou...  \n",
      "4467   [stepup, makeitright, re, you, best, Or, just,...  \n",
      "3472   [sure, Youve, been, saying, that, for, as, lon...  \n",
      "2881   [after, several, trials, and, several, hours, ...  \n",
      "13614  [Thank, you, for, the, worst, experience, ever...  \n",
      "8461   [“, JetBlue, reLate, Flightd, this, time, ”, n...  \n",
      "4380                       [your, phone, service, sucks]  \n",
      "12608  [the, last, was, dfw, to, ord, Not, weather, r...  \n",
      "10983     [offloading, the, plane, This, is, ridiculous]  \n",
      "8001   [Im, flying, your, airline, just, out, of, LGA...  \n",
      "13285  [contact, me, or, do, something, to, alleviate...  \n",
      "12978  [from, the, rude, ticket, counter, employees, ...  \n",
      "5288   [I, fly, back, to, Omaha, for, my, grandmother...  \n",
      "5937   [now, flying, non, stop, CMHOAK, has, me, dayd...  \n",
      "9511   [thanks, for, a, subpar, travel, experience, a...  \n",
      "112    [has, getaway, deals, through, May, from, $, o...  \n",
      "12884  [probs, the, worst, customer, service, ever, R...  \n",
      "14279  [they, could, be, seen, if, your, horrible, cu...  \n",
      "8390   [claims, w, customer, protection, they, will, ...  \n",
      "13301  [as, of, now, will, do, my, best, to, never, f...  \n",
      "12577  [how, about, connecting, me, with, a, customer...  \n",
      "2019             [I, still, like, you, united, airlines]  \n",
      "1411   [youre, my, early, frontrunner, for, best, air...  \n",
      "1762   [this, is, why, I, fly, Never, have, any, issues]  \n",
      "4799   [customer, service, has, been, very, passive, ...  \n",
      "...                                                  ...  \n",
      "110    [has, getaway, deals, through, May, from, $, o...  \n",
      "9739   [worst, service, at, Reagan, Int, I, rarely, p...  \n",
      "2060   [an, inconvenience, is, a, weather, delay, You...  \n",
      "1118   [there, are, a, lot, of, unhappy, cold, people...  \n",
      "5229   [has, the, absolute, worst, customer, service,...  \n",
      "2895   [it, just, goes, round, amp, round, its, be, g...  \n",
      "1748   [I, was, rebooked, but, had, to, take, bus, fr...  \n",
      "11447  [Are, you, an, incompetent, airline, every, da...  \n",
      "2064   [aftermin, of, avoidable, nonweather, delays, ...  \n",
      "1025   [pedophile, airline, Split, myself, and, my, y...  \n",
      "2410   [well, if, you, dont, want, your, customers, t...  \n",
      "11349                [just, lost, a, faithful, customer]  \n",
      "11005  [I, bet, you, dont, but, Im, the, paying, cust...  \n",
      "2850   [thanks, for, reminding, me, how, much, easier...  \n",
      "12420  [Requiring, customers, to, fill, out, all, the...  \n",
      "633    [By, the, way, a, simple, apology, goes, a, lo...  \n",
      "2427   [I, take, it, as, a, compliment, that, I, was,...  \n",
      "13184  [its, really, the, small, thingsthe, detailsth...  \n",
      "11063  [vueloes, vueloses, sell, too, cheap, MADNYC, ...  \n",
      "2071   [I, have, $, and, Ill, draw, you, a, super, sw...  \n",
      "5764   [Why, do, airlines, change, ticket, prices, in...  \n",
      "3921   [speaking, of, my, flight, skateboards, are, a...  \n",
      "5091   [weather, delays, arent, your, fault, today, b...  \n",
      "9386   [I, find, it, funny, that, responds, but, you,...  \n",
      "6505   [I, told, myself, last, time, I, would, never,...  \n",
      "14546  [how, realistic, is, it, to, make, an, minute,...  \n",
      "2441   [remember, in, business, a, pleasant, experien...  \n",
      "1497   [what, about, the, poor, customer, service, at...  \n",
      "14525  [more, of, the, insane, treatment, by, your, c...  \n",
      "7740                                   [Boston, gate, C]  \n",
      "\n",
      "[922 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "df = data.processed_Traindata\n",
    "print(df.loc[df['topic_label'] == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2VecProvider(object):\n",
    "    \n",
    "    word2vec = None\n",
    "    dimensions = 0\n",
    "    \n",
    "    def load(self, path_to_word2vec):\n",
    "        self.word2vec = gensim.models.KeyedVectors.load_word2vec_format(path_to_word2vec, binary=False)\n",
    "        self.word2vec.init_sims(replace=True)\n",
    "        self.dimensions = self.word2vec.vector_size\n",
    "    \n",
    "    def get_vector(self, word):\n",
    "        if word not in self.word2vec.vocab:\n",
    "            return None\n",
    "\n",
    "        return self.word2vec.syn0norm[self.word2vec.vocab[word].index]\n",
    "    \n",
    "    def get_similarity(self, word1, word2):\n",
    "        if word1 not in self.word2vec.vocab or word2 not in self.word2vec.vocab:\n",
    "            return None\n",
    "\n",
    "        return self.word2vec.similarity(word1, word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = Word2VecProvider()\n",
    "\n",
    "# REPLACE PATH TO THE FILE\n",
    "word2vec.load(\"glove.twitter.27B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwitterData(ExtraFeatures):\n",
    "    \n",
    "    def build_final_model(self, word2vec_provider, stopwords=nltk.corpus.stopwords.words(\"english\")):\n",
    "        whitelist = self.whitelist\n",
    "        stopwords = list(filter(lambda sw: sw not in whitelist, stopwords))\n",
    "        extra_columns = [col for col in self.processed_Traindata.columns if col.startswith(\"number_of\")]\n",
    "        similarity_columns = [\"bad_similarity\", \"good_similarity\", \"information_similarity\"]\n",
    "        label_column = [\"label\"]\n",
    "        columns = label_column + [\"original_id\"] + extra_columns + similarity_columns + list(\n",
    "            map(lambda i: \"word2vec_{0}\".format(i), range(0, word2vec_provider.dimensions))) + list(\n",
    "            map(lambda w: w + \"_bow\",self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_Traindata.index:\n",
    "            current_row = []\n",
    "            current_label = self.processed_Traindata.loc[idx, \"airline_sentiment\"]\n",
    "            labels.append(current_label)\n",
    "            current_row.append(current_label)\n",
    "            \n",
    "            current_row.append(self.processed_Traindata.loc[idx, \"tweet_id\"])\n",
    "            \n",
    "            for _,col in enumerate(extra_columns):\n",
    "                current_row.append(self.processed_Traindata.loc[idx, col])\n",
    "            \n",
    "            #average similarities with words\n",
    "            tokens = self.processed_Traindata.loc[idx, \"tokenizedText\"]\n",
    "            for main_word in map(lambda w: w.split(\"_\")[0], similarity_columns):\n",
    "                current_similarities = [abs(sim) for sim in\n",
    "                                        map(lambda word: word2vec_provider.get_similarity(main_word, word.lower()), tokens) if\n",
    "                                        sim is not None]\n",
    "                if len(current_similarities) <= 1:\n",
    "                    current_row.append(0 if len(current_similarities) == 0 else current_similarities[0])\n",
    "                    continue\n",
    "                max_sim = max(current_similarities)\n",
    "                min_sim = min(current_similarities)\n",
    "                current_similarities = [((sim - min_sim) / (max_sim - min_sim)) for sim in\n",
    "                                        current_similarities]  # normalize to <0;1>\n",
    "                current_row.append(np.array(current_similarities).mean())\n",
    "            # add word2vec vector\n",
    "            tokens = self.processed_Traindata.loc[idx, \"tokenizedText\"]\n",
    "            current_word2vec = []\n",
    "            for _, word in enumerate(tokens):\n",
    "                vec = word2vec_provider.get_vector(word.lower())\n",
    "                if vec is not None:\n",
    "                    current_word2vec.append(vec)\n",
    "            if len(current_word2vec) == 0:\n",
    "                averaged_word2vec = [0] * 100\n",
    "            elif len(current_word2vec) > 0:\n",
    "                averaged_word2vec = list(np.array(current_word2vec).mean(axis=0))\n",
    "            current_row += averaged_word2vec\n",
    "            \n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_Traindata.loc[idx, \"text\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "        \n",
    "        self.data_model = pd.DataFrame(rows, columns=columns).fillna(0)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:337: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:517: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3813: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n",
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
      "\n",
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original_id</th>\n",
       "      <th>number_of_uppercase</th>\n",
       "      <th>number_of_exclamation</th>\n",
       "      <th>number_of_question</th>\n",
       "      <th>number_of_ellipsis</th>\n",
       "      <th>number_of_hashtags</th>\n",
       "      <th>number_of_mentions</th>\n",
       "      <th>number_of_quotes</th>\n",
       "      <th>number_of_urls</th>\n",
       "      <th>...</th>\n",
       "      <th>gorgeou_bow</th>\n",
       "      <th>woohoo_bow</th>\n",
       "      <th>thousand_bow</th>\n",
       "      <th>understat_bow</th>\n",
       "      <th>furiou_bow</th>\n",
       "      <th>manual_bow</th>\n",
       "      <th>smell_bow</th>\n",
       "      <th>ber_bow</th>\n",
       "      <th>charleston_bow</th>\n",
       "      <th>nrt_bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>5.693630e+17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>5.694980e+17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>5.681090e+17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>5.682830e+17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>5.696600e+17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2629 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label   original_id  number_of_uppercase  number_of_exclamation  \\\n",
       "0  negative  5.693630e+17                    0                      0   \n",
       "1  negative  5.694980e+17                    2                      0   \n",
       "2   neutral  5.681090e+17                    2                      0   \n",
       "3   neutral  5.682830e+17                    1                      0   \n",
       "4  positive  5.696600e+17                    0                      1   \n",
       "\n",
       "   number_of_question  number_of_ellipsis  number_of_hashtags  \\\n",
       "0                   0                   0                   2   \n",
       "1                   1                   0                   0   \n",
       "2                   0                   0                   1   \n",
       "3                   0                   0                   0   \n",
       "4                   1                   0                   0   \n",
       "\n",
       "   number_of_mentions  number_of_quotes  number_of_urls   ...     gorgeou_bow  \\\n",
       "0                   1                 0               0   ...             0.0   \n",
       "1                   1                 0               1   ...             0.0   \n",
       "2                   1                 0               1   ...             0.0   \n",
       "3                   1                 1               2   ...             0.0   \n",
       "4                   3                 0               0   ...             0.0   \n",
       "\n",
       "   woohoo_bow  thousand_bow  understat_bow  furiou_bow  manual_bow  smell_bow  \\\n",
       "0         0.0           0.0            0.0         0.0         0.0        0.0   \n",
       "1         0.0           0.0            0.0         0.0         0.0        0.0   \n",
       "2         0.0           0.0            0.0         0.0         0.0        0.0   \n",
       "3         0.0           0.0            0.0         0.0         0.0        0.0   \n",
       "4         0.0           0.0            0.0         0.0         0.0        0.0   \n",
       "\n",
       "   ber_bow  charleston_bow  nrt_bow  \n",
       "0      0.0             0.0      0.0  \n",
       "1      0.0             0.0      0.0  \n",
       "2      0.0             0.0      0.0  \n",
       "3      0.0             0.0      0.0  \n",
       "4      0.0             0.0      0.0  \n",
       "\n",
       "[5 rows x 2629 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TwitterData()\n",
    "td.initialize(\"Tweets.csv\")\n",
    "td.build_features()\n",
    "td.cleaningData(DataPreprocessing())\n",
    "td.tokenize()\n",
    "td.stem()\n",
    "td.buildWordlist()\n",
    "td.build_final_model(word2vec)\n",
    "\n",
    "td.data_model.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_model = td.data_model\n",
    "data_model.drop(\"original_id\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning:\n",
      "\n",
      "From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing RandomForestClassifier\n",
      "Learning time 11.059098482131958s\n",
      "Predicting time 0.36649298667907715s\n",
      "==================Results=======================\n",
      "            Negative     Neutral    Positive\n",
      "F1         [0.83622201 0.41398866 0.54850299]\n",
      "Precision  [0.73101805 0.70873786 0.85447761]\n",
      "Recall     [0.97679709 0.29238985 0.40388007]\n",
      "Accuracy   0.7384746727376209\n",
      "================================================\n",
      "===============================================\n",
      "Crossvalidating RandomForestClassifier...\n",
      "Crosvalidation completed in 138.20311641693115s\n",
      "Accuracy: [0.74283765 0.75699659 0.74931694 0.74385246 0.74180328 0.73205742\n",
      " 0.74914559 0.734108  ]\n",
      "Average accuracy: 0.743764740361236\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_model.iloc[:, 1:], data_model.iloc[:, 0],\n",
    "                                                    train_size=0.7, stratify=data_model.iloc[:, 0],\n",
    "                                                    random_state=seed)\n",
    "precision, recall, accuracy, f1 = test_classifier(X_train, y_train, X_test, y_test, RandomForestClassifier(n_estimators=403,n_jobs=-1, random_state=seed))\n",
    "rf_acc = cv(RandomForestClassifier(n_estimators=403,n_jobs=-1,random_state=seed),data_model.iloc[:, 1:], data_model.iloc[:, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
